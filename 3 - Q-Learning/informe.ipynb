{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d13bbe5-80d1-4879-bc47-d8e366f38456",
   "metadata": {},
   "source": [
    "# **Entrega 3 - Lunar Lander con Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82127016-b75d-4621-a53e-8b2bb63cd3f8",
   "metadata": {},
   "source": [
    "### **1. Bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e0a19d-696b-440e-84eb-fe2b8761d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import pygame\n",
    "from pygame.locals import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831e9d4-7840-485c-bc38-af987a76de4f",
   "metadata": {},
   "source": [
    "### **2. Jugando a mano**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e744255-5e3f-4c17-b9dc-c6374c2f06c2",
   "metadata": {},
   "source": [
    "A continuación se puede jugar un episodio del lunar lander. Se controlan los motores con el teclado. Notar que solo se puede realizar una acción a la vez (que es parte del problema), y que en esta implementación, izq toma precedencia sobre derecha, que toma precedencia sobre el motor principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6211ed30-b1a3-4b8e-9858-17eee433ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar pygame (para el control con el teclado) y el ambiente\n",
    "pygame.init()\n",
    "env = gym.make('LunarLander-v2', render_mode='human')\n",
    "env.reset()\n",
    "pygame.display.set_caption('Lunar Lander')\n",
    "\n",
    "clock = pygame.time.Clock()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == QUIT:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "    keys = pygame.key.get_pressed()\n",
    "\n",
    "    # Map keys to actions\n",
    "    if keys[K_LEFT]:\n",
    "        action = 3  # Fire left orientation engine\n",
    "    elif keys[K_RIGHT]:\n",
    "        action = 1 # Fire right orientation engine\n",
    "    elif keys[K_UP]:\n",
    "        action = 2  # Fire main engine\n",
    "    else:\n",
    "        action = 0  # Do nothing\n",
    "\n",
    "    _, _, terminated, truncated, _ = env.step(action)\n",
    "    env.render()\n",
    "    clock.tick(10)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        done = True\n",
    "\n",
    "env.close()\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721a629-7f3f-4a70-83c0-e12f43b0f285",
   "metadata": {},
   "source": [
    "## **3. Discretizando el estado**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d35189-3a15-4ceb-b978-38226518092a",
   "metadata": {},
   "source": [
    "El estado consiste de posiciones y velocidades en (x,y,theta) y en información de contacto de los pies con la superficie.\n",
    "\n",
    "Como varios de estos son continuos, tenemos que discretizarlos para aplicar nuestro algoritmo de aprendizaje por refuerzo tabular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7b6bed3-12ba-4d92-8b09-3d3f8429217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuántos bins queremos por dimensión\n",
    "# Pueden considerar variar este parámetro\n",
    "bins_per_dim = 16\n",
    "\n",
    "#          Estado:\n",
    "#          (x,            y,            x_vel,        y_vel,        theta,        theta_vel,    pie_izq_en_contacto, pie_derecho_en_contacto)\n",
    "NUM_BINS = [bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, 2, 2]\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "\n",
    "# Tomamos los rangos del env\n",
    "OBS_SPACE_HIGH = env.observation_space.high\n",
    "OBS_SPACE_LOW = env.observation_space.low\n",
    "OBS_SPACE_LOW[1] = 0 # Para la coordenada y (altura), no podemos ir más abajo que la zona de aterrizaje (que está en el 0, 0)\n",
    "\n",
    "# Los bins para cada dimensión\n",
    "bins = [\n",
    "    np.linspace(OBS_SPACE_LOW[i], OBS_SPACE_HIGH[i], NUM_BINS[i] - 1)\n",
    "    for i in range(len(NUM_BINS) - 2) # last two are binary\n",
    "]\n",
    "# Se recomienda observar los bins para entender su estructura\n",
    "# print (\"Bins: \", bins)\n",
    "\n",
    "env.close()\n",
    "\n",
    "def discretize_state(state, bins):\n",
    "    \"\"\"Discretize the continuous state into a tuple of discrete indices, taking the closest bin.\"\"\"\n",
    "    state_disc = list()\n",
    "    for i in range(len(state)):\n",
    "        if i >= len(bins):  # For binary features (leg contacts)\n",
    "            state_disc.append(int(state[i]))\n",
    "        else:\n",
    "            # Encuentra el índice del valor más cercano en los bins\n",
    "            closest_index = np.argmin(np.abs(bins[i] - state[i]))\n",
    "            state_disc.append(closest_index)\n",
    "    return tuple(state_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45bcc921-c5f6-4f06-9702-6e740b26fc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 0, 7, 7, 7, 7, 1, 1)\n",
      "(7, 14, 7, 7, 7, 7, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos\n",
    "print(discretize_state([0.0, 0.0, 0, 0, 0, 0, 1, 1], bins)) # En la zona de aterrizaje y quieto\n",
    "print(discretize_state([0, 1.5, 0, 0, 0, 0, 0, 0], bins)) # Comenzando la partida, arriba y en el centro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c1576-eff2-4b3c-8069-f6d30243f1e6",
   "metadata": {},
   "source": [
    "## **4. Agentes y la interacción con el entorno**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d594f89c-70b1-4c7c-939f-d654a5263f1c",
   "metadata": {},
   "source": [
    "Vamos a definir una interfaz para nuestro agente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e7059a-7b68-4b13-857b-16e96c5f9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente:\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        \"\"\"Elegir la accion a tomar en el estado actual y el espacio de acciones\n",
    "            - estado: el estado en el que se encuentra actualmente el agente\n",
    "            - max_accion: el espacio de acciones posibles\n",
    "            - explorar: si se debe elegir una acción de forma que explore el espacio de estados, o eligiendo la que mejor recompensa cree que devuelve\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado) -> None:\n",
    "        \"\"\"Aprender a partir de la tupla \n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "            - terminado: si el episodio terminó en esta transición\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fin_episodio(self) -> None:\n",
    "        \"\"\"Actualizar estructuras al final de un episodio\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c61e38-f7d3-40bf-af19-c7fefd143ffd",
   "metadata": {},
   "source": [
    "Para un agente aleatorio, la implementación sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3adcedd-b300-4e30-9cb1-19d5ec96fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenteAleatorio(Agente):\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        # Elige una acción al azar\n",
    "        return random.randrange(max_accion)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado) -> None:\n",
    "        # No aprende\n",
    "        pass\n",
    "\n",
    "    def fin_episodio(self) -> None:\n",
    "        # Nada que actualizar\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19498b0d-eb74-431e-ac7f-612497ca07f3",
   "metadata": {},
   "source": [
    "Luego podemos definir una función para ejecutar un episodio con un agente dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "038e9f56-f0cf-40b7-b3c3-a63b34572bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio(agente, aprender=True, render=None, max_iteraciones=500):\n",
    "    entorno = gym.make('LunarLander-v2', render_mode=render).env\n",
    "    \n",
    "    iteraciones = 0\n",
    "    recompensa_total = 0\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset()\n",
    "    while iteraciones < max_iteraciones and not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n, aprender)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        # Le informamos al agente para que aprenda\n",
    "        if (aprender):\n",
    "            agente.aprender(estado_anterior, estado_siguiente, accion, recompensa, termino)\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "        recompensa_total += recompensa\n",
    "        \n",
    "    if (aprender):\n",
    "        agente.fin_episodio()\n",
    "\n",
    "    entorno.close()\n",
    "    return recompensa_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29de7442-4565-4ce9-b442-a0e8f7c038a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-109.55631357973508"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejecutamos un episodio con el agente aleatorio y modo render 'human', para poder verlo\n",
    "ejecutar_episodio(AgenteAleatorio(), render = 'human')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9cb2a-37b0-4115-afe8-bb89ce49f605",
   "metadata": {},
   "source": [
    "Podemos ejecutar este ambiente muchas veces y tomar métricas al respecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e81ee-6038-44c7-b887-35db442815ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "AgenteAleatorio = AgenteAleatorio()\n",
    "recompensa_episodios = []\n",
    "\n",
    "exitos = 0\n",
    "num_episodios = 100\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio(AgenteAleatorio, render='human')\n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        exitos += 1\n",
    "    recompensa_episodios += [recompensa]\n",
    "\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {np.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086cb250-7bc9-4bd4-a4cd-43cfd561facb",
   "metadata": {},
   "source": [
    "### **5. Programando un agente que aprende**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827e3e2-a84a-462f-8b86-e03425cfc645",
   "metadata": {},
   "source": [
    "La tarea a realizar consiste en programar un agente de aprendizaje por refuerzos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c40f3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hiperparametros\n",
    "  Politica de aprendizaje (Aprender en el momento / Aprender al final) = Aprender al final\n",
    "  Politica de exploracion (random, epsilon-greedy, softmax) = epsilon greedy\n",
    "  epsilon = 0.1\n",
    "  Temperatura / k = Dudoso\n",
    "  Alfa / Learning rate = 1 / cantidad_visitas(s, a)\n",
    "  Cantidad de bins = 16\n",
    "'''\n",
    "\n",
    "class AgenteRL(Agente):\n",
    "\n",
    "    def __init__(self, k=2, epsilon=0.1, politica_exploracion='epsilon-greedy', aprender_inmediatamente=False) -> None:\n",
    "        '''\n",
    "        Parametros\n",
    "        ----------\n",
    "        k: Usado si politica_exploracion=softmax / A mayor valor, mas probable es que se use explotacion\n",
    "        epsilon: Usado si politica_exploracion=epsilon-greedy / Probabilidad de exploracion en cada paso\n",
    "        politica_exploracion: Funcion que determinará la accion a tomar en caso de encontrarse explorando. ( random, epsilon-greedy, softmax )\n",
    "        aprender_inmediatamente: Si el agente debe aprender en el momento o al final del episodio haciendo un reccorido \"hacia atras\"\n",
    "\n",
    "        Utiliza una función de aprendizaje Q-Learning no-determinista, mediante la formula:\n",
    "        Q(s, a) = (1 - alfa) * Q(s, a) + alfa * (recompensa + max(Q(s', a')))\n",
    "        donde alfa=1/cantidad_visitas(s, a)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.Q = {}\n",
    "        self.cantidad_visitas = {}\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.politica_exploracion = politica_exploracion\n",
    "        self.aprender_inmediatamente = aprender_inmediatamente\n",
    "\n",
    "        # En caso de que aprenda al finalizar el episodio, se guardan las acciones tomadas\n",
    "        self.acciones_tomadas = []\n",
    "\n",
    "    def elegir_accion(self, estado, max_accion, explorar=True) -> int:\n",
    "        estado = discretize_state(estado, bins)\n",
    "\n",
    "        # La tabla Q y cantidad_visitas se genera a medida que se visitan estados\n",
    "        if estado not in self.Q:\n",
    "            self.Q[estado] = {i: 0 for i in range(max_accion)}\n",
    "            for accion in range(max_accion):\n",
    "                self.cantidad_visitas[(estado, accion)] = 0     \n",
    "\n",
    "        if explorar: # Explorar\n",
    "\n",
    "            match self.politica_exploracion:\n",
    "                case 'random':\n",
    "                    return random.randrange(max_accion)\n",
    "            \n",
    "                case 'epsilon-greedy':\n",
    "                    if random.random() < self.epsilon:\n",
    "                        return random.randrange(max_accion)\n",
    "                    else:\n",
    "                        return max(self.Q[estado], key=self.Q[estado].get)\n",
    "                \n",
    "                case 'softmax':\n",
    "                    q_values = [self.Q[estado][a] for a in range(max_accion)]\n",
    "                    min_q, max_q = min(q_values), max(q_values)\n",
    "                    \n",
    "                    normalized_q_values = [(q - min_q) / (max_q - min_q) for q in q_values] if max_q != min_q else [0] * len(q_values)  \n",
    "                    \n",
    "                    k_values = [self.k**q for q in normalized_q_values]\n",
    "                    sum_k_values = sum(k_values)\n",
    "                    probabilidades = [k_elevado / sum_k_values for k_elevado in k_values]\n",
    "                    \n",
    "                    return np.random.choice(range(max_accion), p=probabilidades)\n",
    "\n",
    "        else: # Explotacion\n",
    "            return max(self.Q[estado], key=self.Q[estado].get)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_actual, accion, recompensa, terminado) -> None:\n",
    "        estado_anterior = discretize_state(estado_anterior, bins)\n",
    "        estado_actual   = discretize_state(estado_actual, bins)\n",
    "        \n",
    "        # La tabla Q y cantidad_visitas se genera a medida que se visitan estados\n",
    "        for estado in [estado_anterior, estado_actual]:\n",
    "            if estado not in self.Q:\n",
    "                self.Q[estado] = {i: 0 for i in range(4)}\n",
    "                for a in range(4):\n",
    "                    self.cantidad_visitas[(estado, a)] = 0\n",
    "            \n",
    "        self.cantidad_visitas[(estado_anterior, accion)] += 1 \n",
    "\n",
    "        if (self.aprender_inmediatamente):\n",
    "            alfa = 1 / self.cantidad_visitas[(estado_anterior, accion)]\n",
    "            self.Q[estado_anterior][accion] = (1 - alfa) * self.Q[estado_anterior][accion] + alfa * (recompensa + max(self.Q[estado_actual].values()))\n",
    "        else:\n",
    "            self.acciones_tomadas.append((estado_anterior, estado_actual, accion, recompensa))     \n",
    "\n",
    "\n",
    "    def fin_episodio(self) -> None:\n",
    "        '''\n",
    "        En caso de que el agente aprenda al final del episodio, se recorre hacia atras las acciones tomadas durante\n",
    "        el episodio y se actualizan los valores de Q\n",
    "        En caso contrario, no se hace nada\n",
    "        '''\n",
    "        if (not self.aprender_inmediatamente):\n",
    "            for i in range(len(self.acciones_tomadas)-1, -1, -1):\n",
    "                estado_anterior, estado_actual, accion, recompensa = self.acciones_tomadas[i]\n",
    "                alfa = 1 / self.cantidad_visitas[(estado_anterior, accion)]\n",
    "                self.Q[estado_anterior][accion] = (1 - alfa) * self.Q[estado_anterior][accion] + alfa * (recompensa + max(self.Q[estado_actual].values()))\n",
    "\n",
    "            self.acciones_tomadas = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee51f5-aff8-4d18-934e-92acdcb617c0",
   "metadata": {},
   "source": [
    "Y ejecutar con el muchos episodios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e0524d5-0d12-46a8-8437-984b981fbae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 0\n",
      "Recompensa parcial promedio: -172.22140325666993\n",
      "\n",
      "Episodio: 100\n",
      "Recompensa parcial promedio: -219.65655221586917\n",
      "\n",
      "Episodio: 200\n",
      "Recompensa parcial promedio: -234.41036868541622\n",
      "\n",
      "Episodio: 300\n",
      "Recompensa parcial promedio: -223.62709490811355\n",
      "\n",
      "Episodio: 400\n",
      "Recompensa parcial promedio: -199.42386365852445\n",
      "\n",
      "Episodio: 500\n",
      "Recompensa parcial promedio: -198.8002897104285\n",
      "\n",
      "Episodio: 600\n",
      "Recompensa parcial promedio: -188.55169564993528\n",
      "\n",
      "Episodio: 700\n",
      "Recompensa parcial promedio: -169.46199591355963\n",
      "\n",
      "Episodio: 800\n",
      "Recompensa parcial promedio: -178.16832582774813\n",
      "\n",
      "Episodio: 900\n",
      "Recompensa parcial promedio: -183.47435259399688\n",
      "\n",
      "Episodio: 1000\n",
      "Recompensa parcial promedio: -198.91346239617226\n",
      "\n",
      "Episodio: 1100\n",
      "Recompensa parcial promedio: -178.19774646211692\n",
      "\n",
      "Episodio: 1200\n",
      "Recompensa parcial promedio: -155.11773281904283\n",
      "\n",
      "Episodio: 1300\n",
      "Recompensa parcial promedio: -175.80949563288414\n",
      "\n",
      "Episodio: 1400\n",
      "Recompensa parcial promedio: -183.94299716752192\n",
      "\n",
      "Episodio: 1500\n",
      "Recompensa parcial promedio: -172.8437110928318\n",
      "\n",
      "Episodio: 1600\n",
      "Recompensa parcial promedio: -166.20950687132054\n",
      "\n",
      "Episodio: 1700\n",
      "Recompensa parcial promedio: -204.1733787961239\n",
      "\n",
      "Episodio: 1800\n",
      "Recompensa parcial promedio: -171.61139606731024\n",
      "\n",
      "Episodio: 1900\n",
      "Recompensa parcial promedio: -186.7684373449655\n",
      "\n",
      "Episodio: 2000\n",
      "Recompensa parcial promedio: -161.07221769840618\n",
      "\n",
      "Episodio: 2100\n",
      "Recompensa parcial promedio: -180.54678025277786\n",
      "\n",
      "Episodio: 2200\n",
      "Recompensa parcial promedio: -144.29303461168894\n",
      "\n",
      "Episodio: 2300\n",
      "Recompensa parcial promedio: -146.83568932290152\n",
      "\n",
      "Episodio: 2400\n",
      "Recompensa parcial promedio: -175.05004689494123\n",
      "\n",
      "Episodio: 2500\n",
      "Recompensa parcial promedio: -170.5233987232757\n",
      "\n",
      "Episodio: 2600\n",
      "Recompensa parcial promedio: -156.6822614364347\n",
      "\n",
      "Episodio: 2700\n",
      "Recompensa parcial promedio: -166.2616541860911\n",
      "\n",
      "Episodio: 2800\n",
      "Recompensa parcial promedio: -153.7253051451213\n",
      "\n",
      "Episodio: 2900\n",
      "Recompensa parcial promedio: -150.73373977159116\n",
      "\n",
      "Episodio: 3000\n",
      "Recompensa parcial promedio: -162.12178917245413\n",
      "\n",
      "Episodio: 3100\n",
      "Recompensa parcial promedio: -155.03005811091444\n",
      "\n",
      "Episodio: 3200\n",
      "Recompensa parcial promedio: -150.19164630658472\n",
      "\n",
      "Episodio: 3300\n",
      "Recompensa parcial promedio: -143.91778816446612\n",
      "\n",
      "Episodio: 3400\n",
      "Recompensa parcial promedio: -146.8179851172547\n",
      "\n",
      "Episodio: 3500\n",
      "Recompensa parcial promedio: -124.84010092014593\n",
      "\n",
      "Episodio: 3600\n",
      "Recompensa parcial promedio: -144.99614376396775\n",
      "\n",
      "Episodio: 3700\n",
      "Recompensa parcial promedio: -153.4185507445494\n",
      "\n",
      "Episodio: 3800\n",
      "Recompensa parcial promedio: -135.64703391581378\n",
      "\n",
      "Episodio: 3900\n",
      "Recompensa parcial promedio: -152.7856291701853\n",
      "\n",
      "Episodio: 4000\n",
      "Recompensa parcial promedio: -135.0254903396027\n",
      "\n",
      "Episodio: 4100\n",
      "Recompensa parcial promedio: -131.1648499558701\n",
      "\n",
      "Episodio: 4200\n",
      "Recompensa parcial promedio: -141.30292046722641\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m num_episodios \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200000\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodios):\n\u001b[1;32m----> 7\u001b[0m     recompensa \u001b[38;5;241m=\u001b[39m \u001b[43mejecutar_episodio\u001b[49m\u001b[43m(\u001b[49m\u001b[43magente\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maprender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iteraciones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (recompensa \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m):\n",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m, in \u001b[0;36mejecutar_episodio\u001b[1;34m(agente, aprender, render, max_iteraciones)\u001b[0m\n\u001b[0;32m      9\u001b[0m estado_anterior, info \u001b[38;5;241m=\u001b[39m entorno\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m iteraciones \u001b[38;5;241m<\u001b[39m max_iteraciones \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m termino \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m truncado:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     accion \u001b[38;5;241m=\u001b[39m \u001b[43magente\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melegir_accion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestado_anterior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentorno\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maprender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Realizamos la accion\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     estado_siguiente, recompensa, termino, truncado, info \u001b[38;5;241m=\u001b[39m entorno\u001b[38;5;241m.\u001b[39mstep(accion)\n",
      "Cell \u001b[1;32mIn[14], line 38\u001b[0m, in \u001b[0;36mAgenteRL.elegir_accion\u001b[1;34m(self, estado, max_accion, explorar)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melegir_accion\u001b[39m(\u001b[38;5;28mself\u001b[39m, estado, max_accion, explorar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m---> 38\u001b[0m     estado \u001b[38;5;241m=\u001b[39m \u001b[43mdiscretize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestado\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# La tabla Q y cantidad_visitas se genera a medida que se visitan estados\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m estado \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ:\n",
      "Cell \u001b[1;32mIn[18], line 35\u001b[0m, in \u001b[0;36mdiscretize_state\u001b[1;34m(state, bins)\u001b[0m\n\u001b[0;32m     32\u001b[0m         state_disc\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mint\u001b[39m(state[i]))\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;66;03m# Encuentra el índice del valor más cercano en los bins\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m         closest_index \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbins\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m         state_disc\u001b[38;5;241m.\u001b[39mappend(closest_index)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(state_disc)\n",
      "File \u001b[1;32mc:\\Users\\nicoa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1325\u001b[0m, in \u001b[0;36margmin\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;124;03mReturns the indices of the minimum values along an axis.\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmin\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\nicoa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agente = AgenteRL()\n",
    "exitos = 0\n",
    "recompensa_parcial=[]\n",
    "recompensa_episodios = []\n",
    "num_episodios = 200000\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio(agente, aprender=True, max_iteraciones=1000)\n",
    "    \n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        exitos += 1\n",
    "    recompensa_episodios += [recompensa]\n",
    "    recompensa_parcial += [recompensa]\n",
    "    \n",
    "    if (i % 100 == 0):\n",
    "        print(f'Episodio: {i}')\n",
    "        print(f'Recompensa parcial promedio: {np.mean(recompensa_parcial)}')\n",
    "        print('')\n",
    "        recompensa_parcial = []\n",
    "    \n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {np.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e7b346-6f23-44e2-a645-e4548ab470ef",
   "metadata": {},
   "source": [
    "Analizar los resultados de la ejecución anterior, incluyendo:\n",
    " * Un análisis de los parámetros utilizados en el algoritmo (aprendizaje, política de exploración)\n",
    " * Un análisis de algunos 'cortes' de la matriz Q y la política (p.e. qué hace la nave cuando está cayendo rápidamente hacia abajo, sin rotación)\n",
    " * Un análisis de la evolución de la recompensa promedio\n",
    " * Un análisis de los casos de éxito\n",
    " * Un análisis de los casos en el que el agente falla\n",
    " * Qué limitante del agente de RL les parece que afecta más negativamente su desempeño. Cómo lo mejorarían? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f93fe904-e691-42ff-8fd4-b360d08431cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 71\n",
      "Recompensa: 200.26177327984996\n",
      "\n",
      "Episodio: 117\n",
      "Recompensa: 219.57381888641552\n",
      "\n",
      "Episodio: 182\n",
      "Recompensa: 215.24744637639049\n",
      "\n",
      "Episodio: 231\n",
      "Recompensa: 206.11593591233452\n",
      "\n",
      "Episodio: 248\n",
      "Recompensa: 252.18405348369407\n",
      "\n",
      "Episodio: 352\n",
      "Recompensa: 236.1177855664987\n",
      "\n",
      "Episodio: 433\n",
      "Recompensa: 232.83477256120406\n",
      "\n",
      "Episodio: 467\n",
      "Recompensa: 231.31060955951355\n",
      "\n",
      "Episodio: 475\n",
      "Recompensa: 210.9573502156818\n",
      "\n",
      "Episodio: 570\n",
      "Recompensa: 237.4533569109859\n",
      "\n",
      "Episodio: 646\n",
      "Recompensa: 220.05494828382825\n",
      "\n",
      "Episodio: 701\n",
      "Recompensa: 234.20920055922642\n",
      "\n",
      "Episodio: 702\n",
      "Recompensa: 211.72260510171608\n",
      "\n",
      "Episodio: 758\n",
      "Recompensa: 236.0622822886867\n",
      "\n",
      "Episodio: 779\n",
      "Recompensa: 212.6516218680351\n",
      "\n",
      "Episodio: 802\n",
      "Recompensa: 202.46095204317544\n",
      "\n",
      "Episodio: 896\n",
      "Recompensa: 242.4654467505292\n",
      "\n",
      "Episodio: 928\n",
      "Recompensa: 216.09823364953087\n",
      "\n",
      "Episodio: 933\n",
      "Recompensa: 227.76295834238027\n",
      "\n",
      "Episodio: 967\n",
      "Recompensa: 265.2259698444155\n",
      "\n",
      "Episodio: 982\n",
      "Recompensa: 249.81414093186953\n",
      "\n",
      "Tasa de éxito: 0.021. Se obtuvo -108.62714621715666 de recompensa, en promedio\n"
     ]
    }
   ],
   "source": [
    "exitos = 0\n",
    "recompensa_episodios = []\n",
    "num_episodios = 1000\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio(agente, aprender=False, max_iteraciones=1000)\n",
    "    recompensa_episodios += [recompensa]\n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        exitos += 1\n",
    "        print(f'Episodio: {i}')\n",
    "        print(f'Recompensa: {recompensa}')\n",
    "        print('')\n",
    "\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {np.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
