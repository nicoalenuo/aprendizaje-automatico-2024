{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d13bbe5-80d1-4879-bc47-d8e366f38456",
   "metadata": {},
   "source": [
    "# **Lunar Lander con Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82127016-b75d-4621-a53e-8b2bb63cd3f8",
   "metadata": {},
   "source": [
    "### **1. Bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e0a19d-696b-440e-84eb-fe2b8761d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install cmake gymnasium scipy numpy gymnasium[box2d] pygame==2.6.0 swig\n",
    "# Tal vez tengan que ejecutar lo siguiente en sus máquinas (ubuntu 20.04)\n",
    "# sudo apt-get remove swig\n",
    "# sudo apt-get install swig3.0\n",
    "# sudo ln -s /usr/bin/swig3.0 /usr/bin/swig\n",
    "# En windows tambien puede ser necesario MSVC++\n",
    "\n",
    "import sys\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import pygame\n",
    "from pygame.locals import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831e9d4-7840-485c-bc38-af987a76de4f",
   "metadata": {},
   "source": [
    "### **2. Jugando a mano**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e744255-5e3f-4c17-b9dc-c6374c2f06c2",
   "metadata": {},
   "source": [
    "A continuación se puede jugar un episodio del lunar lander. Se controlan los motores con el teclado. Notar que solo se puede realizar una acción a la vez (que es parte del problema), y que en esta implementación, izq toma precedencia sobre derecha, que toma precedencia sobre el motor principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6211ed30-b1a3-4b8e-9858-17eee433ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar pygame (para el control con el teclado) y el ambiente\n",
    "pygame.init()\n",
    "env = gym.make('LunarLander-v2', render_mode='human')\n",
    "env.reset()\n",
    "pygame.display.set_caption('Lunar Lander')\n",
    "\n",
    "clock = pygame.time.Clock()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == QUIT:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "    keys = pygame.key.get_pressed()\n",
    "\n",
    "    # Map keys to actions\n",
    "    if keys[K_LEFT]:\n",
    "        action = 3  # Fire left orientation engine\n",
    "    elif keys[K_RIGHT]:\n",
    "        action = 1 # Fire right orientation engine\n",
    "    elif keys[K_UP]:\n",
    "        action = 2  # Fire main engine\n",
    "    else:\n",
    "        action = 0  # Do nothing\n",
    "\n",
    "    _, _, terminated, truncated, _ = env.step(action)\n",
    "    env.render()\n",
    "    clock.tick(10)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        done = True\n",
    "\n",
    "env.close()\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721a629-7f3f-4a70-83c0-e12f43b0f285",
   "metadata": {},
   "source": [
    "## **3. Discretizando el estado**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d35189-3a15-4ceb-b978-38226518092a",
   "metadata": {},
   "source": [
    "El estado consiste de posiciones y velocidades en (x,y,theta) y en información de contacto de los pies con la superficie.\n",
    "\n",
    "Como varios de estos son continuos, tenemos que discretizarlos para aplicar nuestro algoritmo de aprendizaje por refuerzo tabular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b6bed3-12ba-4d92-8b09-3d3f8429217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuántos bins queremos por dimensión\n",
    "# Pueden considerar variar este parámetro\n",
    "bins_per_dim = 20\n",
    "\n",
    "#          Estado:\n",
    "#          (x,            y,            x_vel,        y_vel,        theta,        theta_vel,    pie_izq_en_contacto, pie_derecho_en_contacto)\n",
    "NUM_BINS = [bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, 2, 2]\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode='human')\n",
    "env.reset()\n",
    "\n",
    "# Tomamos los rangos del env\n",
    "OBS_SPACE_HIGH = env.observation_space.high\n",
    "OBS_SPACE_LOW = env.observation_space.low\n",
    "OBS_SPACE_LOW[1] = 0 # Para la coordenada y (altura), no podemos ir más abajo que la zona de aterrizaje (que está en el 0, 0)\n",
    "\n",
    "# Los bins para cada dimensión\n",
    "bins = [\n",
    "    np.linspace(OBS_SPACE_LOW[i], OBS_SPACE_HIGH[i], NUM_BINS[i] - 1)\n",
    "    for i in range(len(NUM_BINS) - 2) # last two are binary\n",
    "]\n",
    "# Se recomienda observar los bins para entender su estructura\n",
    "# print (\"Bins: \", bins)\n",
    "\n",
    "def discretize_state(state, bins):\n",
    "    \"\"\"Discretize the continuous state into a tuple of discrete indices, taking the closest bin.\"\"\"\n",
    "    state_disc = list()\n",
    "    for i in range(len(state)):\n",
    "        if i >= len(bins):  # For binary features (leg contacts)\n",
    "            state_disc.append(int(state[i]))\n",
    "        else:\n",
    "            # Encuentra el índice del valor más cercano en los bins\n",
    "            closest_index = np.argmin(np.abs(bins[i] - state[i]))\n",
    "            state_disc.append(closest_index)\n",
    "    return tuple(state_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45bcc921-c5f6-4f06-9702-6e740b26fc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 10, 10, 10, 10, 1, 1)\n",
      "(10, 19, 10, 10, 10, 10, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos\n",
    "print(discretize_state([0.0, 0.0, 0, 0, 0, 0, 1, 1], bins)) # En la zona de aterrizaje y quieto\n",
    "print(discretize_state([0, 1.5, 0, 0, 0, 0, 0, 0], bins)) # Comenzando la partida, arriba y en el centro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c1576-eff2-4b3c-8069-f6d30243f1e6",
   "metadata": {},
   "source": [
    "## **4. Agentes y la interacción con el entorno**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d594f89c-70b1-4c7c-939f-d654a5263f1c",
   "metadata": {},
   "source": [
    "Vamos a definir una interfaz para nuestro agente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e7059a-7b68-4b13-857b-16e96c5f9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente:\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        \"\"\"Elegir la accion a tomar en el estado actual y el espacio de acciones\n",
    "            - estado: el estado en el que se encuentra actualmente el agente\n",
    "            - max_accion: el espacio de acciones posibles\n",
    "            - explorar: si se debe elegir una acción de forma que explore el espacio de estados, o eligiendo la que mejor recompensa cree que devuelve\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado) -> None:\n",
    "        \"\"\"Aprender a partir de la tupla \n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "            - terminado: si el episodio terminó en esta transición\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fin_episodio(self) -> None:\n",
    "        \"\"\"Actualizar estructuras al final de un episodio\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c61e38-f7d3-40bf-af19-c7fefd143ffd",
   "metadata": {},
   "source": [
    "Para un agente aleatorio, la implementación sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3adcedd-b300-4e30-9cb1-19d5ec96fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class AgenteAleatorio(Agente):\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        # Elige una acción al azar\n",
    "        return random.randrange(max_accion)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado) -> None:\n",
    "        # No aprende\n",
    "        pass\n",
    "\n",
    "    def fin_episodio(self) -> None:\n",
    "        # Nada que actualizar\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19498b0d-eb74-431e-ac7f-612497ca07f3",
   "metadata": {},
   "source": [
    "Luego podemos definir una función para ejecutar un episodio con un agente dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "038e9f56-f0cf-40b7-b3c3-a63b34572bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio(agente, aprender=True, render=None, max_iteraciones=500):\n",
    "    entorno = gym.make('LunarLander-v2', render_mode=render).env\n",
    "    \n",
    "    iteraciones = 0\n",
    "    recompensa_total = 0\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset()\n",
    "    while iteraciones < max_iteraciones and not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n, aprender)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        # Le informamos al agente para que aprenda\n",
    "        if (aprender):\n",
    "            agente.aprender(estado_anterior, estado_siguiente, accion, recompensa, termino)\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "        recompensa_total += recompensa\n",
    "        \n",
    "    if (aprender):\n",
    "        agente.fin_episodio()\n",
    "\n",
    "    entorno.close()\n",
    "    return recompensa_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29de7442-4565-4ce9-b442-a0e8f7c038a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-85.00346673972187"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejecutamos un episodio con el agente aleatorio y modo render 'human', para poder verlo\n",
    "ejecutar_episodio(AgenteAleatorio(), render = 'human')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9cb2a-37b0-4115-afe8-bb89ce49f605",
   "metadata": {},
   "source": [
    "Podemos ejecutar este ambiente muchas veces y tomar métricas al respecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "536e81ee-6038-44c7-b887-35db442815ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m num_episodios \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodios):\n\u001b[1;32m----> 7\u001b[0m     recompensa \u001b[38;5;241m=\u001b[39m \u001b[43mejecutar_episodio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAgenteAleatorio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (recompensa \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m):\n",
      "Cell \u001b[1;32mIn[18], line 14\u001b[0m, in \u001b[0;36mejecutar_episodio\u001b[1;34m(agente, aprender, render, max_iteraciones)\u001b[0m\n\u001b[0;32m     12\u001b[0m accion \u001b[38;5;241m=\u001b[39m agente\u001b[38;5;241m.\u001b[39melegir_accion(estado_anterior, entorno\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn, aprender)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Realizamos la accion\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m estado_siguiente, recompensa, termino, truncado, info \u001b[38;5;241m=\u001b[39m \u001b[43mentorno\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Le informamos al agente para que aprenda\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (aprender):\n",
      "File \u001b[1;32mc:\\Users\\nicoa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicoa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicoa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:675\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    672\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 675\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32mc:\\Users\\nicoa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:787\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    786\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    788\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#AgenteAleatorio = AgenteAleatorio()\n",
    "recompensa_episodios = []\n",
    "\n",
    "exitos = 0\n",
    "num_episodios = 100\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio(AgenteAleatorio, render='human')\n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        exitos += 1\n",
    "    recompensa_episodios += [recompensa]\n",
    "\n",
    "import numpy\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {numpy.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086cb250-7bc9-4bd4-a4cd-43cfd561facb",
   "metadata": {},
   "source": [
    "### **5. Programando un agente que aprende**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827e3e2-a84a-462f-8b86-e03425cfc645",
   "metadata": {},
   "source": [
    "La tarea a realizar consiste en programar un agente de aprendizaje por refuerzos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c40f3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "'''\n",
    "Hiperparametros\n",
    "  Politica de exploracion (random, epsilon-greedy, softmax)\n",
    "  Politica de aprendizaje (Aprender en el momento / Aprender al final)\n",
    "  Temperatura / k\n",
    "  Alfa / Learning rate\n",
    "  Cantidad de bins\n",
    "'''\n",
    "\n",
    "class AgenteRL(Agente):\n",
    "\n",
    "    def __init__(self, k=1, epsilon=0.1, politica_exploracion='softmax', aprender_inmediatamente=True) -> None:\n",
    "        '''\n",
    "        Parametros\n",
    "        ----------\n",
    "        alfa: Learning rate\n",
    "        k: Temperatura\n",
    "        epsilon: Probabilidad de explotacion en caso de que politica_exploracion=epsilon-greedy\n",
    "        politica_exploracion: Funcion que determinará la accion a tomar en caso de encontrarse explorando. ( random, epsilon-greedy, softmax )\n",
    "        aprender_inmediatamente: Si el agente debe aprender en el momento o al final del episodio haciendo un reccorido \"hacia atras\"\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.Q = {}\n",
    "        self.cant_visitas = {}\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.politica_exploracion = politica_exploracion\n",
    "        self.aprender_inmediatamente = aprender_inmediatamente\n",
    "\n",
    "        # En caso de que aprenda al finalizar el episodio, se guardan las acciones tomadas\n",
    "        self.acciones_tomadas = []\n",
    "\n",
    "    def elegir_accion(self, estado, max_accion, explorar=True) -> int:\n",
    "        estado = discretize_state(estado, bins)\n",
    "\n",
    "        if estado not in self.Q:\n",
    "            self.Q[estado] = {i: 0 for i in range(max_accion)}\n",
    "            for accion in range(max_accion):\n",
    "                self.cant_visitas[(estado, accion)] = 0\n",
    "\n",
    "        if (estado[6] == 1 and estado[7] == 1):\n",
    "            return 0        \n",
    "\n",
    "        if explorar: # Explorar\n",
    "\n",
    "            if (self.politica_exploracion == 'random'):\n",
    "                return random.randrange(max_accion)\n",
    "            \n",
    "            elif self.politica_exploracion == 'epsilon-greedy':\n",
    "                if random.random() < self.epsilon:\n",
    "                    return max(self.Q[estado], key=self.Q[estado].get)\n",
    "                else:\n",
    "                    return random.randrange(max_accion)\n",
    "                \n",
    "            elif self.politica_exploracion == 'softmax':\n",
    "                # Voy probando varias veces y no funciona :(\n",
    "                '''\n",
    "                q_values = [self.Q[estado][a] for a in range(max_accion)]\n",
    "    \n",
    "                min_q, max_q = min(q_values), max(q_values)\n",
    "                \n",
    "                if max_q - min_q > 0:\n",
    "                    normalized_q_values = [(q - min_q) / (max_q - min_q) for q in q_values]\n",
    "                else:\n",
    "                    normalized_q_values = [0] * len(q_values)  \n",
    "                \n",
    "                k_values = [self.k**q for q in normalized_q_values]\n",
    "                sum_k_values = sum(k_values)\n",
    "                probs = [k / sum_k_values for k in k_values]\n",
    "                \n",
    "                return np.random.choice(range(max_accion), p=probs)\n",
    "                '''\n",
    "\n",
    "                '''\n",
    "                q_values = [self.Q[estado][a] for a in range(max_accion)]\n",
    "                q_values_scaled = [q / self.k for q in q_values]\n",
    "                max_q = max(q_values_scaled)\n",
    "                q_values_scaled = [q - max_q for q in q_values_scaled]\n",
    "                exp_q = [np.exp(q) for q in q_values_scaled]\n",
    "                sum_exp_q = sum(exp_q)\n",
    "                probs = [q / sum_exp_q for q in exp_q]\n",
    "                return np.random.choice(range(max_accion), p=probs)\n",
    "                '''\n",
    "\n",
    "        else: # Explotacion\n",
    "            return max(self.Q[estado], key=self.Q[estado].get)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_actual, accion, recompensa, terminado) -> None:\n",
    "        estado_anterior = discretize_state(estado_anterior, bins)\n",
    "        estado_actual   = discretize_state(estado_actual, bins)\n",
    "\n",
    "        for estado in [estado_anterior, estado_actual]:\n",
    "            if estado not in self.Q:\n",
    "                self.Q[estado] = {i: 0 for i in range(4)}\n",
    "                for a in range(4):\n",
    "                    self.cant_visitas[(estado, a)] = 0\n",
    "            \n",
    "        self.cant_visitas[(estado_anterior, accion)] += 1 \n",
    "\n",
    "        if (self.aprender_inmediatamente):\n",
    "            alfa = 1 / self.cant_visitas[(estado_anterior, accion)]\n",
    "            self.Q[estado_anterior][accion] = (1 - alfa) * self.Q[estado_anterior][accion] + alfa * (recompensa + max(self.Q[estado_actual].values()))\n",
    "        else:\n",
    "            self.acciones_tomadas.append((estado_anterior, estado_actual, accion, recompensa))     \n",
    "\n",
    "\n",
    "    def fin_episodio(self) -> None:\n",
    "        if (not self.aprender_inmediatamente):\n",
    "            for i in range(len(self.acciones_tomadas)-1, -1, -1):\n",
    "                estado_anterior, estado_actual, accion, recompensa = self.acciones_tomadas[i]\n",
    "                alfa = 1 / self.cant_visitas[(estado_anterior, accion)]\n",
    "                self.Q[estado_anterior][accion] = (1 - alfa) * self.Q[estado_anterior][accion] + alfa * (recompensa + max(self.Q[estado_actual].values()))\n",
    "\n",
    "            self.acciones_tomadas = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee51f5-aff8-4d18-934e-92acdcb617c0",
   "metadata": {},
   "source": [
    "Y ejecutar con el muchos episodios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8e0524d5-0d12-46a8-8437-984b981fbae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 0\n",
      "Recompensa parcial promedio: -35.4804810527489\n",
      "\n",
      "Episodio: 100\n",
      "Recompensa parcial promedio: -139.63642831106444\n",
      "\n",
      "Episodio: 200\n",
      "Recompensa parcial promedio: -160.08616365901062\n",
      "\n",
      "Episodio: 300\n",
      "Recompensa parcial promedio: -139.5559725493582\n",
      "\n",
      "Episodio: 400\n",
      "Recompensa parcial promedio: -157.93106005535242\n",
      "\n",
      "Episodio: 500\n",
      "Recompensa parcial promedio: -146.82198516970521\n",
      "\n",
      "Episodio: 600\n",
      "Recompensa parcial promedio: -140.67566501347284\n",
      "\n",
      "Episodio: 700\n",
      "Recompensa parcial promedio: -137.79560103017025\n",
      "\n",
      "Episodio: 800\n",
      "Recompensa parcial promedio: -145.3956364692944\n",
      "\n",
      "Episodio: 900\n",
      "Recompensa parcial promedio: -160.98624834146213\n",
      "\n",
      "Episodio: 1000\n",
      "Recompensa parcial promedio: -146.45731623806216\n",
      "\n",
      "Episodio: 1100\n",
      "Recompensa parcial promedio: -144.13490960765802\n",
      "\n",
      "Episodio: 1200\n",
      "Recompensa parcial promedio: -144.8121581353799\n",
      "\n",
      "Episodio: 1300\n",
      "Recompensa parcial promedio: -148.94858759243573\n",
      "\n",
      "Episodio: 1400\n",
      "Recompensa parcial promedio: -159.16272554582807\n",
      "\n",
      "Episodio: 1500\n",
      "Recompensa parcial promedio: -144.16958791530894\n",
      "\n",
      "Episodio: 1600\n",
      "Recompensa parcial promedio: -150.9694913012176\n",
      "\n",
      "Episodio: 1700\n",
      "Recompensa parcial promedio: -141.44769252256722\n",
      "\n",
      "Episodio: 1800\n",
      "Recompensa parcial promedio: -161.44840559249084\n",
      "\n",
      "Episodio: 1900\n",
      "Recompensa parcial promedio: -146.64792203209907\n",
      "\n",
      "Episodio: 2000\n",
      "Recompensa parcial promedio: -148.05180141263207\n",
      "\n",
      "Episodio: 2100\n",
      "Recompensa parcial promedio: -154.64860711418072\n",
      "\n",
      "Episodio: 2200\n",
      "Recompensa parcial promedio: -143.12774071820616\n",
      "\n",
      "Episodio: 2300\n",
      "Recompensa parcial promedio: -129.8845159647734\n",
      "\n",
      "Episodio: 2400\n",
      "Recompensa parcial promedio: -134.1427280687066\n",
      "\n",
      "Episodio: 2500\n",
      "Recompensa parcial promedio: -143.6034579554705\n",
      "\n",
      "Episodio: 2600\n",
      "Recompensa parcial promedio: -155.1095780932605\n",
      "\n",
      "Episodio: 2700\n",
      "Recompensa parcial promedio: -149.14392841889133\n",
      "\n",
      "Episodio: 2800\n",
      "Recompensa parcial promedio: -144.51224257056572\n",
      "\n",
      "Episodio: 2900\n",
      "Recompensa parcial promedio: -163.26214027690918\n",
      "\n",
      "Episodio: 3000\n",
      "Recompensa parcial promedio: -149.5502719133841\n",
      "\n",
      "Episodio: 3100\n",
      "Recompensa parcial promedio: -158.6651806490395\n",
      "\n",
      "Episodio: 3200\n",
      "Recompensa parcial promedio: -154.32447584877298\n",
      "\n",
      "Episodio: 3300\n",
      "Recompensa parcial promedio: -144.50959807962596\n",
      "\n",
      "Episodio: 3400\n",
      "Recompensa parcial promedio: -139.73002553418672\n",
      "\n",
      "Episodio: 3500\n",
      "Recompensa parcial promedio: -155.14744888370174\n",
      "\n",
      "Episodio: 3600\n",
      "Recompensa parcial promedio: -146.80992142816206\n",
      "\n",
      "Episodio: 3700\n",
      "Recompensa parcial promedio: -140.2701114589712\n",
      "\n",
      "Episodio: 3800\n",
      "Recompensa parcial promedio: -139.62288919296986\n",
      "\n",
      "Episodio: 3900\n",
      "Recompensa parcial promedio: -138.16519732942538\n",
      "\n",
      "Episodio: 4000\n",
      "Recompensa parcial promedio: -145.86060391178628\n",
      "\n",
      "Episodio: 4100\n",
      "Recompensa parcial promedio: -140.85440999328125\n",
      "\n",
      "Episodio: 4200\n",
      "Recompensa parcial promedio: -139.87691508905803\n",
      "\n",
      "Episodio: 4300\n",
      "Recompensa parcial promedio: -149.7467253972089\n",
      "\n",
      "Episodio: 4400\n",
      "Recompensa parcial promedio: -141.4060613054245\n",
      "\n",
      "Episodio: 4500\n",
      "Recompensa parcial promedio: -169.23403741214636\n",
      "\n",
      "Episodio: 4600\n",
      "Recompensa parcial promedio: -128.40873773569837\n",
      "\n",
      "Episodio: 4700\n",
      "Recompensa parcial promedio: -156.13220832435695\n",
      "\n",
      "Episodio: 4800\n",
      "Recompensa parcial promedio: -146.23682733806888\n",
      "\n",
      "Episodio: 4900\n",
      "Recompensa parcial promedio: -155.4873436933909\n",
      "\n",
      "Episodio: 5000\n",
      "Recompensa parcial promedio: -150.3258895382862\n",
      "\n",
      "Episodio: 5100\n",
      "Recompensa parcial promedio: -165.6414844306793\n",
      "\n",
      "Episodio: 5200\n",
      "Recompensa parcial promedio: -136.38512752389295\n",
      "\n",
      "Episodio: 5300\n",
      "Recompensa parcial promedio: -155.44869324454513\n",
      "\n",
      "Episodio: 5400\n",
      "Recompensa parcial promedio: -148.88982032654815\n",
      "\n",
      "Episodio: 5500\n",
      "Recompensa parcial promedio: -155.49262379776516\n",
      "\n",
      "Episodio: 5600\n",
      "Recompensa parcial promedio: -147.06998784448834\n",
      "\n",
      "Episodio: 5700\n",
      "Recompensa parcial promedio: -150.78735137413526\n",
      "\n",
      "Episodio: 5800\n",
      "Recompensa parcial promedio: -146.27042154320813\n",
      "\n",
      "Episodio: 5900\n",
      "Recompensa parcial promedio: -140.14120013552937\n",
      "\n",
      "Episodio: 6000\n",
      "Recompensa parcial promedio: -153.8796395151582\n",
      "\n",
      "Episodio: 6100\n",
      "Recompensa parcial promedio: -160.92495699229556\n",
      "\n",
      "Episodio: 6200\n",
      "Recompensa parcial promedio: -161.8130301360237\n",
      "\n",
      "Episodio: 6300\n",
      "Recompensa parcial promedio: -149.4103030134305\n",
      "\n",
      "Episodio: 6400\n",
      "Recompensa parcial promedio: -141.80604879322823\n",
      "\n",
      "Episodio: 6500\n",
      "Recompensa parcial promedio: -154.21881420655245\n",
      "\n",
      "Episodio: 6600\n",
      "Recompensa parcial promedio: -155.74337943059243\n",
      "\n",
      "Episodio: 6700\n",
      "Recompensa parcial promedio: -140.10472322674653\n",
      "\n",
      "Episodio: 6800\n",
      "Recompensa parcial promedio: -149.7499729307326\n",
      "\n",
      "Episodio: 6900\n",
      "Recompensa parcial promedio: -148.27254913748158\n",
      "\n",
      "Episodio: 7000\n",
      "Recompensa parcial promedio: -141.26019053523075\n",
      "\n",
      "Episodio: 7100\n",
      "Recompensa parcial promedio: -166.2788703378488\n",
      "\n",
      "Episodio: 7200\n",
      "Recompensa parcial promedio: -131.41407199797482\n",
      "\n",
      "Episodio: 7300\n",
      "Recompensa parcial promedio: -135.15370726931948\n",
      "\n",
      "Episodio: 7400\n",
      "Recompensa parcial promedio: -146.25484307191354\n",
      "\n",
      "Episodio: 7500\n",
      "Recompensa parcial promedio: -149.05178184925276\n",
      "\n",
      "Episodio: 7600\n",
      "Recompensa parcial promedio: -149.1422276476153\n",
      "\n",
      "Episodio: 7700\n",
      "Recompensa parcial promedio: -132.45201454254752\n",
      "\n",
      "Episodio: 7800\n",
      "Recompensa parcial promedio: -128.06759152609015\n",
      "\n",
      "Episodio: 7900\n",
      "Recompensa parcial promedio: -147.68787040718797\n",
      "\n",
      "Episodio: 8000\n",
      "Recompensa parcial promedio: -121.70172829290206\n",
      "\n",
      "Episodio: 8100\n",
      "Recompensa parcial promedio: -139.4942557545566\n",
      "\n",
      "Episodio: 8200\n",
      "Recompensa parcial promedio: -161.5172073131452\n",
      "\n",
      "Episodio: 8300\n",
      "Recompensa parcial promedio: -138.8008323275519\n",
      "\n",
      "Episodio: 8400\n",
      "Recompensa parcial promedio: -145.6895822933942\n",
      "\n",
      "Episodio: 8500\n",
      "Recompensa parcial promedio: -143.09390678567877\n",
      "\n",
      "Episodio: 8600\n",
      "Recompensa parcial promedio: -156.9177906382768\n",
      "\n",
      "Episodio: 8700\n",
      "Recompensa parcial promedio: -150.07289756022746\n",
      "\n",
      "Episodio: 8800\n",
      "Recompensa parcial promedio: -137.42749468929912\n",
      "\n",
      "Episodio: 8900\n",
      "Recompensa parcial promedio: -139.4437275745533\n",
      "\n",
      "Episodio: 9000\n",
      "Recompensa parcial promedio: -137.03158253387556\n",
      "\n",
      "Episodio: 9100\n",
      "Recompensa parcial promedio: -136.84386930257438\n",
      "\n",
      "Episodio: 9200\n",
      "Recompensa parcial promedio: -159.68951143866497\n",
      "\n",
      "Episodio: 9300\n",
      "Recompensa parcial promedio: -145.15469098813665\n",
      "\n",
      "Episodio: 9400\n",
      "Recompensa parcial promedio: -138.86379003500736\n",
      "\n",
      "Episodio: 9500\n",
      "Recompensa parcial promedio: -140.5054154170775\n",
      "\n",
      "Episodio: 9600\n",
      "Recompensa parcial promedio: -149.84224246563687\n",
      "\n",
      "Episodio: 9700\n",
      "Recompensa parcial promedio: -149.03784084884893\n",
      "\n",
      "Episodio: 9800\n",
      "Recompensa parcial promedio: -146.89186243550373\n",
      "\n",
      "Episodio: 9900\n",
      "Recompensa parcial promedio: -144.69171226073368\n",
      "\n",
      "Episodio: 10000\n",
      "Recompensa parcial promedio: -141.66485151427264\n",
      "\n",
      "Episodio: 10100\n",
      "Recompensa parcial promedio: -134.96313982283127\n",
      "\n",
      "Episodio: 10200\n",
      "Recompensa parcial promedio: -154.0680463282767\n",
      "\n",
      "Episodio: 10300\n",
      "Recompensa parcial promedio: -144.00184060257038\n",
      "\n",
      "Episodio: 10400\n",
      "Recompensa parcial promedio: -144.07554746312474\n",
      "\n",
      "Episodio: 10500\n",
      "Recompensa parcial promedio: -147.7091720758528\n",
      "\n",
      "Episodio: 10600\n",
      "Recompensa parcial promedio: -140.20591097706733\n",
      "\n",
      "Episodio: 10700\n",
      "Recompensa parcial promedio: -131.85396427289078\n",
      "\n",
      "Episodio: 10800\n",
      "Recompensa parcial promedio: -147.08667166789348\n",
      "\n",
      "Episodio: 10900\n",
      "Recompensa parcial promedio: -141.39023135955097\n",
      "\n",
      "Episodio: 11000\n",
      "Recompensa parcial promedio: -134.14257513786916\n",
      "\n",
      "Episodio: 11100\n",
      "Recompensa parcial promedio: -154.28332929299555\n",
      "\n",
      "Episodio: 11200\n",
      "Recompensa parcial promedio: -150.98997373960182\n",
      "\n",
      "Episodio: 11300\n",
      "Recompensa parcial promedio: -164.8076111783008\n",
      "\n",
      "Episodio: 11400\n",
      "Recompensa parcial promedio: -151.35663110165558\n",
      "\n",
      "Episodio: 11500\n",
      "Recompensa parcial promedio: -143.0769392136931\n",
      "\n",
      "Episodio: 11600\n",
      "Recompensa parcial promedio: -136.3661596910258\n",
      "\n",
      "Episodio: 11700\n",
      "Recompensa parcial promedio: -146.50052545471314\n",
      "\n",
      "Episodio: 11800\n",
      "Recompensa parcial promedio: -142.06345729958738\n",
      "\n",
      "Episodio: 11900\n",
      "Recompensa parcial promedio: -152.46535327253272\n",
      "\n",
      "Episodio: 12000\n",
      "Recompensa parcial promedio: -138.3427277706844\n",
      "\n",
      "Episodio: 12100\n",
      "Recompensa parcial promedio: -140.5870370868559\n",
      "\n",
      "Episodio: 12200\n",
      "Recompensa parcial promedio: -150.28350494921852\n",
      "\n",
      "Episodio: 12300\n",
      "Recompensa parcial promedio: -146.71466717650205\n",
      "\n",
      "Episodio: 12400\n",
      "Recompensa parcial promedio: -145.3935233323142\n",
      "\n",
      "Episodio: 12500\n",
      "Recompensa parcial promedio: -131.00600093949674\n",
      "\n",
      "Episodio: 12600\n",
      "Recompensa parcial promedio: -150.20563554229227\n",
      "\n",
      "Episodio: 12700\n",
      "Recompensa parcial promedio: -150.92785672158664\n",
      "\n",
      "Episodio: 12800\n",
      "Recompensa parcial promedio: -136.34484594838432\n",
      "\n",
      "Episodio: 12900\n",
      "Recompensa parcial promedio: -159.63908318758868\n",
      "\n",
      "Episodio: 13000\n",
      "Recompensa parcial promedio: -152.69489872235323\n",
      "\n",
      "Episodio: 13100\n",
      "Recompensa parcial promedio: -143.9626230735106\n",
      "\n",
      "Episodio: 13200\n",
      "Recompensa parcial promedio: -143.78092289121068\n",
      "\n",
      "Episodio: 13300\n",
      "Recompensa parcial promedio: -143.20321872708067\n",
      "\n",
      "Episodio: 13400\n",
      "Recompensa parcial promedio: -142.1257509089855\n",
      "\n",
      "Episodio: 13500\n",
      "Recompensa parcial promedio: -145.645887015931\n",
      "\n",
      "Episodio: 13600\n",
      "Recompensa parcial promedio: -164.54539956047066\n",
      "\n",
      "Episodio: 13700\n",
      "Recompensa parcial promedio: -129.0830636009224\n",
      "\n",
      "Episodio: 13800\n",
      "Recompensa parcial promedio: -151.81756916670489\n",
      "\n",
      "Episodio: 13900\n",
      "Recompensa parcial promedio: -145.75184724816742\n",
      "\n",
      "Episodio: 14000\n",
      "Recompensa parcial promedio: -141.53956564816883\n",
      "\n",
      "Episodio: 14100\n",
      "Recompensa parcial promedio: -139.52826514740119\n",
      "\n",
      "Episodio: 14200\n",
      "Recompensa parcial promedio: -143.76076216709396\n",
      "\n",
      "Episodio: 14300\n",
      "Recompensa parcial promedio: -148.48979994046624\n",
      "\n",
      "Episodio: 14400\n",
      "Recompensa parcial promedio: -161.08923854121323\n",
      "\n",
      "Episodio: 14500\n",
      "Recompensa parcial promedio: -158.85724087033296\n",
      "\n",
      "Episodio: 14600\n",
      "Recompensa parcial promedio: -136.15911322246623\n",
      "\n",
      "Episodio: 14700\n",
      "Recompensa parcial promedio: -149.0997965437427\n",
      "\n",
      "Episodio: 14800\n",
      "Recompensa parcial promedio: -159.1444467649861\n",
      "\n",
      "Episodio: 14900\n",
      "Recompensa parcial promedio: -135.83973625948704\n",
      "\n",
      "Episodio: 15000\n",
      "Recompensa parcial promedio: -137.38041541126597\n",
      "\n",
      "Episodio: 15100\n",
      "Recompensa parcial promedio: -139.217715301529\n",
      "\n",
      "Episodio: 15200\n",
      "Recompensa parcial promedio: -143.0206372785336\n",
      "\n",
      "Episodio: 15300\n",
      "Recompensa parcial promedio: -154.68090795516417\n",
      "\n",
      "Episodio: 15400\n",
      "Recompensa parcial promedio: -151.72947504492683\n",
      "\n",
      "Episodio: 15500\n",
      "Recompensa parcial promedio: -145.18789276632094\n",
      "\n",
      "Episodio: 15600\n",
      "Recompensa parcial promedio: -145.08182177112099\n",
      "\n",
      "Episodio: 15700\n",
      "Recompensa parcial promedio: -150.42118296469056\n",
      "\n",
      "Episodio: 15800\n",
      "Recompensa parcial promedio: -136.7339234527994\n",
      "\n",
      "Episodio: 15900\n",
      "Recompensa parcial promedio: -145.32931231925312\n",
      "\n",
      "Episodio: 16000\n",
      "Recompensa parcial promedio: -150.27644327462897\n",
      "\n",
      "Episodio: 16100\n",
      "Recompensa parcial promedio: -151.6093982422816\n",
      "\n",
      "Episodio: 16200\n",
      "Recompensa parcial promedio: -156.08752508198052\n",
      "\n",
      "Episodio: 16300\n",
      "Recompensa parcial promedio: -144.40071026200891\n",
      "\n",
      "Episodio: 16400\n",
      "Recompensa parcial promedio: -154.45068881459588\n",
      "\n",
      "Episodio: 16500\n",
      "Recompensa parcial promedio: -145.8454361846133\n",
      "\n",
      "Episodio: 16600\n",
      "Recompensa parcial promedio: -143.6376786643603\n",
      "\n",
      "Episodio: 16700\n",
      "Recompensa parcial promedio: -142.85254418502797\n",
      "\n",
      "Episodio: 16800\n",
      "Recompensa parcial promedio: -153.03595777635633\n",
      "\n",
      "Episodio: 16900\n",
      "Recompensa parcial promedio: -146.5854546914814\n",
      "\n",
      "Episodio: 17000\n",
      "Recompensa parcial promedio: -152.51778469720352\n",
      "\n",
      "Episodio: 17100\n",
      "Recompensa parcial promedio: -144.51407747794096\n",
      "\n",
      "Episodio: 17200\n",
      "Recompensa parcial promedio: -140.01905486621766\n",
      "\n",
      "Episodio: 17300\n",
      "Recompensa parcial promedio: -146.29866707519895\n",
      "\n",
      "Episodio: 17400\n",
      "Recompensa parcial promedio: -140.21295736011595\n",
      "\n",
      "Episodio: 17500\n",
      "Recompensa parcial promedio: -152.63749924176926\n",
      "\n",
      "Episodio: 17600\n",
      "Recompensa parcial promedio: -144.90831455846146\n",
      "\n",
      "Episodio: 17700\n",
      "Recompensa parcial promedio: -123.17202796362385\n",
      "\n",
      "Episodio: 17800\n",
      "Recompensa parcial promedio: -146.64816703304726\n",
      "\n",
      "Episodio: 17900\n",
      "Recompensa parcial promedio: -138.16744431237942\n",
      "\n",
      "Episodio: 18000\n",
      "Recompensa parcial promedio: -149.82439808397584\n",
      "\n",
      "Episodio: 18100\n",
      "Recompensa parcial promedio: -149.27285222235668\n",
      "\n",
      "Episodio: 18200\n",
      "Recompensa parcial promedio: -146.46522649182026\n",
      "\n",
      "Episodio: 18300\n",
      "Recompensa parcial promedio: -142.35375968554655\n",
      "\n",
      "Episodio: 18400\n",
      "Recompensa parcial promedio: -147.00383304398827\n",
      "\n",
      "Episodio: 18500\n",
      "Recompensa parcial promedio: -144.89718017586475\n",
      "\n",
      "Episodio: 18600\n",
      "Recompensa parcial promedio: -150.22822450267694\n",
      "\n",
      "Episodio: 18700\n",
      "Recompensa parcial promedio: -150.04022195672505\n",
      "\n",
      "Episodio: 18800\n",
      "Recompensa parcial promedio: -149.93576001475515\n",
      "\n",
      "Episodio: 18900\n",
      "Recompensa parcial promedio: -140.39061458456285\n",
      "\n",
      "Episodio: 19000\n",
      "Recompensa parcial promedio: -137.1102332208387\n",
      "\n",
      "Episodio: 19100\n",
      "Recompensa parcial promedio: -161.65617130373602\n",
      "\n",
      "Episodio: 19200\n",
      "Recompensa parcial promedio: -142.6628744520591\n",
      "\n",
      "Episodio: 19300\n",
      "Recompensa parcial promedio: -143.6897159397371\n",
      "\n",
      "Episodio: 19400\n",
      "Recompensa parcial promedio: -138.75852449509057\n",
      "\n",
      "Episodio: 19500\n",
      "Recompensa parcial promedio: -140.65501191648573\n",
      "\n",
      "Episodio: 19600\n",
      "Recompensa parcial promedio: -149.11528920628967\n",
      "\n",
      "Episodio: 19700\n",
      "Recompensa parcial promedio: -151.51544049266386\n",
      "\n",
      "Episodio: 19800\n",
      "Recompensa parcial promedio: -146.17216853204823\n",
      "\n",
      "Episodio: 19900\n",
      "Recompensa parcial promedio: -150.76167145585472\n",
      "\n",
      "Episodio: 20000\n",
      "Recompensa parcial promedio: -153.88827647493045\n",
      "\n",
      "Episodio: 20100\n",
      "Recompensa parcial promedio: -155.3978563879782\n",
      "\n",
      "Episodio: 20200\n",
      "Recompensa parcial promedio: -140.74111490554623\n",
      "\n",
      "Episodio: 20300\n",
      "Recompensa parcial promedio: -155.39540260299722\n",
      "\n",
      "Episodio: 20400\n",
      "Recompensa parcial promedio: -159.02658970435883\n",
      "\n",
      "Episodio: 20500\n",
      "Recompensa parcial promedio: -144.1286602658283\n",
      "\n",
      "Episodio: 20600\n",
      "Recompensa parcial promedio: -144.32410364145198\n",
      "\n",
      "Episodio: 20700\n",
      "Recompensa parcial promedio: -141.58602856406355\n",
      "\n",
      "Episodio: 20800\n",
      "Recompensa parcial promedio: -140.84381441419094\n",
      "\n",
      "Episodio: 20900\n",
      "Recompensa parcial promedio: -146.1324128786113\n",
      "\n",
      "Episodio: 21000\n",
      "Recompensa parcial promedio: -145.7927590314551\n",
      "\n",
      "Episodio: 21100\n",
      "Recompensa parcial promedio: -143.40813423262276\n",
      "\n",
      "Episodio: 21200\n",
      "Recompensa parcial promedio: -141.9959909948544\n",
      "\n",
      "Episodio: 21300\n",
      "Recompensa parcial promedio: -141.80430381845838\n",
      "\n",
      "Episodio: 21400\n",
      "Recompensa parcial promedio: -140.36165064508657\n",
      "\n",
      "Episodio: 21500\n",
      "Recompensa parcial promedio: -153.50732774902144\n",
      "\n",
      "Episodio: 21600\n",
      "Recompensa parcial promedio: -153.5690669617779\n",
      "\n",
      "Episodio: 21700\n",
      "Recompensa parcial promedio: -146.3706208125106\n",
      "\n",
      "Episodio: 21800\n",
      "Recompensa parcial promedio: -138.21007964960208\n",
      "\n",
      "Episodio: 21900\n",
      "Recompensa parcial promedio: -134.54193557243693\n",
      "\n",
      "Episodio: 22000\n",
      "Recompensa parcial promedio: -155.33003787502184\n",
      "\n",
      "Episodio: 22100\n",
      "Recompensa parcial promedio: -143.99040124589374\n",
      "\n",
      "Episodio: 22200\n",
      "Recompensa parcial promedio: -147.5573600549761\n",
      "\n",
      "Episodio: 22300\n",
      "Recompensa parcial promedio: -138.59606379156872\n",
      "\n",
      "Episodio: 22400\n",
      "Recompensa parcial promedio: -140.1182356364713\n",
      "\n",
      "Episodio: 22500\n",
      "Recompensa parcial promedio: -130.2672676170594\n",
      "\n",
      "Episodio: 22600\n",
      "Recompensa parcial promedio: -154.68202167343583\n",
      "\n",
      "Episodio: 22700\n",
      "Recompensa parcial promedio: -141.39363994225243\n",
      "\n",
      "Episodio: 22800\n",
      "Recompensa parcial promedio: -132.16942694526196\n",
      "\n",
      "Episodio: 22900\n",
      "Recompensa parcial promedio: -149.19931311565833\n",
      "\n",
      "Episodio: 23000\n",
      "Recompensa parcial promedio: -149.000667262289\n",
      "\n",
      "Episodio: 23100\n",
      "Recompensa parcial promedio: -156.43352099024776\n",
      "\n",
      "Episodio: 23200\n",
      "Recompensa parcial promedio: -149.75492801796128\n",
      "\n",
      "Episodio: 23300\n",
      "Recompensa parcial promedio: -152.64541104455668\n",
      "\n",
      "Episodio: 23400\n",
      "Recompensa parcial promedio: -133.91575383451578\n",
      "\n",
      "Episodio: 23500\n",
      "Recompensa parcial promedio: -147.12456566592502\n",
      "\n",
      "Episodio: 23600\n",
      "Recompensa parcial promedio: -163.33697355799006\n",
      "\n",
      "Episodio: 23700\n",
      "Recompensa parcial promedio: -153.14763948288532\n",
      "\n",
      "Episodio: 23800\n",
      "Recompensa parcial promedio: -145.15269451738945\n",
      "\n",
      "Episodio: 23900\n",
      "Recompensa parcial promedio: -145.3088201662851\n",
      "\n",
      "Episodio: 24000\n",
      "Recompensa parcial promedio: -134.51447904767517\n",
      "\n",
      "Episodio: 24100\n",
      "Recompensa parcial promedio: -149.46708646309148\n",
      "\n",
      "Episodio: 24200\n",
      "Recompensa parcial promedio: -153.02856229989223\n",
      "\n",
      "Episodio: 24300\n",
      "Recompensa parcial promedio: -148.26351372382712\n",
      "\n",
      "Episodio: 24400\n",
      "Recompensa parcial promedio: -150.90186723258222\n",
      "\n",
      "Episodio: 24500\n",
      "Recompensa parcial promedio: -142.66982493590496\n",
      "\n",
      "Episodio: 24600\n",
      "Recompensa parcial promedio: -133.93841953497034\n",
      "\n",
      "Episodio: 24700\n",
      "Recompensa parcial promedio: -146.54958850765905\n",
      "\n",
      "Episodio: 24800\n",
      "Recompensa parcial promedio: -142.70118503892303\n",
      "\n",
      "Episodio: 24900\n",
      "Recompensa parcial promedio: -160.05527116451873\n",
      "\n",
      "Episodio: 25000\n",
      "Recompensa parcial promedio: -155.77719101421565\n",
      "\n",
      "Episodio: 25100\n",
      "Recompensa parcial promedio: -161.53548388785717\n",
      "\n",
      "Episodio: 25200\n",
      "Recompensa parcial promedio: -146.78236392198994\n",
      "\n",
      "Episodio: 25300\n",
      "Recompensa parcial promedio: -155.38789081870627\n",
      "\n",
      "Episodio: 25400\n",
      "Recompensa parcial promedio: -137.94898228957564\n",
      "\n",
      "Episodio: 25500\n",
      "Recompensa parcial promedio: -146.0760315347333\n",
      "\n",
      "Episodio: 25600\n",
      "Recompensa parcial promedio: -142.41197882173913\n",
      "\n",
      "Episodio: 25700\n",
      "Recompensa parcial promedio: -149.51948069460965\n",
      "\n",
      "Episodio: 25800\n",
      "Recompensa parcial promedio: -144.5180015770643\n",
      "\n",
      "Episodio: 25900\n",
      "Recompensa parcial promedio: -148.85594525231215\n",
      "\n",
      "Episodio: 26000\n",
      "Recompensa parcial promedio: -153.20063551498916\n",
      "\n",
      "Episodio: 26100\n",
      "Recompensa parcial promedio: -141.71572677799438\n",
      "\n",
      "Episodio: 26200\n",
      "Recompensa parcial promedio: -148.34714957850107\n",
      "\n",
      "Episodio: 26300\n",
      "Recompensa parcial promedio: -141.0207642605476\n",
      "\n",
      "Episodio: 26400\n",
      "Recompensa parcial promedio: -143.20547523707882\n",
      "\n",
      "Episodio: 26500\n",
      "Recompensa parcial promedio: -161.27270315342219\n",
      "\n",
      "Episodio: 26600\n",
      "Recompensa parcial promedio: -147.34193800481714\n",
      "\n",
      "Episodio: 26700\n",
      "Recompensa parcial promedio: -147.3476370381105\n",
      "\n",
      "Episodio: 26800\n",
      "Recompensa parcial promedio: -151.9807332349494\n",
      "\n",
      "Episodio: 26900\n",
      "Recompensa parcial promedio: -158.4192477985962\n",
      "\n",
      "Episodio: 27000\n",
      "Recompensa parcial promedio: -143.78310848492092\n",
      "\n",
      "Episodio: 27100\n",
      "Recompensa parcial promedio: -139.54257977318105\n",
      "\n",
      "Episodio: 27200\n",
      "Recompensa parcial promedio: -149.47037605231563\n",
      "\n",
      "Episodio: 27300\n",
      "Recompensa parcial promedio: -145.67644579189894\n",
      "\n",
      "Episodio: 27400\n",
      "Recompensa parcial promedio: -168.02980018135113\n",
      "\n",
      "Episodio: 27500\n",
      "Recompensa parcial promedio: -144.27621323120388\n",
      "\n",
      "Episodio: 27600\n",
      "Recompensa parcial promedio: -158.9762637557791\n",
      "\n",
      "Episodio: 27700\n",
      "Recompensa parcial promedio: -147.3773123303849\n",
      "\n",
      "Episodio: 27800\n",
      "Recompensa parcial promedio: -146.4556930402105\n",
      "\n",
      "Episodio: 27900\n",
      "Recompensa parcial promedio: -162.50841184138767\n",
      "\n",
      "Episodio: 28000\n",
      "Recompensa parcial promedio: -150.0664423798055\n",
      "\n",
      "Episodio: 28100\n",
      "Recompensa parcial promedio: -144.57517529089725\n",
      "\n",
      "Episodio: 28200\n",
      "Recompensa parcial promedio: -159.70264476087448\n",
      "\n",
      "Episodio: 28300\n",
      "Recompensa parcial promedio: -146.67599549322324\n",
      "\n",
      "Episodio: 28400\n",
      "Recompensa parcial promedio: -155.34141223171295\n",
      "\n",
      "Episodio: 28500\n",
      "Recompensa parcial promedio: -148.75861661723715\n",
      "\n",
      "Episodio: 28600\n",
      "Recompensa parcial promedio: -149.29680120139565\n",
      "\n",
      "Episodio: 28700\n",
      "Recompensa parcial promedio: -148.3034766269156\n",
      "\n",
      "Episodio: 28800\n",
      "Recompensa parcial promedio: -132.4116981385068\n",
      "\n",
      "Episodio: 28900\n",
      "Recompensa parcial promedio: -137.35794694375826\n",
      "\n",
      "Episodio: 29000\n",
      "Recompensa parcial promedio: -136.81243609497952\n",
      "\n",
      "Episodio: 29100\n",
      "Recompensa parcial promedio: -143.9233388567545\n",
      "\n",
      "Episodio: 29200\n",
      "Recompensa parcial promedio: -138.83382825505592\n",
      "\n",
      "Episodio: 29300\n",
      "Recompensa parcial promedio: -148.44843137099244\n",
      "\n",
      "Episodio: 29400\n",
      "Recompensa parcial promedio: -148.78640103991344\n",
      "\n",
      "Episodio: 29500\n",
      "Recompensa parcial promedio: -141.81721236207852\n",
      "\n",
      "Episodio: 29600\n",
      "Recompensa parcial promedio: -157.01702648238\n",
      "\n",
      "Episodio: 29700\n",
      "Recompensa parcial promedio: -157.24153594028482\n",
      "\n",
      "Episodio: 29800\n",
      "Recompensa parcial promedio: -156.49958317382485\n",
      "\n",
      "Episodio: 29900\n",
      "Recompensa parcial promedio: -142.15541602042185\n",
      "\n",
      "Episodio: 30000\n",
      "Recompensa parcial promedio: -130.04450273857336\n",
      "\n",
      "Episodio: 30100\n",
      "Recompensa parcial promedio: -154.56381106840843\n",
      "\n",
      "Episodio: 30200\n",
      "Recompensa parcial promedio: -146.9747359994696\n",
      "\n",
      "Episodio: 30300\n",
      "Recompensa parcial promedio: -151.2397619128817\n",
      "\n",
      "Episodio: 30400\n",
      "Recompensa parcial promedio: -135.8637076793377\n",
      "\n",
      "Episodio: 30500\n",
      "Recompensa parcial promedio: -127.97142556604562\n",
      "\n",
      "Episodio: 30600\n",
      "Recompensa parcial promedio: -151.48119262952235\n",
      "\n",
      "Episodio: 30700\n",
      "Recompensa parcial promedio: -175.0204949090952\n",
      "\n",
      "Episodio: 30800\n",
      "Recompensa parcial promedio: -137.9509846216422\n",
      "\n",
      "Episodio: 30900\n",
      "Recompensa parcial promedio: -147.50341584073846\n",
      "\n",
      "Episodio: 31000\n",
      "Recompensa parcial promedio: -155.77390649446173\n",
      "\n",
      "Episodio: 31100\n",
      "Recompensa parcial promedio: -136.73278518775075\n",
      "\n",
      "Episodio: 31200\n",
      "Recompensa parcial promedio: -154.8730864597619\n",
      "\n",
      "Episodio: 31300\n",
      "Recompensa parcial promedio: -149.27528947278523\n",
      "\n",
      "Episodio: 31400\n",
      "Recompensa parcial promedio: -134.02478108547734\n",
      "\n",
      "Episodio: 31500\n",
      "Recompensa parcial promedio: -175.76650979764648\n",
      "\n",
      "Episodio: 31600\n",
      "Recompensa parcial promedio: -145.78617660153085\n",
      "\n",
      "Episodio: 31700\n",
      "Recompensa parcial promedio: -154.52881669689376\n",
      "\n",
      "Episodio: 31800\n",
      "Recompensa parcial promedio: -151.56851369610723\n",
      "\n",
      "Episodio: 31900\n",
      "Recompensa parcial promedio: -142.9024436407245\n",
      "\n",
      "Episodio: 32000\n",
      "Recompensa parcial promedio: -155.62897599318686\n",
      "\n",
      "Episodio: 32100\n",
      "Recompensa parcial promedio: -168.44175796407086\n",
      "\n",
      "Episodio: 32200\n",
      "Recompensa parcial promedio: -127.52045450578782\n",
      "\n",
      "Episodio: 32300\n",
      "Recompensa parcial promedio: -156.41787180352108\n",
      "\n",
      "Episodio: 32400\n",
      "Recompensa parcial promedio: -156.28464118656942\n",
      "\n",
      "Episodio: 32500\n",
      "Recompensa parcial promedio: -136.06984029316894\n",
      "\n",
      "Episodio: 32600\n",
      "Recompensa parcial promedio: -156.4929307720398\n",
      "\n",
      "Episodio: 32700\n",
      "Recompensa parcial promedio: -143.35147849810798\n",
      "\n",
      "Episodio: 32800\n",
      "Recompensa parcial promedio: -145.6290890805562\n",
      "\n",
      "Episodio: 32900\n",
      "Recompensa parcial promedio: -131.65800274081653\n",
      "\n",
      "Episodio: 33000\n",
      "Recompensa parcial promedio: -150.97940392575998\n",
      "\n",
      "Episodio: 33100\n",
      "Recompensa parcial promedio: -136.53508097134483\n",
      "\n",
      "Episodio: 33200\n",
      "Recompensa parcial promedio: -153.1212289307194\n",
      "\n",
      "Episodio: 33300\n",
      "Recompensa parcial promedio: -147.95544167584015\n",
      "\n",
      "Episodio: 33400\n",
      "Recompensa parcial promedio: -155.80832782934442\n",
      "\n",
      "Episodio: 33500\n",
      "Recompensa parcial promedio: -158.2335624426282\n",
      "\n",
      "Episodio: 33600\n",
      "Recompensa parcial promedio: -148.42065120149098\n",
      "\n",
      "Episodio: 33700\n",
      "Recompensa parcial promedio: -132.57519162785763\n",
      "\n",
      "Episodio: 33800\n",
      "Recompensa parcial promedio: -153.62062689417232\n",
      "\n",
      "Episodio: 33900\n",
      "Recompensa parcial promedio: -160.87963475369546\n",
      "\n",
      "Episodio: 34000\n",
      "Recompensa parcial promedio: -146.02141946540857\n",
      "\n",
      "Episodio: 34100\n",
      "Recompensa parcial promedio: -141.6498223948609\n",
      "\n",
      "Episodio: 34200\n",
      "Recompensa parcial promedio: -153.5667695384812\n",
      "\n",
      "Episodio: 34300\n",
      "Recompensa parcial promedio: -146.73411773487697\n",
      "\n",
      "Episodio: 34400\n",
      "Recompensa parcial promedio: -145.71846288841974\n",
      "\n",
      "Episodio: 34500\n",
      "Recompensa parcial promedio: -137.25925842979396\n",
      "\n",
      "Episodio: 34600\n",
      "Recompensa parcial promedio: -145.5354915310335\n",
      "\n",
      "Episodio: 34700\n",
      "Recompensa parcial promedio: -136.72822347571184\n",
      "\n",
      "Episodio: 34800\n",
      "Recompensa parcial promedio: -135.85894232986985\n",
      "\n",
      "Episodio: 34900\n",
      "Recompensa parcial promedio: -155.06597799310012\n",
      "\n",
      "Episodio: 35000\n",
      "Recompensa parcial promedio: -153.05435205146424\n",
      "\n",
      "Episodio: 35100\n",
      "Recompensa parcial promedio: -156.21881268178606\n",
      "\n",
      "Episodio: 35200\n",
      "Recompensa parcial promedio: -129.2123107609649\n",
      "\n",
      "Episodio: 35300\n",
      "Recompensa parcial promedio: -140.68297439907178\n",
      "\n",
      "Episodio: 35400\n",
      "Recompensa parcial promedio: -135.33344388347558\n",
      "\n",
      "Episodio: 35500\n",
      "Recompensa parcial promedio: -138.31675126381862\n",
      "\n",
      "Episodio: 35600\n",
      "Recompensa parcial promedio: -144.83823078409824\n",
      "\n",
      "Episodio: 35700\n",
      "Recompensa parcial promedio: -141.278656332375\n",
      "\n",
      "Episodio: 35800\n",
      "Recompensa parcial promedio: -149.7251888193255\n",
      "\n",
      "Episodio: 35900\n",
      "Recompensa parcial promedio: -150.77923556286453\n",
      "\n",
      "Episodio: 36000\n",
      "Recompensa parcial promedio: -150.2030126221003\n",
      "\n",
      "Episodio: 36100\n",
      "Recompensa parcial promedio: -136.53233468381632\n",
      "\n",
      "Episodio: 36200\n",
      "Recompensa parcial promedio: -153.09596691177777\n",
      "\n",
      "Episodio: 36300\n",
      "Recompensa parcial promedio: -162.16270136066748\n",
      "\n",
      "Episodio: 36400\n",
      "Recompensa parcial promedio: -148.49898790098285\n",
      "\n",
      "Episodio: 36500\n",
      "Recompensa parcial promedio: -146.54289977744577\n",
      "\n",
      "Episodio: 36600\n",
      "Recompensa parcial promedio: -157.41916295423965\n",
      "\n",
      "Episodio: 36700\n",
      "Recompensa parcial promedio: -140.2879963536708\n",
      "\n",
      "Episodio: 36800\n",
      "Recompensa parcial promedio: -142.35163135384673\n",
      "\n",
      "Episodio: 36900\n",
      "Recompensa parcial promedio: -147.56866943350983\n",
      "\n",
      "Episodio: 37000\n",
      "Recompensa parcial promedio: -147.71660359003494\n",
      "\n",
      "Episodio: 37100\n",
      "Recompensa parcial promedio: -152.15020904385352\n",
      "\n",
      "Episodio: 37200\n",
      "Recompensa parcial promedio: -146.7717788915773\n",
      "\n",
      "Episodio: 37300\n",
      "Recompensa parcial promedio: -147.8863452211824\n",
      "\n",
      "Episodio: 37400\n",
      "Recompensa parcial promedio: -132.93229228639464\n",
      "\n",
      "Episodio: 37500\n",
      "Recompensa parcial promedio: -142.77176428950762\n",
      "\n",
      "Episodio: 37600\n",
      "Recompensa parcial promedio: -140.5431566595115\n",
      "\n",
      "Episodio: 37700\n",
      "Recompensa parcial promedio: -151.89644738586927\n",
      "\n",
      "Episodio: 37800\n",
      "Recompensa parcial promedio: -146.0161003651323\n",
      "\n",
      "Episodio: 37900\n",
      "Recompensa parcial promedio: -149.8131630657251\n",
      "\n",
      "Episodio: 38000\n",
      "Recompensa parcial promedio: -146.6185028449189\n",
      "\n",
      "Episodio: 38100\n",
      "Recompensa parcial promedio: -147.09123646744413\n",
      "\n",
      "Episodio: 38200\n",
      "Recompensa parcial promedio: -147.9346211599582\n",
      "\n",
      "Episodio: 38300\n",
      "Recompensa parcial promedio: -135.61638195033416\n",
      "\n",
      "Episodio: 38400\n",
      "Recompensa parcial promedio: -162.31786818360186\n",
      "\n",
      "Episodio: 38500\n",
      "Recompensa parcial promedio: -154.19602787701263\n",
      "\n",
      "Episodio: 38600\n",
      "Recompensa parcial promedio: -163.1794937588847\n",
      "\n",
      "Episodio: 38700\n",
      "Recompensa parcial promedio: -159.30317760116145\n",
      "\n",
      "Episodio: 38800\n",
      "Recompensa parcial promedio: -135.57612755929608\n",
      "\n",
      "Episodio: 38900\n",
      "Recompensa parcial promedio: -139.6103108467905\n",
      "\n",
      "Episodio: 39000\n",
      "Recompensa parcial promedio: -149.13392115331848\n",
      "\n",
      "Episodio: 39100\n",
      "Recompensa parcial promedio: -152.77967442463594\n",
      "\n",
      "Episodio: 39200\n",
      "Recompensa parcial promedio: -146.33813067485164\n",
      "\n",
      "Episodio: 39300\n",
      "Recompensa parcial promedio: -138.92588917149152\n",
      "\n",
      "Episodio: 39400\n",
      "Recompensa parcial promedio: -155.94431559950905\n",
      "\n",
      "Episodio: 39500\n",
      "Recompensa parcial promedio: -145.83376468428096\n",
      "\n",
      "Episodio: 39600\n",
      "Recompensa parcial promedio: -156.52422838327684\n",
      "\n",
      "Episodio: 39700\n",
      "Recompensa parcial promedio: -146.58728448142622\n",
      "\n",
      "Episodio: 39800\n",
      "Recompensa parcial promedio: -165.4662730375751\n",
      "\n",
      "Episodio: 39900\n",
      "Recompensa parcial promedio: -152.89125937543068\n",
      "\n",
      "Episodio: 40000\n",
      "Recompensa parcial promedio: -156.53422807472762\n",
      "\n",
      "Episodio: 40100\n",
      "Recompensa parcial promedio: -121.20304616500044\n",
      "\n",
      "Episodio: 40200\n",
      "Recompensa parcial promedio: -151.0718543998434\n",
      "\n",
      "Episodio: 40300\n",
      "Recompensa parcial promedio: -140.11113524183207\n",
      "\n",
      "Episodio: 40400\n",
      "Recompensa parcial promedio: -147.7618839807853\n",
      "\n",
      "Episodio: 40500\n",
      "Recompensa parcial promedio: -134.63358015682113\n",
      "\n",
      "Episodio: 40600\n",
      "Recompensa parcial promedio: -140.21786054465946\n",
      "\n",
      "Episodio: 40700\n",
      "Recompensa parcial promedio: -153.08837889124612\n",
      "\n",
      "Episodio: 40800\n",
      "Recompensa parcial promedio: -155.0871030803002\n",
      "\n",
      "Episodio: 40900\n",
      "Recompensa parcial promedio: -139.45461697413765\n",
      "\n",
      "Episodio: 41000\n",
      "Recompensa parcial promedio: -154.26590452306738\n",
      "\n",
      "Episodio: 41100\n",
      "Recompensa parcial promedio: -135.5477237265964\n",
      "\n",
      "Episodio: 41200\n",
      "Recompensa parcial promedio: -149.4214292938252\n",
      "\n",
      "Episodio: 41300\n",
      "Recompensa parcial promedio: -149.90377857668716\n",
      "\n",
      "Episodio: 41400\n",
      "Recompensa parcial promedio: -154.07658364512568\n",
      "\n",
      "Episodio: 41500\n",
      "Recompensa parcial promedio: -148.03489800053913\n",
      "\n",
      "Episodio: 41600\n",
      "Recompensa parcial promedio: -144.74374836840846\n",
      "\n",
      "Episodio: 41700\n",
      "Recompensa parcial promedio: -137.8551926223399\n",
      "\n",
      "Episodio: 41800\n",
      "Recompensa parcial promedio: -148.22062824986216\n",
      "\n",
      "Episodio: 41900\n",
      "Recompensa parcial promedio: -137.73567985700024\n",
      "\n",
      "Episodio: 42000\n",
      "Recompensa parcial promedio: -143.96532902035946\n",
      "\n",
      "Episodio: 42100\n",
      "Recompensa parcial promedio: -136.91736536475182\n",
      "\n",
      "Episodio: 42200\n",
      "Recompensa parcial promedio: -142.48953084818845\n",
      "\n",
      "Episodio: 42300\n",
      "Recompensa parcial promedio: -145.20850386152054\n",
      "\n",
      "Episodio: 42400\n",
      "Recompensa parcial promedio: -144.70210387909623\n",
      "\n",
      "Episodio: 42500\n",
      "Recompensa parcial promedio: -131.95432310204967\n",
      "\n",
      "Episodio: 42600\n",
      "Recompensa parcial promedio: -137.1818459148204\n",
      "\n",
      "Episodio: 42700\n",
      "Recompensa parcial promedio: -139.5684736943717\n",
      "\n",
      "Episodio: 42800\n",
      "Recompensa parcial promedio: -161.07579704488845\n",
      "\n",
      "Episodio: 42900\n",
      "Recompensa parcial promedio: -147.9831638730713\n",
      "\n",
      "Episodio: 43000\n",
      "Recompensa parcial promedio: -146.62546283964858\n",
      "\n",
      "Episodio: 43100\n",
      "Recompensa parcial promedio: -148.15656485385585\n",
      "\n",
      "Episodio: 43200\n",
      "Recompensa parcial promedio: -132.75808090415248\n",
      "\n",
      "Episodio: 43300\n",
      "Recompensa parcial promedio: -152.54255007460242\n",
      "\n",
      "Episodio: 43400\n",
      "Recompensa parcial promedio: -130.7469408942393\n",
      "\n",
      "Episodio: 43500\n",
      "Recompensa parcial promedio: -147.64570542299097\n",
      "\n",
      "Episodio: 43600\n",
      "Recompensa parcial promedio: -150.53037456393523\n",
      "\n",
      "Episodio: 43700\n",
      "Recompensa parcial promedio: -139.38571657868263\n",
      "\n",
      "Episodio: 43800\n",
      "Recompensa parcial promedio: -149.41954229917164\n",
      "\n",
      "Episodio: 43900\n",
      "Recompensa parcial promedio: -156.1792108971137\n",
      "\n",
      "Episodio: 44000\n",
      "Recompensa parcial promedio: -148.506145592377\n",
      "\n",
      "Episodio: 44100\n",
      "Recompensa parcial promedio: -152.26083437065887\n",
      "\n",
      "Episodio: 44200\n",
      "Recompensa parcial promedio: -145.6951334476464\n",
      "\n",
      "Episodio: 44300\n",
      "Recompensa parcial promedio: -143.24889060413477\n",
      "\n",
      "Episodio: 44400\n",
      "Recompensa parcial promedio: -143.30876656883362\n",
      "\n",
      "Episodio: 44500\n",
      "Recompensa parcial promedio: -145.62153537736617\n",
      "\n",
      "Episodio: 44600\n",
      "Recompensa parcial promedio: -148.32399364663092\n",
      "\n",
      "Episodio: 44700\n",
      "Recompensa parcial promedio: -162.17512960444753\n",
      "\n",
      "Episodio: 44800\n",
      "Recompensa parcial promedio: -141.48876260683818\n",
      "\n",
      "Episodio: 44900\n",
      "Recompensa parcial promedio: -139.43066170523954\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m num_episodios \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200000\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodios):\n\u001b[1;32m----> 8\u001b[0m     recompensa \u001b[38;5;241m=\u001b[39m \u001b[43mejecutar_episodio\u001b[49m\u001b[43m(\u001b[49m\u001b[43magente\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maprender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (recompensa \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m):\n",
      "Cell \u001b[1;32mIn[18], line 17\u001b[0m, in \u001b[0;36mejecutar_episodio\u001b[1;34m(agente, aprender, render, max_iteraciones)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Le informamos al agente para que aprenda\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (aprender):\n\u001b[1;32m---> 17\u001b[0m     \u001b[43magente\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maprender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestado_anterior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestado_siguiente\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecompensa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtermino\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m estado_anterior \u001b[38;5;241m=\u001b[39m estado_siguiente\n\u001b[0;32m     20\u001b[0m iteraciones \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[19], line 88\u001b[0m, in \u001b[0;36mAgenteRL.aprender\u001b[1;34m(self, estado_anterior, estado_actual, accion, recompensa, terminado)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maprender\u001b[39m(\u001b[38;5;28mself\u001b[39m, estado_anterior, estado_actual, accion, recompensa, terminado) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m     estado_anterior \u001b[38;5;241m=\u001b[39m \u001b[43mdiscretize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestado_anterior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     estado_actual   \u001b[38;5;241m=\u001b[39m discretize_state(estado_actual, bins)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m estado \u001b[38;5;129;01min\u001b[39;00m [estado_anterior, estado_actual]:\n",
      "Cell \u001b[1;32mIn[2], line 33\u001b[0m, in \u001b[0;36mdiscretize_state\u001b[1;34m(state, bins)\u001b[0m\n\u001b[0;32m     30\u001b[0m         state_disc\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mint\u001b[39m(state[i]))\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;66;03m# Encuentra el índice del valor más cercano en los bins\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m         closest_index \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbins\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m         state_disc\u001b[38;5;241m.\u001b[39mappend(closest_index)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(state_disc)\n",
      "File \u001b[1;32mc:\\Users\\nicoa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1325\u001b[0m, in \u001b[0;36margmin\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;124;03mReturns the indices of the minimum values along an axis.\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmin\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\nicoa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "entorno = gym.make('LunarLander-v2').env\n",
    "agente = AgenteRL(politica_exploracion='epsilon-greedy', k=0.9, aprender_inmediatamente=False)\n",
    "exitos = 0\n",
    "recompensa_parcial=[]\n",
    "recompensa_episodios = []\n",
    "num_episodios = 200000\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio(agente, aprender=True)\n",
    "    \n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        exitos += 1\n",
    "    recompensa_episodios += [recompensa]\n",
    "    recompensa_parcial += [recompensa]\n",
    "    \n",
    "    if (i % 100 == 0):\n",
    "        print(f'Episodio: {i}')\n",
    "        print(f'Recompensa parcial promedio: {np.mean(recompensa_parcial)}')\n",
    "        print('')\n",
    "        recompensa_parcial = []\n",
    "    \n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {np.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e7b346-6f23-44e2-a645-e4548ab470ef",
   "metadata": {},
   "source": [
    "Analizar los resultados de la ejecución anterior, incluyendo:\n",
    " * Un análisis de los parámetros utilizados en el algoritmo (aprendizaje, política de exploración)\n",
    " * Un análisis de algunos 'cortes' de la matriz Q y la política (p.e. qué hace la nave cuando está cayendo rápidamente hacia abajo, sin rotación)\n",
    " * Un análisis de la evolución de la recompensa promedio\n",
    " * Un análisis de los casos de éxito\n",
    " * Un análisis de los casos en el que el agente falla\n",
    " * Qué limitante del agente de RL les parece que afecta más negativamente su desempeño. Cómo lo mejorarían? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f93fe904-e691-42ff-8fd4-b360d08431cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 1\n",
      "Recompensa: 262.3587855844662\n",
      "Recompensa promedio: -81.10348401708532\n",
      "\n",
      "Episodio: 11\n",
      "Recompensa: 284.5717902096558\n",
      "Recompensa promedio: -86.40504425647619\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m num_episodios \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodios):\n\u001b[1;32m----> 6\u001b[0m     recompensa \u001b[38;5;241m=\u001b[39m \u001b[43mejecutar_episodio\u001b[49m\u001b[43m(\u001b[49m\u001b[43magente\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maprender\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (recompensa \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m):\n",
      "Cell \u001b[1;32mIn[18], line 14\u001b[0m, in \u001b[0;36mejecutar_episodio\u001b[1;34m(agente, aprender, render, max_iteraciones)\u001b[0m\n\u001b[0;32m     12\u001b[0m accion \u001b[38;5;241m=\u001b[39m agente\u001b[38;5;241m.\u001b[39melegir_accion(estado_anterior, entorno\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn, aprender)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Realizamos la accion\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m estado_siguiente, recompensa, termino, truncado, info \u001b[38;5;241m=\u001b[39m \u001b[43mentorno\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Le informamos al agente para que aprenda\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (aprender):\n",
      "File \u001b[1;32mc:\\Users\\nicoa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicoa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicoa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:675\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    672\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 675\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32mc:\\Users\\nicoa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:787\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    786\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    788\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "entorno = gym.make('LunarLander-v2').env\n",
    "exitos = 0\n",
    "recompensa_episodios = []\n",
    "num_episodios = 1000\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio(agente, aprender = False, render='human')\n",
    "    \n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        exitos += 1\n",
    "        print(f'Episodio: {i}')\n",
    "        print(f'Recompensa: {recompensa}')\n",
    "        print(f'Recompensa promedio: {np.mean(recompensa_episodios)}')\n",
    "        print('')\n",
    "    recompensa_episodios += [recompensa]\n",
    "\n",
    "    \n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {np.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
