{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d13bbe5-80d1-4879-bc47-d8e366f38456",
   "metadata": {},
   "source": [
    "# **Lunar Lander con Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82127016-b75d-4621-a53e-8b2bb63cd3f8",
   "metadata": {},
   "source": [
    "### **1. Bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e0a19d-696b-440e-84eb-fe2b8761d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install cmake gymnasium scipy numpy gymnasium[box2d] pygame==2.6.0 swig\n",
    "# Tal vez tengan que ejecutar lo siguiente en sus máquinas (ubuntu 20.04)\n",
    "# sudo apt-get remove swig\n",
    "# sudo apt-get install swig3.0\n",
    "# sudo ln -s /usr/bin/swig3.0 /usr/bin/swig\n",
    "# En windows tambien puede ser necesario MSVC++\n",
    "\n",
    "import sys\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import pygame\n",
    "from pygame.locals import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831e9d4-7840-485c-bc38-af987a76de4f",
   "metadata": {},
   "source": [
    "### **2. Jugando a mano**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e744255-5e3f-4c17-b9dc-c6374c2f06c2",
   "metadata": {},
   "source": [
    "A continuación se puede jugar un episodio del lunar lander. Se controlan los motores con el teclado. Notar que solo se puede realizar una acción a la vez (que es parte del problema), y que en esta implementación, izq toma precedencia sobre derecha, que toma precedencia sobre el motor principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6211ed30-b1a3-4b8e-9858-17eee433ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar pygame (para el control con el teclado) y el ambiente\n",
    "pygame.init()\n",
    "env = gym.make('LunarLander-v2', render_mode='human')\n",
    "env.reset()\n",
    "pygame.display.set_caption('Lunar Lander')\n",
    "\n",
    "clock = pygame.time.Clock()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == QUIT:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "    keys = pygame.key.get_pressed()\n",
    "\n",
    "    # Map keys to actions\n",
    "    if keys[K_LEFT]:\n",
    "        action = 3  # Fire left orientation engine\n",
    "    elif keys[K_RIGHT]:\n",
    "        action = 1 # Fire right orientation engine\n",
    "    elif keys[K_UP]:\n",
    "        action = 2  # Fire main engine\n",
    "    else:\n",
    "        action = 0  # Do nothing\n",
    "\n",
    "    _, _, terminated, truncated, _ = env.step(action)\n",
    "    env.render()\n",
    "    clock.tick(10)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        done = True\n",
    "\n",
    "env.close()\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721a629-7f3f-4a70-83c0-e12f43b0f285",
   "metadata": {},
   "source": [
    "## **3. Discretizando el estado**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d35189-3a15-4ceb-b978-38226518092a",
   "metadata": {},
   "source": [
    "El estado consiste de posiciones y velocidades en (x,y,theta) y en información de contacto de los pies con la superficie.\n",
    "\n",
    "Como varios de estos son continuos, tenemos que discretizarlos para aplicar nuestro algoritmo de aprendizaje por refuerzo tabular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b6bed3-12ba-4d92-8b09-3d3f8429217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuántos bins queremos por dimensión\n",
    "# Pueden considerar variar este parámetro\n",
    "bins_per_dim = 20\n",
    "\n",
    "#          Estado:\n",
    "#          (x,            y,            x_vel,        y_vel,        theta,        theta_vel,    pie_izq_en_contacto, pie_derecho_en_contacto)\n",
    "NUM_BINS = [bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, 2,                   2]\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode='human')\n",
    "env.reset()\n",
    "\n",
    "# Tomamos los rangos del env\n",
    "OBS_SPACE_HIGH = env.observation_space.high\n",
    "OBS_SPACE_LOW = env.observation_space.low\n",
    "OBS_SPACE_LOW[1] = 0 # Para la coordenada y (altura), no podemos ir más abajo que la zona de aterrizaje (que está en el 0, 0)\n",
    "\n",
    "# Los bins para cada dimensión\n",
    "bins = [\n",
    "    np.linspace(OBS_SPACE_LOW[i], OBS_SPACE_HIGH[i], NUM_BINS[i] - 1)\n",
    "    for i in range(len(NUM_BINS) - 2) # last two are binary\n",
    "]\n",
    "# Se recomienda observar los bins para entender su estructura\n",
    "# print (\"Bins: \", bins)\n",
    "\n",
    "def discretize_state(state, bins):\n",
    "    \"\"\"Discretize the continuous state into a tuple of discrete indices.\"\"\"\n",
    "    state_disc = list()\n",
    "    for i in range(len(state)):\n",
    "        if i >= len(bins):  # For binary features (leg contacts)\n",
    "            state_disc.append(int(state[i]))\n",
    "        else:\n",
    "            state_disc.append(\n",
    "                np.digitize(state[i], bins[i])\n",
    "            )\n",
    "    return tuple(state_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45bcc921-c5f6-4f06-9702-6e740b26fc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 10, 10, 10, 10, 1, 1)\n",
      "(10, 19, 10, 10, 10, 10, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos\n",
    "print(discretize_state([0.0, 0.0, 0, 0, 0, 0, 1, 1], bins)) # En la zona de aterrizaje y quieto\n",
    "print(discretize_state([0, 1.5, 0, 0, 0, 0, 0, 0], bins)) # Comenzando la partida, arriba y en el centro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c1576-eff2-4b3c-8069-f6d30243f1e6",
   "metadata": {},
   "source": [
    "## **4. Agentes y la interacción con el entorno**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d594f89c-70b1-4c7c-939f-d654a5263f1c",
   "metadata": {},
   "source": [
    "Vamos a definir una interfaz para nuestro agente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e7059a-7b68-4b13-857b-16e96c5f9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente:\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        \"\"\"Elegir la accion a tomar en el estado actual y el espacio de acciones\n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "            - terminado: si el episodio terminó\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        \"\"\"Aprender a partir de la tupla \n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "            - terminado: si el episodio terminó en esta transición\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c61e38-f7d3-40bf-af19-c7fefd143ffd",
   "metadata": {},
   "source": [
    "Para un agente aleatorio, la implementación sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3adcedd-b300-4e30-9cb1-19d5ec96fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class AgenteAleatorio(Agente):\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        # Elige una acción al azar\n",
    "        return random.randrange(max_accion)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        # No aprende\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19498b0d-eb74-431e-ac7f-612497ca07f3",
   "metadata": {},
   "source": [
    "Luego podemos definir una función para ejecutar un episodio con un agente dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "038e9f56-f0cf-40b7-b3c3-a63b34572bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio(agente, aprender = True, render = None):\n",
    "    entorno = gym.make('LunarLander-v2', render_mode=render).env\n",
    "\n",
    "    iteraciones = 0\n",
    "    recompensa_total = 0\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset()\n",
    "    \n",
    "    while not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        # Si no estamos aprendiendo, explotamos sin explorar\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n, aprender)\n",
    "\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        \n",
    "        # Le informamos al agente para que aprenda\n",
    "        if (aprender):\n",
    "            agente.aprender(estado_anterior, estado_siguiente, accion, recompensa, termino)\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "        recompensa_total += recompensa\n",
    "    env.close()\n",
    "    return recompensa_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29de7442-4565-4ce9-b442-a0e8f7c038a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-94.44971519408841"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejecutamos un episodio con el agente aleatorio y modo render 'human', para poder verlo\n",
    "ejecutar_episodio(AgenteAleatorio(), render = 'human')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9cb2a-37b0-4115-afe8-bb89ce49f605",
   "metadata": {},
   "source": [
    "Podemos ejecutar este ambiente muchas veces y tomar métricas al respecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e81ee-6038-44c7-b887-35db442815ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "agente = AgenteAleatorio()\n",
    "recompensa_episodios = []\n",
    "\n",
    "exitos = 0\n",
    "num_episodios = 100\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio(agente)\n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        exitos += 1\n",
    "    recompensa_episodios += [recompensa]\n",
    "\n",
    "import numpy\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {numpy.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086cb250-7bc9-4bd4-a4cd-43cfd561facb",
   "metadata": {},
   "source": [
    "### **5. Programando un agente que aprende**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827e3e2-a84a-462f-8b86-e03425cfc645",
   "metadata": {},
   "source": [
    "La tarea a realizar consiste en programar un agente de aprendizaje por refuerzos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a3f37011-ffaa-4d08-b832-c45d0b9060da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Hiperparametros\n",
    "#   Politica de exploracion\n",
    "#   Temperatura / k\n",
    "#   Alfa / Learning rate\n",
    "#   Aprender en el momento / Aprender al final\n",
    "#   Cantidad de bins\n",
    "\n",
    "class AgenteRL(Agente):\n",
    "\n",
    "    def __init__(self, alfa) -> None:\n",
    "        super().__init__()\n",
    "        self.alfa = alfa\n",
    "        self.Q = {}\n",
    "\n",
    "    def elegir_accion(self, estado, max_accion, explorar=True) -> int:\n",
    "        estado = discretize_state(estado, bins)\n",
    "        estado = tuple(estado)\n",
    "\n",
    "        if explorar:\n",
    "            # Explorar\n",
    "            if random.random() < 0.5 or estado not in self.Q:\n",
    "                return random.randrange(max_accion)\n",
    "            else:\n",
    "                accion = None\n",
    "                for a in self.Q[estado]:  \n",
    "                    if accion is None or self.Q[estado][a] > self.Q[estado][accion]:\n",
    "                        accion = a\n",
    "                return accion  \n",
    "        else:\n",
    "            # Explotar\n",
    "            accion = None\n",
    "            if estado not in self.Q:\n",
    "                return random.randrange(max_accion)\n",
    "            else:\n",
    "                for a in self.Q[estado]:\n",
    "                    if accion is None or self.Q[estado][a] > self.Q[estado][accion]:\n",
    "                        accion = a\n",
    "                return accion  \n",
    "\n",
    "    def aprender(self, estado_anterior, estado_actual, accion, recompensa, terminado):\n",
    "        estado_anterior = discretize_state(estado_anterior, bins)\n",
    "        estado_actual = discretize_state(estado_actual, bins)\n",
    "\n",
    "        estado_anterior = tuple(estado_anterior)\n",
    "        estado_actual = tuple(estado_actual)\n",
    "\n",
    "        if estado_anterior not in self.Q:\n",
    "            self.Q[estado_anterior] = {}\n",
    "            for i in range(4):\n",
    "                self.Q[estado_anterior][i] = 0\n",
    "\n",
    "        if estado_actual not in self.Q:\n",
    "            self.Q[estado_actual] = {}\n",
    "            for i in range(4):\n",
    "                self.Q[estado_actual][i] = 0\n",
    "        \n",
    "        self.Q[estado_anterior][accion] = (1 - self.alfa) * + self.alfa * (recompensa + max(self.Q[estado_actual].values()))\n",
    "\n",
    "\n",
    "    def fin_episodio(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee51f5-aff8-4d18-934e-92acdcb617c0",
   "metadata": {},
   "source": [
    "Y ejecutar con el muchos episodios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8e0524d5-0d12-46a8-8437-984b981fbae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-231.7275746887171\n",
      "-175.06147011956824\n",
      "-195.9334462642756\n",
      "-164.53116177496238\n",
      "-150.69313222940625\n",
      "-167.3773788460752\n",
      "-181.32733043844604\n",
      "-169.06847637834775\n",
      "-155.6457439048981\n",
      "-147.30244662650708\n",
      "-139.23876217935722\n",
      "-129.79655281485196\n",
      "-138.21035167858108\n",
      "-140.5167680511977\n",
      "-141.6738591710907\n",
      "-149.94958887562694\n",
      "-143.9404051255815\n",
      "-153.67645666747381\n",
      "-153.1006956161466\n",
      "-150.1390233467955\n",
      "-147.58221615848657\n",
      "-145.32574932363798\n",
      "-144.2022261183033\n",
      "-139.3045950448734\n",
      "-141.24537884899132\n",
      "-141.06955910395752\n",
      "-139.11349908131976\n",
      "-136.45308377199862\n",
      "-137.2172407561971\n",
      "-140.5304010980522\n",
      "-139.78936423904824\n",
      "-142.96005172684363\n",
      "-141.80504849483015\n",
      "-141.4794730580913\n",
      "-139.37584430948914\n",
      "-137.53715636105602\n",
      "-138.1971080723575\n",
      "-140.74518064650024\n",
      "-139.66162519563076\n",
      "-138.69778757370287\n",
      "-140.95191243905023\n",
      "-143.89800498112496\n",
      "-144.3695494367371\n",
      "-140.9873336886025\n",
      "-138.97341783143574\n",
      "-142.5947397233519\n",
      "-142.59542417540442\n",
      "-144.95799392147083\n",
      "-148.35170246151753\n",
      "-147.05631593077777\n",
      "-147.9874346108379\n",
      "-147.6018374955969\n",
      "-144.67343525921638\n",
      "-143.53019398022994\n",
      "-146.57780424580278\n",
      "-146.8519750161131\n",
      "-145.7148018915642\n",
      "-144.647066683857\n",
      "-144.088018666552\n",
      "-143.45069850989316\n",
      "-142.17326687878622\n",
      "-143.39423744437056\n",
      "-143.06239529319794\n",
      "-146.4513080791068\n",
      "-145.9291116248811\n",
      "-146.55905190698886\n",
      "-146.3658993438072\n",
      "-145.48767020653054\n",
      "-146.9395563782213\n",
      "-146.1844234216365\n",
      "-147.67341450938014\n",
      "-148.84283392378433\n",
      "-150.16302770272424\n",
      "-151.40110647081002\n",
      "-152.52549877354804\n",
      "-153.19752257526872\n",
      "-152.52214404767435\n",
      "-153.68846590576442\n",
      "-154.31580889158266\n",
      "-157.362011047826\n",
      "-157.30140353821497\n",
      "-156.28695484656978\n",
      "-156.0454282042464\n",
      "-156.93108437646572\n",
      "-155.39464590965696\n",
      "-157.0332050521238\n",
      "-158.6755703441191\n",
      "-158.5243778686747\n",
      "-157.36877418116896\n",
      "-156.35032998904515\n",
      "-156.0799927135426\n",
      "-155.1548213742746\n",
      "-154.54583836902836\n",
      "-153.0105183036335\n",
      "-153.75785375219604\n",
      "-154.4417234654751\n",
      "-153.80469238120529\n",
      "-153.4032315673193\n",
      "-152.73817089897054\n",
      "-152.6830953082565\n",
      "-151.87408697158304\n",
      "-151.12892464089063\n",
      "-152.31491862636784\n",
      "-152.99441352867692\n",
      "-152.3582972899296\n",
      "-152.00589947734744\n",
      "-153.57018626155707\n",
      "-152.49710558449192\n",
      "-152.13449944158572\n",
      "-151.1800436636484\n",
      "-150.6084074492282\n",
      "-151.8016413185324\n",
      "-151.57138902892683\n",
      "-151.17748669330092\n",
      "-149.9122108728527\n",
      "-149.0621263963869\n",
      "-148.8648375719358\n",
      "-150.0225923533621\n",
      "-152.57380036544149\n",
      "-151.84166384942156\n"
     ]
    }
   ],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "# Advertencia: este bloque es un loop infinito si el agente se deja sin implementar\n",
    "\n",
    "entorno = gym.make('LunarLander-v2').env\n",
    "agente = AgenteRL(0.1)\n",
    "exitos = 0\n",
    "recompensa_episodios = []\n",
    "num_episodios = 1000\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio(agente, aprender = True, render='human')\n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        exitos += 1\n",
    "    recompensa_episodios += [recompensa]\n",
    "    #imprimir promedio\n",
    "    print(np.mean(recompensa_episodios))\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {np.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e7b346-6f23-44e2-a645-e4548ab470ef",
   "metadata": {},
   "source": [
    "Analizar los resultados de la ejecución anterior, incluyendo:\n",
    " * Un análisis de los parámetros utilizados en el algoritmo (aprendizaje, política de exploración)\n",
    " * Un análisis de algunos 'cortes' de la matriz Q y la política (p.e. qué hace la nave cuando está cayendo rápidamente hacia abajo, sin rotación)\n",
    " * Un análisis de la evolución de la recompensa promedio\n",
    " * Un análisis de los casos de éxito\n",
    " * Un análisis de los casos en el que el agente falla\n",
    " * Qué limitante del agente de RL les parece que afecta más negativamente su desempeño. Cómo lo mejorarían? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f93fe904-e691-42ff-8fd4-b360d08431cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar los resultados aqui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
