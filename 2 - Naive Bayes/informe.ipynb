{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"b7a7befe16bd4f4c8e31e702c5c13e09","deepnote_cell_type":"markdown"},"source":["# Entrega 2 - Predictor de muerte de pacientes con HIV utilizando Naive Bayes\n","\n","### Grupo 26:\n","     - A. Martínez\n","     - J. Mezquita\n","     - N. Núñez"]},{"cell_type":"markdown","metadata":{"cell_id":"1282e958fcb643e68a187a548bbba3b7","deepnote_cell_type":"markdown"},"source":["## 1. Objetivo"]},{"cell_type":"markdown","metadata":{"cell_id":"d50d6478bc704a07964eb0e48cdd8487","deepnote_cell_type":"markdown"},"source":["El objetivo principal de esta tarea es construir un algoritmo capaz de predecir la muerte de pacientes bajo observación por HIV utilizando metodos bayesianos, croncretamente el algoritmo de Naive Bayes visto en el curso. Para ello, utilizaremos el dataset `AIDS Clinical Trials Group Study 175` tomando como valor a predecir el indicador de censura `cid`.\n","\n","El indicador de censura puede tomar dos valores \"censoring\" o \"failure\". La censura ocurre cuando un valor de una observación solo se conoce parcialmente. \n","\n","En este caso ocurre cuando el experimento termina en un momento determinado, tras el cuál los pacientes todavía vivos quedan todos censurados por la derecha.  \n","\n","Luego del momento de la última observación no se conocen datos sobre la muerte o supervivencia de los pacientes. Por lo que se puede tomar únicamente el caso \"failure\" como la muerte de un paciente.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"cell_id":"0b29a12c09ff44fa852bd1aa7bcbbca6","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2340,"execution_start":1726188674749,"source_hash":null},"outputs":[],"source":["import funciones\n","import pandas as pd\n","import numpy as np\n","import sklearn.preprocessing as sk_pre\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score, cross_validate\n","from sklearn.metrics import  accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","from sklearn.model_selection import KFold\n","from scipy.stats import chi2_contingency\n","from sklearn.preprocessing import KBinsDiscretizer\n","import importlib\n","\n","importlib.reload(funciones)\n","\n","# Objetivo a predecir\n","OBJETIVO = 'cid'\n","\n","dataset = pd.read_csv('data.csv')"]},{"cell_type":"markdown","metadata":{"cell_id":"43be85e9133541398b547844b6e6aff5","deepnote_cell_type":"markdown"},"source":["## 2. Diseño del predictor"]},{"cell_type":"markdown","metadata":{"cell_id":"f56a48cf42564f01b4599708cca0ffd3","deepnote_cell_type":"markdown"},"source":[" ### 2.1 Partición del conjunto de datos\n","\n","Para realizar el entrenamiento, ajuste de la solución y evaluación del modelo, el dataset es separado en 2 conjuntos:\n","\n","- Entrenamiento (85%): Utilizado para entrenar el algoritmo de Naive Bayes\n","- Evaluación (15%): Para obtener métricas de rendimiento del modelo una vez finalizado.\n","\n","Además, para ajustar los hiperparametros del modelo, utilizaremos `validación cruzada`, dividiendo el conjunto de  entrenamiento en 5 partes iguales. Es por esto que no se utiliza una partición de validación.\n","\n","Pot último, a la hora de separarlos, utilizamos el parámetro `stratify=Y` de forma que la distribución de la columna `cid` sea similar en los 2 subconjuntos.\n","\n","Al momento de evaluar las soluciones se utilizan las siguientes métricas, calculadas con `scikit-learn`:\n","\n","- Accuracy: $ \\frac{TP + TN}{TP + TN + FP + FN}$\n","- Precision: $ \\frac{TP}{TP + FP}$\n","- Recall: $ \\frac{TP}{TP + FN}$\n","- F1: $ \\frac{2 \\cdot Precision}{Precision + Recall} $\n","\n","Estas métricas serán acompañadas con la visualización de la matriz de confusión y curva precision-recall."]},{"cell_type":"code","execution_count":2,"metadata":{"cell_id":"8f55ff1acd944b4da95ef435d7d311c2","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":54,"execution_start":1726188677091,"source_hash":null},"outputs":[],"source":["X = dataset.copy().drop(columns=[OBJETIVO, 'pidnum'])\n","Y = dataset[OBJETIVO].copy()\n","\n","X_train, X_val, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15, random_state = 12345, stratify=Y)\n","#X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.15, random_state = 12345, stratify=Y_train)"]},{"cell_type":"markdown","metadata":{"cell_id":"0d6e52cce47147e797e430592ad9fc2f","deepnote_cell_type":"markdown"},"source":["### 2.2 Valores posibles"]},{"cell_type":"markdown","metadata":{"cell_id":"47be64e786144647a5675d1fb14c8f80","deepnote_cell_type":"markdown"},"source":["El modelo Naive Bayes necesita conocer de antemano los valores posibles que puede tomar cada atributo, por lo que se construye un diccionario `valores_posibles` que mapee el nombre de cada atributo a los posibles valores que este pueda tomar. \n","Esto se hace sobre el dataset original, y no sobre el subconjunto de entrenamiento, puesto que podría ocurrir que, al hacer la partición de estos, el subconjunto de validación/test contenga valores en ciertos atributos que no esten en el subconjunto de entrenamiento.\n","Además, como este procedimiento se hace sobre el dataset sin ningún preprocesamiento, el modelo sustituirá los valores de las claves pertenecientes al array `atributos_a_categorizar` por sus valores posibles luego de ser categorizados."]},{"cell_type":"code","execution_count":3,"metadata":{"cell_id":"a93ec2442a5d41e88b256e94ad962af0","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":49,"execution_start":1726188677108,"source_hash":null},"outputs":[],"source":["valores_posibles = {}\n","\n","for categoria in X.columns:\n","    valores_posibles[categoria] = X[categoria].unique()"]},{"cell_type":"markdown","metadata":{"cell_id":"41f7b352f4524bfeb276eb0b8ad411ec","deepnote_cell_type":"markdown"},"source":["### 2.3 Preprocesamiento de datos\n","\n","Debido a que este dataset cuenta con algunos atributos numéricos continuos, se preprocesaran usando la libreria de `scikit-learn`. La categorizacion se hara especificamente con la funcion `KBinsDiscretizer` con los siguientes parametros:\n","- n_bins   =  3 \n","- encode   = 'ordinal'\n","- strategy = 'kmeans'\n","En la seccion de experimentacion explicaremos el porque de estos parametros.\n","\n","Los atributos a categorizar son:\n","\n","- time\n","- age\n","- wtkg\n","- karnof\n","- preanti\n","- cd40\n","- cd420\n","- cd80\n","- cd820\n","\n","Además, el dataset posee una columna `pidnum` que es el identificador de cada observación. Al ser esto un metadato y no un dato de la realidad que influya en el resultado, esta columna se elimina del dataset, para evitar agregar ruído al modelo."]},{"cell_type":"code","execution_count":4,"metadata":{"cell_id":"ce165a7948114cd6a7373e51684363b2","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":104,"execution_start":1726191001695,"source_hash":null},"outputs":[],"source":["atributos_a_categorizar = ['time', 'age', 'wtkg', 'karnof', 'preanti', 'cd40', 'cd420', 'cd80', 'cd820']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["discretizer = KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy='kmeans', subsample=200_000, random_state=12345)\n","puntos_corte = {}\n","for atributo in atributos_a_categorizar:\n","    X_train[atributo] = discretizer.fit_transform(X_train[[atributo]]).astype(int)\n","    puntos_corte[atributo] = discretizer.bin_edges_[0][1:3]\n","    valores_posibles[atributo] = np.unique(X_train[atributo])\n","\n","# TODO: cambiar (está mal)\n","for atributo in atributos_a_categorizar:\n","    X_val[atributo] = discretizer.fit_transform(X_val[[atributo]]).astype(int)\n","\n","for atributo in atributos_a_categorizar:\n","    X_test[atributo] = discretizer.fit_transform(X_test[[atributo]]).astype(int)"]},{"cell_type":"markdown","metadata":{"cell_id":"1579ee8e74a5480b850826567f005c91","deepnote_cell_type":"markdown"},"source":["### 2.4 Linea base \n","\n","Al estar trabajando con el mismo dataset que el informe anterior, sabemos que la linea base sera la misma. Recordemos que dicho dataset cuenta con 1618 entradas cuyo resultado es 0 y 521 cuyo resultado es 1. Por lo que el predictor simple devolverá que el resultado es siempre 0. Los resultados de este predictor eran:\n","\n","- Accuracy de linea base: 0.7564282374941561\n","- Precision de linea base: 0.7564282374941561\n","- Recall de linea base: 1.0\n","- F1 de linea base: 0.8613255256853873\n"]},{"cell_type":"markdown","metadata":{"cell_id":"d504211d94914bbaaad6f566dac73493","deepnote_cell_type":"markdown"},"source":["### 2.5 Selección de atributos\n","\n","Por último, se aplica también selección de atributos sobre el dataset en aras de intentar obtener mejores resultados.\n","Para esto aplicamos el método $ \\chi^2$ que evalúa la independencia entre las categorias y la categoría objetivo, utilizando un umbral de p = 0.01, es decir, las variables tales que su p-valor sea menor o igual que 0.01, se consideran relevantes. Luego de esto, se iterará sobre todos los subconjuntos del array de atributos tales que su p-valor sea mayor a 0.01, y se seleccionará como atributo a eliminar del dataset aquel subconjunto que de mejores resultados.\n","A continuación se desarrollarán los resultados del método $ \\chi^2$ y la iteración sobre los subconjuntos se hará durante la sección de experimentación."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"85465605ac9740de881a97383108416b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":318,"execution_start":1726188677578,"source_hash":null},"outputs":[],"source":["correlacion = []\n","\n","for atributo in X.columns:\n","    tabla_contingencia = pd.crosstab(X[atributo], Y)\n","    _, p, _, _ = chi2_contingency(tabla_contingencia)\n","    correlacion.append((atributo, p))\n","\n","df_correlacion = pd.DataFrame(correlacion, columns=['Atributo', 'p'])\n","print(\"Tabla de correlación:\")\n","print(df_correlacion.to_string(index=False))\n","\n","# Para auementar la cantidad de potenciales atributos a dropear, se puede aumentar el valor de p\n","potenciales_atributos_a_dropear = df_correlacion[df_correlacion['p'] > 0.01]['Atributo'].tolist()\n","print(f'\\nAtributos a dropear: {potenciales_atributos_a_dropear}')"]},{"cell_type":"markdown","metadata":{"cell_id":"4e3ab173cc964675a3fc9fa95176d9eb","deepnote_cell_type":"markdown"},"source":["### 2.6 Otras decisiones\n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"4179b88f9aee4f3cb8d52cb9eb5cf309","deepnote_cell_type":"markdown"},"source":["## 3. Experimentación"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"62d0c1c02d5c460bbaf1a00ee5e84bd6","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1236,"execution_start":1726188677900,"source_hash":null},"outputs":[],"source":["from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.model_selection import cross_validate\n","\n","m = 0\n","modeloCV = funciones.NaiveBayesAIDS(m, valores_posibles)\n","\n","# Definir las métricas con pos_label=0 donde sea necesario\n","scoring = {\n","    'accuracy': make_scorer(accuracy_score),\n","    'precision': make_scorer(precision_score, pos_label=0),\n","    'recall': make_scorer(recall_score, pos_label=0),\n","    'f1': make_scorer(f1_score, pos_label=0)\n","}\n","\n","# Usar cross_validate con las métricas personalizadas\n","scores = cross_validate(modeloCV, X_train, Y_train, cv=5, scoring=scoring)\n","\n","# Mostrar los resultados\n","print(\"Accuracy:\", scores['test_accuracy'].mean())\n","print(\"Precision:\", scores['test_precision'].mean())\n","print(\"Recall:\", scores['test_recall'].mean())\n","print(\"F1:\", scores['test_f1'].mean())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"8454735a422540278455a5b7e4bc36cb","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":269,"execution_start":1726190790429,"source_hash":null},"outputs":[],"source":["for m in c:\n","    print(m)\n","    \n","    pipeline = Pipeline([\n","        ('discretizer', KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')),  \n","        ('actualizar_valores', ActualizarValoresPosiblesTransformer(atributos_a_categorizar, valores_posibles)),  \n","        ('naive_bayes', funciones.NaiveBayesAIDS(m, valores_posibles))\n","    ])\n","    \n","    # Realizar la validación cruzada\n","    scores = cross_validate(pipeline, X_train, Y_train, cv=5, scoring=scoring)\n","    \n","    # Guardar los resultados\n","    resultados[m] = [scores['test_accuracy'].mean(), \n","                     scores['test_precision'].mean(), \n","                     scores['test_recall'].mean(), \n","                     scores['test_f1'].mean()]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"6a51cf4602d74fef98d41e825547910f","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":86,"execution_start":1726189386957,"source_hash":null},"outputs":[],"source":["clave_max = max(resultados, key=lambda k: resultados[k][0])\n","print(clave_max)\n","print(resultados[clave_max])"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"84fcf03b17d249d8946d76a35c4e3318","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":260,"execution_start":1726188768508,"source_hash":null},"outputs":[],"source":["funciones.plot_metricas(resultados,1000)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"d34d25277b644f1a923479a34b3c605a","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":265,"execution_start":1726188368392,"source_hash":null},"outputs":[],"source":["funciones.plot_metricas(resultados,100)"]},{"cell_type":"markdown","metadata":{"cell_id":"e8155ff3846f4498a8977afe9873a7bc","deepnote_cell_type":"markdown"},"source":["### 3.2 Selección de atributos\n","\n","TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"2161a723a14d4629bf8ab22330cf119a","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":313,"execution_start":1726187574419,"source_hash":null},"outputs":[],"source":["atributos_a_eliminar = funciones.seleccionar_subconjunto_a_eliminar(X_train, Y_train, 36, potenciales_atributos_a_dropear, valores_posibles, atributos_a_categorizar, X_validacion, Y_validacion)"]},{"cell_type":"markdown","metadata":{"cell_id":"108b38df3d044340831099c8e4fda461","deepnote_cell_type":"markdown"},"source":["### 3.3 Análisis de resultados\n","\n","La siguiente tabla resume los mejores resultados obtenidos dependiendo de cuando se realizó la categorización y del valor de `max_range_split`, indicando que función de elección de atributo fue la que generó dicho valor.\n","\n","En general, no se observa un claro superior entre categorizar antes o después de ejecutar ID3, sin embargo, si nos enfocamos únicamente en el accuracy y f1, podemos identificar que los mejores resultados se obtienen utilizando:\n","\n","- La función gain ratio.\n","- max-range-split = 3.\n","- Categorizando los atributos necesarios durante la ejecución del algoritmo ID3.\n","\n","Si probamos estas características sobre el conjunto de testeo, obtenemos la siguiente matriz de confusión."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"7341569035344c789f89d45f0dc8b05e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":394,"execution_start":1726178970126,"source_hash":null},"outputs":[],"source":["Y_predicho_test = ArbolDecisionMRS3_gain_ratio.predecir(X_test)\n","funciones.plot_confusion_matrix(Y_test, Y_predicho_test)\n","accuracy_test, precision_test, recall_test, f1_test = funciones.get_accuracy_precision_recall_f1(Y_test, Y_predicho_test)\n","print('Accuracy:', accuracy_test)\n","print('Precision:', precision_test)\n","print('Recall:', recall_test)\n","print('F1:', f1_test)"]},{"cell_type":"markdown","metadata":{"cell_id":"8f6697f94db640e197d80fdc56a410a4","deepnote_cell_type":"markdown"},"source":["## 4. Comparación\n","\n","En esta sección se compara el algoritmo ID3 contra la línea base planteada al inicio del documento, además de las implementaciones de la librería scikit-learn:\n","\n","- DecisionTreeClassifier\n","- RandomForestClassifier"]},{"cell_type":"markdown","metadata":{"cell_id":"ed6321463c2848ca90a01aa434a4fc07","deepnote_cell_type":"markdown"},"source":["### 4.1 Preprocesamiento de datos\n","\n","Para realizar estas comparaciones con la librería `scikit-learn`, primero se preprocesaran los datos ya que estos algoritmos trabajan con atributos numéricos, contrario a la implementación de `ID3` presentada anteriormente. \n","\n","Los atributos categóricos `trt` y `strat` utilizan más de 2 categorías en un solo atributo, por lo que estas dos columnas impondrán un orden donde no lo hay, empeorando así el rendimiento de las implementaciones de la librería `scikit-learn`, por lo que se aplica `one-hot encoding` sobre los dos atributos mencionados esperando una mejora en los resultados.\n","\n","Por otro lado, se observa que en el conjunto de datos no hay elementos faltantes para ningún atributo, por lo que no es necesario realizar preprocesamiento"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"95e94cf65784460291731578db19ef71","deepnote_cell_type":"code","deepnote_table_loading":false,"deepnote_table_state":{"conditionalFilters":[],"filters":[],"pageIndex":0,"pageSize":5,"sortBy":[]},"deepnote_to_be_reexecuted":true,"execution_millis":202,"execution_start":1725485999694,"source_hash":null},"outputs":[],"source":["X_librerias = dataset.copy().drop(columns=[OBJETIVO, 'pidnum'])\n","Y_librerias = dataset[OBJETIVO].copy()\n","\n","X_train_librerias, X_test_librerias, Y_train_librerias, Y_test_librerias = train_test_split(X_librerias, Y_librerias, test_size = 0.15, random_state = 12345, stratify=Y_librerias)\n","X_train_librerias, X_test_librerias = funciones.aplicar_ohe(dataset, X_train_librerias, X_test_librerias, 'trt')\n","X_train_librerias, X_test_librerias = funciones.aplicar_ohe(dataset, X_train_librerias, X_test_librerias, 'strat')\n","\n","\n","Y_predicho_predictor_simple = [0 for _ in range(len(dataset))]\n","Y_real = dataset[OBJETIVO]\n","\n","X_train_librerias.drop(['trt','strat'], axis=1, inplace=True)\n","X_test_librerias.drop(['trt','strat'], axis=1, inplace=True)\n"]},{"cell_type":"markdown","metadata":{"cell_id":"fe20dbc24e124262949398f897b85d2e","deepnote_cell_type":"markdown"},"source":["Antes de hacer la comparacion, veremos primero cual es el mejor criterio a utilizar en las librerias."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"c7dc9a7596824deb8566e666505c0c04","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":685,"execution_start":1725485999727,"source_hash":null},"outputs":[],"source":["#Usamos el criterio entropy\n","ArbolDecisionLibreria_entr = DecisionTreeClassifier(criterion='entropy', random_state=12345)\n","ArbolDecisionLibreria_entr.fit(X_train_librerias, Y_train_librerias)\n","Y_predicho_arbol_libreria_entr = ArbolDecisionLibreria_entr.predict(X_test_librerias)\n","\n","RandomForest_entr = RandomForestClassifier(criterion='entropy', random_state=12345)\n","RandomForest_entr.fit(X_train_librerias, Y_train_librerias)\n","Y_predicho_random_forest_entr = RandomForest_entr.predict(X_test_librerias)\n","\n","# Usamos el criterio gini\n","ArbolDecisionLibreria_gini = DecisionTreeClassifier(criterion='gini', random_state=12345)\n","ArbolDecisionLibreria_gini.fit(X_train_librerias, Y_train_librerias)\n","Y_predicho_arbol_libreria_gini = ArbolDecisionLibreria_gini.predict(X_test_librerias)\n","\n","RandomForest_gini = RandomForestClassifier(criterion='gini', random_state=12345)\n","RandomForest_gini.fit(X_train_librerias, Y_train_librerias)\n","Y_predicho_random_forest_gini = RandomForest_gini.predict(X_test_librerias)\n","\n","accuracy_arbol_libreria_entr, _, _, f1_arbol_libreria_entr = funciones.get_accuracy_precision_recall_f1(Y_test_librerias, Y_predicho_arbol_libreria_entr)\n","accuracy_random_forest_entr, _, _, f1_random_forest_entr = funciones.get_accuracy_precision_recall_f1(Y_test_librerias, Y_predicho_random_forest_entr)\n","\n","\n","accuracy_arbol_libreria_gini, _, _, f1_arbol_libreria_gini = funciones.get_accuracy_precision_recall_f1(Y_test_librerias, Y_predicho_arbol_libreria_gini)\n","accuracy_random_forest_gini, _, _, f1_random_forest_gini = funciones.get_accuracy_precision_recall_f1(Y_test_librerias, Y_predicho_random_forest_gini)\n","\n","print(\"La accuracy del arbol de decision con el criterio entropy es:\", accuracy_arbol_libreria_entr)\n","print(\"La f1 del arbol de decision con el criterio entropy es:\", f1_arbol_libreria_entr)\n","print(\"La accuracy del random forest con el criterio entropy es:\", accuracy_random_forest_entr)\n","print(\"La f1 del random forest con el criterio entropy es:\", f1_random_forest_entr)\n","\n","print(\"\\n\")\n","\n","print(\"La accuracy del arbol de decision con el criterio gini es:\", accuracy_arbol_libreria_gini)\n","print(\"La f1 del arbol de decision con el criterio gini es:\", f1_arbol_libreria_gini)\n","print(\"La accuracy del random forest con el criterio gini es:\", accuracy_random_forest_gini)\n","print(\"La f1 del random forest con el criterio gini es:\", f1_random_forest_gini)"]},{"cell_type":"markdown","metadata":{"cell_id":"0a254ed29ed94ec3b41106c850bbc9a5","deepnote_cell_type":"markdown"},"source":["Podemos observar como los mejores resultados son dados cuando el criterio usado en los algoritmos de `scikit-learn` es `gini`, por lo que usaremos este para las comparaciones con la linea base y nuestro algoritmo manual."]},{"cell_type":"markdown","metadata":{"cell_id":"8de4034ec6ca48e3be571d7f87923914","deepnote_cell_type":"markdown"},"source":["### 4.2 Resultados de la comparación"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"b7ccbcce17d541dc9a0a807133f331a2","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":435,"execution_start":1725486000457,"source_hash":null},"outputs":[],"source":["accuracy_predictor_simple, _, _, f1_predictor_simple = funciones.get_accuracy_precision_recall_f1(Y_real, Y_predicho_predictor_simple)\n","\n","funciones.plot_accuracies_and_f1s([accuracy_predictor_simple, accuracy_test, accuracy_arbol_libreria_gini, accuracy_random_forest_gini],\n","                                  [f1_predictor_simple, f1_test, f1_arbol_libreria_gini, f1_random_forest_gini])"]},{"cell_type":"markdown","metadata":{"cell_id":"638b029ea4134454af2d0949cc38b5aa","deepnote_cell_type":"markdown"},"source":["Con la gráfica de arriba, podemos ver cómo se cumplen varias cosas:\n","\n","- El predictor manual cumple con lo mínimo deseado de estar por encima de la línea base.\n","- El predictor manual otorga resultados competentes al nivel de la librería usada.\n","- RandomForest es mejor que nuestra implementación pero no por mucho.\n"]},{"cell_type":"markdown","metadata":{"cell_id":"f97df510df0249a08a5063762cd81c80","deepnote_cell_type":"markdown"},"source":["## 5. Conclusiones"]},{"cell_type":"markdown","metadata":{"cell_id":"a6c5b02dbc644d5b95b458a49f0754d8","deepnote_cell_type":"markdown"},"source":["A modo de conclusión general, se logra implementar un predictor de mortalidad en pacientes con VIH utilizando árboles de decisión generados con el algoritmo ID3.\n","\n","En particular, se prueban distintas configuraciones de hiper-parámetros y funciones de atributos. La combinación con mejor resultado fue utilizando la función `gain ratio`, con max_range_split de 3 y categorización _«in situ»_.\n","\n","Las métricas de clasificación dieron una _accuracy_ de 0.84 y una puntuación f1 de 0.90 para la mejor configuración. Y este es competitivo con la implementación de Árboles de `scikit-learn` y, si bien el rendimiento es estrictamente inferior, es comparable con la implementación de Random Forest de la misma librería. \n","\n","Como mejoras a futuro podríamos listar:\n","- Explorar otros métodos de preprocesamiento de datos, como por ejemplo la selección de atributos en el dataset.\n","- Probar métodos más complejos como Random Forest.\n","- Aplicar técnicas para reducir el overfitting en caso de haberlo, como puede ser early stopping."]},{"cell_type":"markdown","metadata":{},"source":["# Validación Cruzada"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["m: 1, Accuracy media: 0.664 (+/-0.014)\n","m: 10, Accuracy media: 0.658 (+/-0.015)\n","m: 20, Accuracy media: 0.651 (+/-0.014)\n","m: 30, Accuracy media: 0.646 (+/-0.019)\n","m: 40, Accuracy media: 0.643 (+/-0.020)\n","m: 50, Accuracy media: 0.641 (+/-0.021)\n","m: 60, Accuracy media: 0.637 (+/-0.019)\n","m: 70, Accuracy media: 0.630 (+/-0.017)\n","m: 80, Accuracy media: 0.623 (+/-0.019)\n","m: 90, Accuracy media: 0.620 (+/-0.017)\n","m: 100, Accuracy media: 0.620 (+/-0.021)\n","m: 110, Accuracy media: 0.608 (+/-0.022)\n","m: 120, Accuracy media: 0.604 (+/-0.025)\n","m: 130, Accuracy media: 0.600 (+/-0.025)\n","m: 140, Accuracy media: 0.586 (+/-0.027)\n","m: 150, Accuracy media: 0.569 (+/-0.027)\n","m: 160, Accuracy media: 0.565 (+/-0.028)\n","m: 170, Accuracy media: 0.561 (+/-0.029)\n","m: 180, Accuracy media: 0.547 (+/-0.034)\n","m: 190, Accuracy media: 0.539 (+/-0.036)\n","m: 200, Accuracy media: 0.525 (+/-0.033)\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[19], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m puntos_corte \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m atributo \u001b[38;5;129;01min\u001b[39;00m atributos_a_categorizar:\n\u001b[1;32m---> 26\u001b[0m     X_train_copy[atributo] \u001b[38;5;241m=\u001b[39m \u001b[43mdiscretizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_copy\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43matributo\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     27\u001b[0m     puntos_corte[atributo] \u001b[38;5;241m=\u001b[39m discretizer\u001b[38;5;241m.\u001b[39mbin_edges_[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m     28\u001b[0m     valores_posibles[atributo] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(X_train_copy[atributo])\n","File \u001b[1;32mc:\\Users\\amartinez\\Documents\\aprendizaje-automatico-2024\\2 - Naive Bayes\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n","File \u001b[1;32mc:\\Users\\amartinez\\Documents\\aprendizaje-automatico-2024\\2 - Naive Bayes\\.venv\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n","File \u001b[1;32mc:\\Users\\amartinez\\Documents\\aprendizaje-automatico-2024\\2 - Naive Bayes\\.venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\amartinez\\Documents\\aprendizaje-automatico-2024\\2 - Naive Bayes\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:294\u001b[0m, in \u001b[0;36mKBinsDiscretizer.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;66;03m# 1D k-means procedure\u001b[39;00m\n\u001b[0;32m    293\u001b[0m km \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mn_bins[jj], init\u001b[38;5;241m=\u001b[39minit, n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 294\u001b[0m centers \u001b[38;5;241m=\u001b[39m \u001b[43mkm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcluster_centers_[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Must sort, centers may be unsorted even with sorted init\u001b[39;00m\n\u001b[0;32m    298\u001b[0m centers\u001b[38;5;241m.\u001b[39msort()\n","File \u001b[1;32mc:\\Users\\amartinez\\Documents\\aprendizaje-automatico-2024\\2 - Naive Bayes\\.venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\amartinez\\Documents\\aprendizaje-automatico-2024\\2 - Naive Bayes\\.venv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1473\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1439\u001b[0m \n\u001b[0;32m   1440\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1464\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1465\u001b[0m     X,\n\u001b[0;32m   1466\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1470\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1471\u001b[0m )\n\u001b[1;32m-> 1473\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params_vs_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1475\u001b[0m random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[0;32m   1476\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\n","File \u001b[1;32mc:\\Users\\amartinez\\Documents\\aprendizaje-automatico-2024\\2 - Naive Bayes\\.venv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1414\u001b[0m, in \u001b[0;36mKMeans._check_params_vs_input\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m-> 1414\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params_vs_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_n_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1416\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_algorithm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_algorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melkan\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","File \u001b[1;32mc:\\Users\\amartinez\\Documents\\aprendizaje-automatico-2024\\2 - Naive Bayes\\.venv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:883\u001b[0m, in \u001b[0;36m_BaseKMeans._check_params_vs_input\u001b[1;34m(self, X, default_n_init)\u001b[0m\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    879\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be >= n_clusters=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    880\u001b[0m     )\n\u001b[0;32m    882\u001b[0m \u001b[38;5;66;03m# tol\u001b[39;00m\n\u001b[1;32m--> 883\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tol \u001b[38;5;241m=\u001b[39m \u001b[43m_tolerance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# n-init\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[1;32mc:\\Users\\amartinez\\Documents\\aprendizaje-automatico-2024\\2 - Naive Bayes\\.venv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:293\u001b[0m, in \u001b[0;36m_tolerance\u001b[1;34m(X, tol)\u001b[0m\n\u001b[0;32m    291\u001b[0m     variances \u001b[38;5;241m=\u001b[39m mean_variance_axis(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 293\u001b[0m     variances \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(variances) \u001b[38;5;241m*\u001b[39m tol\n","File \u001b[1;32mc:\\Users\\amartinez\\Documents\\aprendizaje-automatico-2024\\2 - Naive Bayes\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:4318\u001b[0m, in \u001b[0;36mvar\u001b[1;34m(a, axis, dtype, out, ddof, keepdims, where, mean, correction)\u001b[0m\n\u001b[0;32m   4315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4316\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m var(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, ddof\u001b[38;5;241m=\u001b[39mddof, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 4318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4319\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\amartinez\\Documents\\aprendizaje-automatico-2024\\2 - Naive Bayes\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:208\u001b[0m, in \u001b[0;36m_var\u001b[1;34m(a, axis, dtype, out, ddof, keepdims, where, mean)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# Most general case; includes handling object arrays containing imaginary\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# numbers and complex types with non-native byteorder\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     x \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39mmultiply(x, um\u001b[38;5;241m.\u001b[39mconjugate(x), out\u001b[38;5;241m=\u001b[39mx)\u001b[38;5;241m.\u001b[39mreal\n\u001b[1;32m--> 208\u001b[0m ret \u001b[38;5;241m=\u001b[39m umr_sum(x, axis, dtype, out, keepdims\u001b[38;5;241m=\u001b[39mkeepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Compute degrees of freedom and make sure it is not negative.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m rcount \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39mmaximum(rcount \u001b[38;5;241m-\u001b[39m ddof, \u001b[38;5;241m0\u001b[39m)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from sklearn import metrics\n","import scipy.stats\n","\n","splits = 5\n","m_posibles = list(range(0, 1000, 10))\n","m_posibles[0] = 1\n","# Hacemos cross validation para encontrar el mejor \"m\"\n","for m in m_posibles:\n","\n","    kf = KFold(n_splits=splits)\n","    scores = np.zeros(5)\n","    score_index = 0\n","    \n","    for train_index, val_index in kf.split(X_train):\n","        # Obtengo la nueva partición de X_train y X_val\n","        X_train_cv, X_val_cv= X_train.iloc[train_index], X_train.iloc[val_index]\n","        y_train_cv, y_val_cv= Y_train.iloc[train_index], Y_train.iloc[val_index]\n","\n","        X_train_copy = X_train_cv.copy()\n","        X_val_copy = X_val_cv.copy()\n","        \n","        # Calculo los nuevos puntos de cortes\n","        discretizer = KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy='kmeans', subsample=200_000, random_state=12345)\n","        puntos_corte = {}\n","        for atributo in atributos_a_categorizar:\n","            X_train_copy[atributo] = discretizer.fit_transform(X_train_copy[[atributo]]).astype(int)\n","            puntos_corte[atributo] = discretizer.bin_edges_[0][1:3]\n","            valores_posibles[atributo] = np.unique(X_train_copy[atributo])\n","\n","        # Discretizo el resto del conjunto de datos\n","        for atributo in atributos_a_categorizar:\n","            X_val_copy[atributo] = np.digitize(X_val_copy[atributo], puntos_corte[atributo])\n","        \n","        # Defino y entreno al modelo\n","        modelo_cv = funciones.NaiveBayesAIDS(m, valores_posibles)\n","        modelo_cv.fit(X_train_copy,y_train_cv)\n","        y_pred = modelo_cv.predict(X_val_copy)\n","\n","        # Calculo métricas\n","        scores[score_index] = metrics.f1_score(y_val_cv, y_pred, pos_label=0)\n","        score_index += 1\n","    print (\"m: {0:d}, Accuracy media: {1:.3f} (+/-{2:.3f})\".format(m, np.mean(scores), scipy.stats.sem(scores)))"]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=c37fbc8c-fee1-4291-8075-be0e33896680' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"3f50ce13a93049aa8f181298f39d335c","kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
