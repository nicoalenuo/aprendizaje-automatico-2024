{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d13bbe5-80d1-4879-bc47-d8e366f38456",
   "metadata": {},
   "source": [
    "# **Lunar Lander con Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82127016-b75d-4621-a53e-8b2bb63cd3f8",
   "metadata": {},
   "source": [
    "### **1. Bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e0a19d-696b-440e-84eb-fe2b8761d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import pygame\n",
    "from pygame.locals import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831e9d4-7840-485c-bc38-af987a76de4f",
   "metadata": {},
   "source": [
    "### **2. Jugando a mano**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e744255-5e3f-4c17-b9dc-c6374c2f06c2",
   "metadata": {},
   "source": [
    "A continuación se puede jugar un episodio del lunar lander. Se controlan los motores con el teclado. Notar que solo se puede realizar una acción a la vez (que es parte del problema), y que en esta implementación, izq toma precedencia sobre derecha, que toma precedencia sobre el motor principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6211ed30-b1a3-4b8e-9858-17eee433ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar pygame (para el control con el teclado) y el ambiente\n",
    "pygame.init()\n",
    "env = gym.make('LunarLander-v2', render_mode='human')\n",
    "env.reset()\n",
    "pygame.display.set_caption('Lunar Lander')\n",
    "\n",
    "clock = pygame.time.Clock()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == QUIT:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "    keys = pygame.key.get_pressed()\n",
    "\n",
    "    # Map keys to actions\n",
    "    if keys[K_LEFT]:\n",
    "        action = 3  # Fire left orientation engine\n",
    "    elif keys[K_RIGHT]:\n",
    "        action = 1 # Fire right orientation engine\n",
    "    elif keys[K_UP]:\n",
    "        action = 2  # Fire main engine\n",
    "    else:\n",
    "        action = 0  # Do nothing\n",
    "\n",
    "    _, _, terminated, truncated, _ = env.step(action)\n",
    "    env.render()\n",
    "    clock.tick(10)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        done = True\n",
    "\n",
    "env.close()\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721a629-7f3f-4a70-83c0-e12f43b0f285",
   "metadata": {},
   "source": [
    "## **3. Discretizando el estado**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d35189-3a15-4ceb-b978-38226518092a",
   "metadata": {},
   "source": [
    "El estado consiste de posiciones y velocidades en (x,y,theta) y en información de contacto de los pies con la superficie.\n",
    "\n",
    "Como varios de estos son continuos, tenemos que discretizarlos para aplicar nuestro algoritmo de aprendizaje por refuerzo tabular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b6bed3-12ba-4d92-8b09-3d3f8429217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuántos bins queremos por dimensión\n",
    "# Pueden considerar variar este parámetro\n",
    "bins_per_dim = 16\n",
    "\n",
    "#          Estado:\n",
    "#          (x,            y,            x_vel,        y_vel,        theta,        theta_vel,    pie_izq_en_contacto, pie_derecho_en_contacto)\n",
    "NUM_BINS = [bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, 2, 2]\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode='human')\n",
    "env.reset()\n",
    "\n",
    "# Tomamos los rangos del env\n",
    "OBS_SPACE_HIGH = env.observation_space.high\n",
    "OBS_SPACE_LOW = env.observation_space.low\n",
    "OBS_SPACE_LOW[1] = 0 # Para la coordenada y (altura), no podemos ir más abajo que la zona de aterrizaje (que está en el 0, 0)\n",
    "\n",
    "# Los bins para cada dimensión\n",
    "bins = [\n",
    "    np.linspace(OBS_SPACE_LOW[i], OBS_SPACE_HIGH[i], NUM_BINS[i] - 1)\n",
    "    for i in range(len(NUM_BINS) - 2) # last two are binary\n",
    "]\n",
    "# Se recomienda observar los bins para entender su estructura\n",
    "# print (\"Bins: \", bins)\n",
    "\n",
    "def discretize_state(state, bins):\n",
    "    \"\"\"Discretize the continuous state into a tuple of discrete indices, taking the closest bin.\"\"\"\n",
    "    state_disc = list()\n",
    "    for i in range(len(state)):\n",
    "        if i >= len(bins):  # For binary features (leg contacts)\n",
    "            state_disc.append(int(state[i]))\n",
    "        else:\n",
    "            # Encuentra el índice del valor más cercano en los bins\n",
    "            closest_index = np.argmin(np.abs(bins[i] - state[i]))\n",
    "            state_disc.append(closest_index)\n",
    "    return tuple(state_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45bcc921-c5f6-4f06-9702-6e740b26fc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 10, 10, 10, 10, 1, 1)\n",
      "(10, 19, 10, 10, 10, 10, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos\n",
    "print(discretize_state([0.0, 0.0, 0, 0, 0, 0, 1, 1], bins)) # En la zona de aterrizaje y quieto\n",
    "print(discretize_state([0, 1.5, 0, 0, 0, 0, 0, 0], bins)) # Comenzando la partida, arriba y en el centro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c1576-eff2-4b3c-8069-f6d30243f1e6",
   "metadata": {},
   "source": [
    "## **4. Agentes y la interacción con el entorno**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d594f89c-70b1-4c7c-939f-d654a5263f1c",
   "metadata": {},
   "source": [
    "Vamos a definir una interfaz para nuestro agente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e7059a-7b68-4b13-857b-16e96c5f9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente:\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        \"\"\"Elegir la accion a tomar en el estado actual y el espacio de acciones\n",
    "            - estado: el estado en el que se encuentra actualmente el agente\n",
    "            - max_accion: el espacio de acciones posibles\n",
    "            - explorar: si se debe elegir una acción de forma que explore el espacio de estados, o eligiendo la que mejor recompensa cree que devuelve\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado) -> None:\n",
    "        \"\"\"Aprender a partir de la tupla \n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "            - terminado: si el episodio terminó en esta transición\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fin_episodio(self) -> None:\n",
    "        \"\"\"Actualizar estructuras al final de un episodio\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c61e38-f7d3-40bf-af19-c7fefd143ffd",
   "metadata": {},
   "source": [
    "Para un agente aleatorio, la implementación sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3adcedd-b300-4e30-9cb1-19d5ec96fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class AgenteAleatorio(Agente):\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        # Elige una acción al azar\n",
    "        return random.randrange(max_accion)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado) -> None:\n",
    "        # No aprende\n",
    "        pass\n",
    "\n",
    "    def fin_episodio(self) -> None:\n",
    "        # Nada que actualizar\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19498b0d-eb74-431e-ac7f-612497ca07f3",
   "metadata": {},
   "source": [
    "Luego podemos definir una función para ejecutar un episodio con un agente dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "038e9f56-f0cf-40b7-b3c3-a63b34572bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio(agente, aprender=True, render=None, max_iteraciones=500):\n",
    "    entorno = gym.make('LunarLander-v2', render_mode=render).env\n",
    "    \n",
    "    iteraciones = 0\n",
    "    recompensa_total = 0\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset()\n",
    "    while iteraciones < max_iteraciones and not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n, aprender)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        # Le informamos al agente para que aprenda\n",
    "        if (aprender):\n",
    "            agente.aprender(estado_anterior, estado_siguiente, accion, recompensa, termino)\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "        recompensa_total += recompensa\n",
    "        \n",
    "    if (aprender):\n",
    "        agente.fin_episodio()\n",
    "\n",
    "    entorno.close()\n",
    "    return recompensa_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29de7442-4565-4ce9-b442-a0e8f7c038a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-85.00346673972187"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejecutamos un episodio con el agente aleatorio y modo render 'human', para poder verlo\n",
    "ejecutar_episodio(AgenteAleatorio(), render = 'human')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9cb2a-37b0-4115-afe8-bb89ce49f605",
   "metadata": {},
   "source": [
    "Podemos ejecutar este ambiente muchas veces y tomar métricas al respecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e81ee-6038-44c7-b887-35db442815ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AgenteAleatorio = AgenteAleatorio()\n",
    "recompensa_episodios = []\n",
    "\n",
    "exitos = 0\n",
    "num_episodios = 100\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio(AgenteAleatorio, render='human')\n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        exitos += 1\n",
    "    recompensa_episodios += [recompensa]\n",
    "\n",
    "import numpy\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {numpy.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086cb250-7bc9-4bd4-a4cd-43cfd561facb",
   "metadata": {},
   "source": [
    "### **5. Programando un agente que aprende**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827e3e2-a84a-462f-8b86-e03425cfc645",
   "metadata": {},
   "source": [
    "La tarea a realizar consiste en programar un agente de aprendizaje por refuerzos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c40f3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "'''\n",
    "Hiperparametros\n",
    "  Politica de exploracion (random, epsilon-greedy, softmax)\n",
    "  Politica de aprendizaje (Aprender en el momento / Aprender al final)\n",
    "  Temperatura / k\n",
    "  Alfa / Learning rate\n",
    "  Cantidad de bins\n",
    "'''\n",
    "\n",
    "class AgenteRL(Agente):\n",
    "\n",
    "    def __init__(self, k=1, epsilon=0.1, politica_exploracion='epsilon-greedy', aprender_inmediatamente=True) -> None:\n",
    "        '''\n",
    "        Parametros\n",
    "        ----------\n",
    "        k: Temperatura\n",
    "        epsilon: Probabilidad de explotacion en caso de que politica_exploracion=epsilon-greedy\n",
    "        politica_exploracion: Funcion que determinará la accion a tomar en caso de encontrarse explorando. ( random, epsilon-greedy, softmax )\n",
    "        aprender_inmediatamente: Si el agente debe aprender en el momento o al final del episodio haciendo un reccorido \"hacia atras\"\n",
    "\n",
    "        Utiliza una función de aprendizaje Q-Learning no-determinista, mediante la formula:\n",
    "        Q(s, a) = (1 - alfa) * Q(s, a) + alfa * (recompensa + max(Q(s', a')))\n",
    "        donde a=1/cantidad_visitas(s, a)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.Q = {}\n",
    "        self.cantidad_visitas = {}\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.politica_exploracion = politica_exploracion\n",
    "        self.aprender_inmediatamente = aprender_inmediatamente\n",
    "\n",
    "        # En caso de que aprenda al finalizar el episodio, se guardan las acciones tomadas\n",
    "        self.acciones_tomadas = []\n",
    "\n",
    "    def elegir_accion(self, estado, max_accion, explorar=True) -> int:\n",
    "        estado = discretize_state(estado, bins)\n",
    "\n",
    "        if estado not in self.Q:\n",
    "            self.Q[estado] = {i: 0 for i in range(max_accion)}\n",
    "            for accion in range(max_accion):\n",
    "                self.cantidad_visitas[(estado, accion)] = 0\n",
    "\n",
    "        if (estado[6] == 1 and estado[7] == 1):\n",
    "            return 0        \n",
    "\n",
    "        if explorar: # Explorar\n",
    "\n",
    "            if (self.politica_exploracion == 'random'):\n",
    "                return random.randrange(max_accion)\n",
    "            \n",
    "            elif self.politica_exploracion == 'epsilon-greedy':\n",
    "                if random.random() < self.epsilon:\n",
    "                    return random.randrange(max_accion)\n",
    "                else:\n",
    "                    return max(self.Q[estado], key=self.Q[estado].get)\n",
    "                \n",
    "            elif self.politica_exploracion == 'softmax':\n",
    "                # Voy probando varias veces y no funciona :(\n",
    "                '''\n",
    "                q_values = [self.Q[estado][a] for a in range(max_accion)]\n",
    "    \n",
    "                min_q, max_q = min(q_values), max(q_values)\n",
    "                \n",
    "                if max_q - min_q > 0:\n",
    "                    normalized_q_values = [(q - min_q) / (max_q - min_q) for q in q_values]\n",
    "                else:\n",
    "                    normalized_q_values = [0] * len(q_values)  \n",
    "                \n",
    "                k_values = [self.k**q for q in normalized_q_values]\n",
    "                sum_k_values = sum(k_values)\n",
    "                probs = [k / sum_k_values for k in k_values]\n",
    "                \n",
    "                return np.random.choice(range(max_accion), p=probs)\n",
    "                '''\n",
    "\n",
    "                '''\n",
    "                q_values = [self.Q[estado][a] for a in range(max_accion)]\n",
    "                q_values_scaled = [q / self.k for q in q_values]\n",
    "                max_q = max(q_values_scaled)\n",
    "                q_values_scaled = [q - max_q for q in q_values_scaled]\n",
    "                exp_q = [np.exp(q) for q in q_values_scaled]\n",
    "                sum_exp_q = sum(exp_q)\n",
    "                probs = [q / sum_exp_q for q in exp_q]\n",
    "                return np.random.choice(range(max_accion), p=probs)\n",
    "                '''\n",
    "\n",
    "        else: # Explotacion\n",
    "            return max(self.Q[estado], key=self.Q[estado].get)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_actual, accion, recompensa, terminado) -> None:\n",
    "        estado_anterior = discretize_state(estado_anterior, bins)\n",
    "        estado_actual   = discretize_state(estado_actual, bins)\n",
    "\n",
    "        for estado in [estado_anterior, estado_actual]:\n",
    "            if estado not in self.Q:\n",
    "                self.Q[estado] = {i: 0 for i in range(4)}\n",
    "                for a in range(4):\n",
    "                    self.cant_visitas[(estado, a)] = 0\n",
    "            \n",
    "        self.cantidad_visitas[(estado_anterior, accion)] += 1 \n",
    "\n",
    "        if (self.aprender_inmediatamente):\n",
    "            alfa = 1 / self.cantidad_visitas[(estado_anterior, accion)]\n",
    "            self.Q[estado_anterior][accion] = (1 - alfa) * self.Q[estado_anterior][accion] + alfa * (recompensa + max(self.Q[estado_actual].values()))\n",
    "        else:\n",
    "            self.acciones_tomadas.append((estado_anterior, estado_actual, accion, recompensa))     \n",
    "\n",
    "\n",
    "    def fin_episodio(self) -> None:\n",
    "        '''\n",
    "        En caso de que el agente aprenda al final del episodio, se recorre hacia atras las acciones tomadas durante\n",
    "        el episodio y se actualizan los valores de Q\n",
    "        En caso contrario, no se hace nada\n",
    "        '''\n",
    "        if (not self.aprender_inmediatamente):\n",
    "            for i in range(len(self.acciones_tomadas)-1, -1, -1):\n",
    "                estado_anterior, estado_actual, accion, recompensa = self.acciones_tomadas[i]\n",
    "                alfa = 1 / self.cantidad_visitas[(estado_anterior, accion)]\n",
    "                self.Q[estado_anterior][accion] = (1 - alfa) * self.Q[estado_anterior][accion] + alfa * (recompensa + max(self.Q[estado_actual].values()))\n",
    "\n",
    "            self.acciones_tomadas = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee51f5-aff8-4d18-934e-92acdcb617c0",
   "metadata": {},
   "source": [
    "Y ejecutar con el muchos episodios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e0524d5-0d12-46a8-8437-984b981fbae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 0\n",
      "Recompensa parcial promedio: 245.35356572086934\n",
      "\n",
      "Episodio: 100\n",
      "Recompensa parcial promedio: 3.3364254901326973\n",
      "\n",
      "Episodio: 200\n",
      "Recompensa parcial promedio: -5.386503777088735\n",
      "\n",
      "Episodio: 300\n",
      "Recompensa parcial promedio: 8.324409562916209\n",
      "\n",
      "Episodio: 400\n",
      "Recompensa parcial promedio: -18.24223016285598\n",
      "\n",
      "Episodio: 500\n",
      "Recompensa parcial promedio: 18.603376136572123\n",
      "\n",
      "Episodio: 600\n",
      "Recompensa parcial promedio: 16.159709447470394\n",
      "\n",
      "Episodio: 700\n",
      "Recompensa parcial promedio: -15.88301758518149\n",
      "\n",
      "Episodio: 800\n",
      "Recompensa parcial promedio: -1.5870692120729148\n",
      "\n",
      "Episodio: 900\n",
      "Recompensa parcial promedio: 4.881306871822027\n",
      "\n",
      "Episodio: 1000\n",
      "Recompensa parcial promedio: -19.767102589115094\n",
      "\n",
      "Episodio: 1100\n",
      "Recompensa parcial promedio: 13.763906854221588\n",
      "\n",
      "Episodio: 1200\n",
      "Recompensa parcial promedio: -11.31539204429864\n",
      "\n",
      "Episodio: 1300\n",
      "Recompensa parcial promedio: -37.02650068392963\n",
      "\n",
      "Episodio: 1400\n",
      "Recompensa parcial promedio: 13.224657346972714\n",
      "\n",
      "Episodio: 1500\n",
      "Recompensa parcial promedio: -27.588956383914855\n",
      "\n",
      "Episodio: 1600\n",
      "Recompensa parcial promedio: 17.33688373361939\n",
      "\n",
      "Episodio: 1700\n",
      "Recompensa parcial promedio: -4.532052996008673\n",
      "\n",
      "Episodio: 1800\n",
      "Recompensa parcial promedio: 12.182859022912877\n",
      "\n",
      "Episodio: 1900\n",
      "Recompensa parcial promedio: -19.09975213116148\n",
      "\n",
      "Episodio: 2000\n",
      "Recompensa parcial promedio: 7.913163008693192\n",
      "\n",
      "Episodio: 2100\n",
      "Recompensa parcial promedio: 30.465533978769773\n",
      "\n",
      "Episodio: 2200\n",
      "Recompensa parcial promedio: 2.5424784235801114\n",
      "\n",
      "Episodio: 2300\n",
      "Recompensa parcial promedio: -31.124465903918463\n",
      "\n",
      "Episodio: 2400\n",
      "Recompensa parcial promedio: -12.51418115343738\n",
      "\n",
      "Episodio: 2500\n",
      "Recompensa parcial promedio: 16.748459805521904\n",
      "\n",
      "Episodio: 2600\n",
      "Recompensa parcial promedio: -18.72297623977585\n",
      "\n",
      "Episodio: 2700\n",
      "Recompensa parcial promedio: 28.261960507099953\n",
      "\n",
      "Episodio: 2800\n",
      "Recompensa parcial promedio: -28.577894157920667\n",
      "\n",
      "Episodio: 2900\n",
      "Recompensa parcial promedio: 3.075645186112001\n",
      "\n",
      "Episodio: 3000\n",
      "Recompensa parcial promedio: 11.551524642622446\n",
      "\n",
      "Episodio: 3100\n",
      "Recompensa parcial promedio: -5.315736120490482\n",
      "\n",
      "Episodio: 3200\n",
      "Recompensa parcial promedio: -1.0356418950563624\n",
      "\n",
      "Episodio: 3300\n",
      "Recompensa parcial promedio: 17.32696571411199\n",
      "\n",
      "Episodio: 3400\n",
      "Recompensa parcial promedio: -9.058627025776614\n",
      "\n",
      "Episodio: 3500\n",
      "Recompensa parcial promedio: -5.459039808385582\n",
      "\n",
      "Episodio: 3600\n",
      "Recompensa parcial promedio: 4.201840816633723\n",
      "\n",
      "Episodio: 3700\n",
      "Recompensa parcial promedio: 13.269198505506138\n",
      "\n",
      "Episodio: 3800\n",
      "Recompensa parcial promedio: 12.682497267648692\n",
      "\n",
      "Episodio: 3900\n",
      "Recompensa parcial promedio: -24.14879817673751\n",
      "\n",
      "Episodio: 4000\n",
      "Recompensa parcial promedio: -3.5805372516677227\n",
      "\n",
      "Episodio: 4100\n",
      "Recompensa parcial promedio: -16.130663730146455\n",
      "\n",
      "Episodio: 4200\n",
      "Recompensa parcial promedio: 19.217609853580292\n",
      "\n",
      "Episodio: 4300\n",
      "Recompensa parcial promedio: 15.67843548378126\n",
      "\n",
      "Episodio: 4400\n",
      "Recompensa parcial promedio: 1.759164522202073\n",
      "\n",
      "Episodio: 4500\n",
      "Recompensa parcial promedio: 20.956522939540925\n",
      "\n",
      "Episodio: 4600\n",
      "Recompensa parcial promedio: 13.623357769542112\n",
      "\n",
      "Episodio: 4700\n",
      "Recompensa parcial promedio: 17.915691983658608\n",
      "\n",
      "Episodio: 4800\n",
      "Recompensa parcial promedio: 17.76047994262706\n",
      "\n",
      "Episodio: 4900\n",
      "Recompensa parcial promedio: -6.550397929638773\n",
      "\n",
      "Episodio: 5000\n",
      "Recompensa parcial promedio: -5.926383336438474\n",
      "\n",
      "Episodio: 5100\n",
      "Recompensa parcial promedio: 0.6878707538054886\n",
      "\n",
      "Episodio: 5200\n",
      "Recompensa parcial promedio: 21.322719515150524\n",
      "\n",
      "Episodio: 5300\n",
      "Recompensa parcial promedio: 21.692835391682625\n",
      "\n",
      "Episodio: 5400\n",
      "Recompensa parcial promedio: -27.06024351664195\n",
      "\n",
      "Episodio: 5500\n",
      "Recompensa parcial promedio: 5.659209435940307\n",
      "\n",
      "Episodio: 5600\n",
      "Recompensa parcial promedio: -2.54672063319581\n",
      "\n",
      "Episodio: 5700\n",
      "Recompensa parcial promedio: -20.39842485321305\n",
      "\n",
      "Episodio: 5800\n",
      "Recompensa parcial promedio: 14.539879295297203\n",
      "\n",
      "Episodio: 5900\n",
      "Recompensa parcial promedio: -29.233630176762503\n",
      "\n",
      "Episodio: 6000\n",
      "Recompensa parcial promedio: 10.91577270813972\n",
      "\n",
      "Episodio: 6100\n",
      "Recompensa parcial promedio: 27.240198231497235\n",
      "\n",
      "Episodio: 6200\n",
      "Recompensa parcial promedio: 7.9008513433294345\n",
      "\n",
      "Episodio: 6300\n",
      "Recompensa parcial promedio: -19.82849957451023\n",
      "\n",
      "Episodio: 6400\n",
      "Recompensa parcial promedio: -12.590888532451832\n",
      "\n",
      "Episodio: 6500\n",
      "Recompensa parcial promedio: 18.80577459292004\n",
      "\n",
      "Episodio: 6600\n",
      "Recompensa parcial promedio: 11.45849224122776\n",
      "\n",
      "Episodio: 6700\n",
      "Recompensa parcial promedio: 1.2011939700351824\n",
      "\n",
      "Episodio: 6800\n",
      "Recompensa parcial promedio: -22.741176000834226\n",
      "\n",
      "Episodio: 6900\n",
      "Recompensa parcial promedio: 8.10385534421777\n",
      "\n",
      "Episodio: 7000\n",
      "Recompensa parcial promedio: 12.421760783081302\n",
      "\n",
      "Episodio: 7100\n",
      "Recompensa parcial promedio: 14.535088248000513\n",
      "\n",
      "Episodio: 7200\n",
      "Recompensa parcial promedio: -10.060302700482106\n",
      "\n",
      "Episodio: 7300\n",
      "Recompensa parcial promedio: 16.019510592998117\n",
      "\n",
      "Episodio: 7400\n",
      "Recompensa parcial promedio: 42.81620411532038\n",
      "\n",
      "Episodio: 7500\n",
      "Recompensa parcial promedio: 3.615232372037489\n",
      "\n",
      "Episodio: 7600\n",
      "Recompensa parcial promedio: 12.233385098817937\n",
      "\n",
      "Episodio: 7700\n",
      "Recompensa parcial promedio: -9.786854277337039\n",
      "\n",
      "Episodio: 7800\n",
      "Recompensa parcial promedio: -29.775097265786727\n",
      "\n",
      "Episodio: 7900\n",
      "Recompensa parcial promedio: 0.1692308250359572\n",
      "\n",
      "Episodio: 8000\n",
      "Recompensa parcial promedio: 17.566793361698483\n",
      "\n",
      "Episodio: 8100\n",
      "Recompensa parcial promedio: 3.6557230946772665\n",
      "\n",
      "Episodio: 8200\n",
      "Recompensa parcial promedio: 45.53825765945494\n",
      "\n",
      "Episodio: 8300\n",
      "Recompensa parcial promedio: 3.903857366389866\n",
      "\n",
      "Episodio: 8400\n",
      "Recompensa parcial promedio: 4.002589529145391\n",
      "\n",
      "Episodio: 8500\n",
      "Recompensa parcial promedio: 22.82382587048747\n",
      "\n",
      "Episodio: 8600\n",
      "Recompensa parcial promedio: -0.14593140766583929\n",
      "\n",
      "Episodio: 8700\n",
      "Recompensa parcial promedio: -8.00379139336041\n",
      "\n",
      "Episodio: 8800\n",
      "Recompensa parcial promedio: -12.774723412096678\n",
      "\n",
      "Episodio: 8900\n",
      "Recompensa parcial promedio: 26.039865115354413\n",
      "\n",
      "Episodio: 9000\n",
      "Recompensa parcial promedio: 16.650656848432643\n",
      "\n",
      "Episodio: 9100\n",
      "Recompensa parcial promedio: 10.095638489271137\n",
      "\n",
      "Episodio: 9200\n",
      "Recompensa parcial promedio: 2.3404060530168507\n",
      "\n",
      "Episodio: 9300\n",
      "Recompensa parcial promedio: 8.12752772255603\n",
      "\n",
      "Episodio: 9400\n",
      "Recompensa parcial promedio: 18.602852749019824\n",
      "\n",
      "Episodio: 9500\n",
      "Recompensa parcial promedio: 20.308728860008543\n",
      "\n",
      "Episodio: 9600\n",
      "Recompensa parcial promedio: 19.289415110598814\n",
      "\n",
      "Episodio: 9700\n",
      "Recompensa parcial promedio: 35.29989453309082\n",
      "\n",
      "Episodio: 9800\n",
      "Recompensa parcial promedio: 21.979163557049755\n",
      "\n",
      "Episodio: 9900\n",
      "Recompensa parcial promedio: -19.86021012219874\n",
      "\n",
      "Episodio: 10000\n",
      "Recompensa parcial promedio: 15.346579849295424\n",
      "\n",
      "Episodio: 10100\n",
      "Recompensa parcial promedio: -1.458540193480173\n",
      "\n",
      "Episodio: 10200\n",
      "Recompensa parcial promedio: 19.969808652627595\n",
      "\n",
      "Episodio: 10300\n",
      "Recompensa parcial promedio: 9.101175063668565\n",
      "\n",
      "Episodio: 10400\n",
      "Recompensa parcial promedio: 42.151893130273166\n",
      "\n",
      "Episodio: 10500\n",
      "Recompensa parcial promedio: 5.210604195208143\n",
      "\n",
      "Episodio: 10600\n",
      "Recompensa parcial promedio: 19.32734637955428\n",
      "\n",
      "Episodio: 10700\n",
      "Recompensa parcial promedio: 24.022257195821613\n",
      "\n",
      "Episodio: 10800\n",
      "Recompensa parcial promedio: -1.3929837119179314\n",
      "\n",
      "Episodio: 10900\n",
      "Recompensa parcial promedio: -18.550066611672587\n",
      "\n",
      "Episodio: 11000\n",
      "Recompensa parcial promedio: -7.5969962501290444\n",
      "\n",
      "Episodio: 11100\n",
      "Recompensa parcial promedio: 5.608066419450895\n",
      "\n",
      "Episodio: 11200\n",
      "Recompensa parcial promedio: 44.2490516700278\n",
      "\n",
      "Episodio: 11300\n",
      "Recompensa parcial promedio: -6.809976894304899\n",
      "\n",
      "Episodio: 11400\n",
      "Recompensa parcial promedio: -26.667401881727084\n",
      "\n",
      "Episodio: 11500\n",
      "Recompensa parcial promedio: -12.03555927530344\n",
      "\n",
      "Episodio: 11600\n",
      "Recompensa parcial promedio: -43.58074766775851\n",
      "\n",
      "Episodio: 11700\n",
      "Recompensa parcial promedio: 1.2149556833571635\n",
      "\n",
      "Episodio: 11800\n",
      "Recompensa parcial promedio: 39.92549896737741\n",
      "\n",
      "Episodio: 11900\n",
      "Recompensa parcial promedio: 21.348002869227436\n",
      "\n",
      "Episodio: 12000\n",
      "Recompensa parcial promedio: 12.927438884947001\n",
      "\n",
      "Episodio: 12100\n",
      "Recompensa parcial promedio: 28.952210259796757\n",
      "\n",
      "Episodio: 12200\n",
      "Recompensa parcial promedio: 20.30914870409957\n",
      "\n",
      "Episodio: 12300\n",
      "Recompensa parcial promedio: 11.191915848920292\n",
      "\n",
      "Episodio: 12400\n",
      "Recompensa parcial promedio: 8.58772530209602\n",
      "\n",
      "Episodio: 12500\n",
      "Recompensa parcial promedio: 27.636076244325885\n",
      "\n",
      "Episodio: 12600\n",
      "Recompensa parcial promedio: 9.439913131345055\n",
      "\n",
      "Episodio: 12700\n",
      "Recompensa parcial promedio: 8.625545666127934\n",
      "\n",
      "Episodio: 12800\n",
      "Recompensa parcial promedio: -7.317584473180685\n",
      "\n",
      "Episodio: 12900\n",
      "Recompensa parcial promedio: 6.220259276960166\n",
      "\n",
      "Episodio: 13000\n",
      "Recompensa parcial promedio: 14.86100596778575\n",
      "\n",
      "Episodio: 13100\n",
      "Recompensa parcial promedio: -13.631764981863851\n",
      "\n",
      "Episodio: 13200\n",
      "Recompensa parcial promedio: -2.2467457656149525\n",
      "\n",
      "Episodio: 13300\n",
      "Recompensa parcial promedio: -25.95379652400845\n",
      "\n",
      "Episodio: 13400\n",
      "Recompensa parcial promedio: -11.162186153079597\n",
      "\n",
      "Episodio: 13500\n",
      "Recompensa parcial promedio: -15.331768303602892\n",
      "\n",
      "Episodio: 13600\n",
      "Recompensa parcial promedio: -13.012357398957338\n",
      "\n",
      "Episodio: 13700\n",
      "Recompensa parcial promedio: -19.04897170421971\n",
      "\n",
      "Episodio: 13800\n",
      "Recompensa parcial promedio: 26.65615191797534\n",
      "\n",
      "Episodio: 13900\n",
      "Recompensa parcial promedio: 14.485528706497508\n",
      "\n",
      "Episodio: 14000\n",
      "Recompensa parcial promedio: 5.538053360584643\n",
      "\n",
      "Episodio: 14100\n",
      "Recompensa parcial promedio: -12.586402617313132\n",
      "\n",
      "Episodio: 14200\n",
      "Recompensa parcial promedio: -4.402869621055537\n",
      "\n",
      "Episodio: 14300\n",
      "Recompensa parcial promedio: 17.72548229636446\n",
      "\n",
      "Episodio: 14400\n",
      "Recompensa parcial promedio: 3.724311782965765\n",
      "\n",
      "Episodio: 14500\n",
      "Recompensa parcial promedio: 30.53193121951697\n",
      "\n",
      "Episodio: 14600\n",
      "Recompensa parcial promedio: -3.623504588030611\n",
      "\n",
      "Episodio: 14700\n",
      "Recompensa parcial promedio: 3.5262883566178163\n",
      "\n",
      "Episodio: 14800\n",
      "Recompensa parcial promedio: 15.988085363167754\n",
      "\n",
      "Episodio: 14900\n",
      "Recompensa parcial promedio: 1.7963886093862755\n",
      "\n",
      "Episodio: 15000\n",
      "Recompensa parcial promedio: 6.148428406743622\n",
      "\n",
      "Episodio: 15100\n",
      "Recompensa parcial promedio: -2.5243625261140585\n",
      "\n",
      "Episodio: 15200\n",
      "Recompensa parcial promedio: -1.3652159541925952\n",
      "\n",
      "Episodio: 15300\n",
      "Recompensa parcial promedio: 13.43633683652439\n",
      "\n",
      "Episodio: 15400\n",
      "Recompensa parcial promedio: -5.3777108233143345\n",
      "\n",
      "Episodio: 15500\n",
      "Recompensa parcial promedio: -6.315859075778827\n",
      "\n",
      "Episodio: 15600\n",
      "Recompensa parcial promedio: -7.358566314421453\n",
      "\n",
      "Episodio: 15700\n",
      "Recompensa parcial promedio: 1.0542382760924938\n",
      "\n",
      "Episodio: 15800\n",
      "Recompensa parcial promedio: 13.086410688438537\n",
      "\n",
      "Episodio: 15900\n",
      "Recompensa parcial promedio: 19.10259310403226\n",
      "\n",
      "Episodio: 16000\n",
      "Recompensa parcial promedio: 17.486365964275006\n",
      "\n",
      "Episodio: 16100\n",
      "Recompensa parcial promedio: 4.8821467619449725\n",
      "\n",
      "Episodio: 16200\n",
      "Recompensa parcial promedio: -3.851214031593711\n",
      "\n",
      "Episodio: 16300\n",
      "Recompensa parcial promedio: 10.734162478408944\n",
      "\n",
      "Episodio: 16400\n",
      "Recompensa parcial promedio: -0.10795728123789339\n",
      "\n",
      "Episodio: 16500\n",
      "Recompensa parcial promedio: -7.3009805128572225\n",
      "\n",
      "Episodio: 16600\n",
      "Recompensa parcial promedio: -2.554426861356261\n",
      "\n",
      "Episodio: 16700\n",
      "Recompensa parcial promedio: -5.282077279897956\n",
      "\n",
      "Episodio: 16800\n",
      "Recompensa parcial promedio: 18.165178112844565\n",
      "\n",
      "Episodio: 16900\n",
      "Recompensa parcial promedio: 6.74562484402879\n",
      "\n",
      "Episodio: 17000\n",
      "Recompensa parcial promedio: -8.874503310972896\n",
      "\n",
      "Episodio: 17100\n",
      "Recompensa parcial promedio: 3.2131304859552263\n",
      "\n",
      "Episodio: 17200\n",
      "Recompensa parcial promedio: 10.513274350691859\n",
      "\n",
      "Episodio: 17300\n",
      "Recompensa parcial promedio: 15.287773622910615\n",
      "\n",
      "Episodio: 17400\n",
      "Recompensa parcial promedio: 10.033402979586983\n",
      "\n",
      "Episodio: 17500\n",
      "Recompensa parcial promedio: 18.210254783756326\n",
      "\n",
      "Episodio: 17600\n",
      "Recompensa parcial promedio: 38.031394801285316\n",
      "\n",
      "Episodio: 17700\n",
      "Recompensa parcial promedio: -24.54185317398273\n",
      "\n",
      "Episodio: 17800\n",
      "Recompensa parcial promedio: 12.400445018702175\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m num_episodios \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200000\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodios):\n\u001b[1;32m----> 8\u001b[0m     recompensa \u001b[38;5;241m=\u001b[39m \u001b[43mejecutar_episodio\u001b[49m\u001b[43m(\u001b[49m\u001b[43magente\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maprender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iteraciones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (recompensa \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m):\n",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m, in \u001b[0;36mejecutar_episodio\u001b[1;34m(agente, aprender, render, max_iteraciones)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Le informamos al agente para que aprenda\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (aprender):\n\u001b[1;32m---> 17\u001b[0m     \u001b[43magente\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maprender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestado_anterior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestado_siguiente\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecompensa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtermino\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m estado_anterior \u001b[38;5;241m=\u001b[39m estado_siguiente\n\u001b[0;32m     20\u001b[0m iteraciones \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[6], line 91\u001b[0m, in \u001b[0;36mAgenteRL.aprender\u001b[1;34m(self, estado_anterior, estado_actual, accion, recompensa, terminado)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maprender\u001b[39m(\u001b[38;5;28mself\u001b[39m, estado_anterior, estado_actual, accion, recompensa, terminado) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m     estado_anterior \u001b[38;5;241m=\u001b[39m \u001b[43mdiscretize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestado_anterior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     estado_actual   \u001b[38;5;241m=\u001b[39m discretize_state(estado_actual, bins)\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m estado \u001b[38;5;129;01min\u001b[39;00m [estado_anterior, estado_actual]:\n",
      "Cell \u001b[1;32mIn[3], line 33\u001b[0m, in \u001b[0;36mdiscretize_state\u001b[1;34m(state, bins)\u001b[0m\n\u001b[0;32m     30\u001b[0m         state_disc\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mint\u001b[39m(state[i]))\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;66;03m# Encuentra el índice del valor más cercano en los bins\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m         closest_index \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbins\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m         state_disc\u001b[38;5;241m.\u001b[39mappend(closest_index)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(state_disc)\n",
      "File \u001b[1;32mc:\\Users\\nicoa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1325\u001b[0m, in \u001b[0;36margmin\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;124;03mReturns the indices of the minimum values along an axis.\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmin\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\nicoa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "entorno = gym.make('LunarLander-v2').env\n",
    "agente = AgenteRL(politica_exploracion='epsilon-greedy', epsilon=0.1, aprender_inmediatamente=False)\n",
    "exitos = 0\n",
    "recompensa_parcial=[]\n",
    "recompensa_episodios = []\n",
    "num_episodios = 200000\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio(agente, aprender=True, max_iteraciones=1000)\n",
    "    \n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        exitos += 1\n",
    "    recompensa_episodios += [recompensa]\n",
    "    recompensa_parcial += [recompensa]\n",
    "    \n",
    "    if (i % 100 == 0):\n",
    "        print(f'Episodio: {i}')\n",
    "        print(f'Recompensa parcial promedio: {np.mean(recompensa_parcial)}')\n",
    "        print('')\n",
    "        recompensa_parcial = []\n",
    "    \n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {np.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e7b346-6f23-44e2-a645-e4548ab470ef",
   "metadata": {},
   "source": [
    "Analizar los resultados de la ejecución anterior, incluyendo:\n",
    " * Un análisis de los parámetros utilizados en el algoritmo (aprendizaje, política de exploración)\n",
    " * Un análisis de algunos 'cortes' de la matriz Q y la política (p.e. qué hace la nave cuando está cayendo rápidamente hacia abajo, sin rotación)\n",
    " * Un análisis de la evolución de la recompensa promedio\n",
    " * Un análisis de los casos de éxito\n",
    " * Un análisis de los casos en el que el agente falla\n",
    " * Qué limitante del agente de RL les parece que afecta más negativamente su desempeño. Cómo lo mejorarían? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f93fe904-e691-42ff-8fd4-b360d08431cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 0\n",
      "Recompensa: 302.76429676931855\n",
      "Recompensa promedio: nan\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicoa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\nicoa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 6\n",
      "Recompensa: 239.41004553694577\n",
      "Recompensa promedio: -23.170199066852174\n",
      "\n",
      "Episodio: 9\n",
      "Recompensa: 234.0533557878106\n",
      "Recompensa promedio: 8.081132353349727\n",
      "\n",
      "Episodio: 18\n",
      "Recompensa: 257.3529935827414\n",
      "Recompensa promedio: -8.798332854190933\n",
      "\n",
      "Episodio: 19\n",
      "Recompensa: 223.6439866242992\n",
      "Recompensa promedio: 5.2096316951212955\n",
      "\n",
      "Episodio: 34\n",
      "Recompensa: 213.58223946839553\n",
      "Recompensa promedio: -18.158430585490947\n",
      "\n",
      "Episodio: 38\n",
      "Recompensa: 250.5290524483476\n",
      "Recompensa promedio: -13.006138465322953\n",
      "\n",
      "Episodio: 44\n",
      "Recompensa: 217.33306540209145\n",
      "Recompensa promedio: -10.778786890886359\n",
      "\n",
      "Episodio: 50\n",
      "Recompensa: 207.4394328331374\n",
      "Recompensa promedio: -11.006006750466238\n",
      "\n",
      "Episodio: 62\n",
      "Recompensa: 300.9749612241151\n",
      "Recompensa promedio: -29.71259351234987\n",
      "\n",
      "Episodio: 64\n",
      "Recompensa: 231.4753753071301\n",
      "Recompensa promedio: -24.275950060192727\n",
      "\n",
      "Episodio: 68\n",
      "Recompensa: 217.60454972342663\n",
      "Recompensa promedio: -20.902501761008935\n",
      "\n",
      "Episodio: 69\n",
      "Recompensa: 246.60290582133663\n",
      "Recompensa promedio: -17.4458778264519\n",
      "\n",
      "Episodio: 71\n",
      "Recompensa: 203.12210100329412\n",
      "Recompensa promedio: -13.226661044210033\n",
      "\n",
      "Episodio: 73\n",
      "Recompensa: 267.36900555377906\n",
      "Recompensa promedio: -10.952696007783093\n",
      "\n",
      "Episodio: 75\n",
      "Recompensa: 230.6471817657093\n",
      "Recompensa promedio: -4.570017507336339\n",
      "\n",
      "Episodio: 76\n",
      "Recompensa: 231.7641927052554\n",
      "Recompensa promedio: -1.475054359006791\n",
      "\n",
      "Episodio: 82\n",
      "Recompensa: 279.4626221637763\n",
      "Recompensa promedio: -2.5511315695514325\n",
      "\n",
      "Episodio: 91\n",
      "Recompensa: 240.6237978362668\n",
      "Recompensa promedio: -3.0403512105195705\n",
      "\n",
      "Episodio: 94\n",
      "Recompensa: 250.18638355852576\n",
      "Recompensa promedio: -3.3387668543277034\n",
      "\n",
      "Episodio: 95\n",
      "Recompensa: 248.65315146357676\n",
      "Recompensa promedio: -0.6700810605081933\n",
      "\n",
      "Episodio: 96\n",
      "Recompensa: 265.20372886718167\n",
      "Recompensa promedio: 1.927035944951026\n",
      "\n",
      "Episodio: 99\n",
      "Recompensa: 249.34926057817646\n",
      "Recompensa promedio: 2.780800600249416\n",
      "\n",
      "Episodio: 101\n",
      "Recompensa: 203.9877900319214\n",
      "Recompensa promedio: 5.173622790214853\n",
      "\n",
      "Episodio: 104\n",
      "Recompensa: 218.51060850604975\n",
      "Recompensa promedio: 9.273537129401877\n",
      "\n",
      "Episodio: 109\n",
      "Recompensa: 297.38539451271527\n",
      "Recompensa promedio: 7.1087012660884366\n",
      "\n",
      "Episodio: 112\n",
      "Recompensa: 200.2262491298065\n",
      "Recompensa promedio: 10.285313013645752\n",
      "\n",
      "Episodio: 120\n",
      "Recompensa: 246.84083882893933\n",
      "Recompensa promedio: 11.092700717665235\n",
      "\n",
      "Episodio: 129\n",
      "Recompensa: 310.2162427173258\n",
      "Recompensa promedio: 5.4441042110573905\n",
      "\n",
      "Episodio: 130\n",
      "Recompensa: 218.74455420913193\n",
      "Recompensa promedio: 7.788505276490223\n",
      "\n",
      "Episodio: 131\n",
      "Recompensa: 236.27201360900494\n",
      "Recompensa promedio: 9.398856795060007\n",
      "\n",
      "Episodio: 136\n",
      "Recompensa: 261.5448843645744\n",
      "Recompensa promedio: 11.16492382387634\n",
      "\n",
      "Episodio: 150\n",
      "Recompensa: 236.63789866706657\n",
      "Recompensa promedio: 1.634364625426039\n",
      "\n",
      "Episodio: 163\n",
      "Recompensa: 212.97513225641836\n",
      "Recompensa promedio: -0.16584147682514594\n",
      "\n",
      "Episodio: 166\n",
      "Recompensa: 278.23151159700717\n",
      "Recompensa promedio: -0.9866520660699019\n",
      "\n",
      "Episodio: 169\n",
      "Recompensa: 227.7424661226368\n",
      "Recompensa promedio: 0.003305786572324899\n",
      "\n",
      "Episodio: 177\n",
      "Recompensa: 261.00539059384903\n",
      "Recompensa promedio: -1.746980694517522\n",
      "\n",
      "Episodio: 183\n",
      "Recompensa: 262.4945460091724\n",
      "Recompensa promedio: -3.110637187322746\n",
      "\n",
      "Episodio: 184\n",
      "Recompensa: 255.7145140652813\n",
      "Recompensa promedio: -1.6671307569070113\n",
      "\n",
      "Episodio: 186\n",
      "Recompensa: 252.77328403591994\n",
      "Recompensa promedio: -1.4161354078379047\n",
      "\n",
      "Episodio: 188\n",
      "Recompensa: 237.21462815238993\n",
      "Recompensa promedio: -0.5245276045835199\n",
      "\n",
      "Episodio: 191\n",
      "Recompensa: 255.95426643483918\n",
      "Recompensa promedio: 2.5241401458035124\n",
      "\n",
      "Episodio: 196\n",
      "Recompensa: 217.74198750464345\n",
      "Recompensa promedio: 3.2044730345809933\n",
      "\n",
      "Episodio: 207\n",
      "Recompensa: 261.25458468667074\n",
      "Recompensa promedio: -0.5150898043965957\n",
      "\n",
      "Episodio: 229\n",
      "Recompensa: 279.4164024304141\n",
      "Recompensa promedio: -4.475296591436973\n",
      "\n",
      "Episodio: 238\n",
      "Recompensa: 205.1507817204469\n",
      "Recompensa promedio: -2.805783122248403\n",
      "\n",
      "Episodio: 245\n",
      "Recompensa: 275.96659429671234\n",
      "Recompensa promedio: -3.3310614632444997\n",
      "\n",
      "Episodio: 246\n",
      "Recompensa: 262.86400275836047\n",
      "Recompensa promedio: -2.195705139017033\n",
      "\n",
      "Episodio: 251\n",
      "Recompensa: 248.01351215886825\n",
      "Recompensa promedio: -1.0097067218926956\n",
      "\n",
      "Episodio: 261\n",
      "Recompensa: 270.65973120726363\n",
      "Recompensa promedio: -1.6587201344430775\n",
      "\n",
      "Episodio: 264\n",
      "Recompensa: 232.2518197530227\n",
      "Recompensa promedio: -0.7110069454202638\n",
      "\n",
      "Episodio: 275\n",
      "Recompensa: 222.042053123872\n",
      "Recompensa promedio: 0.055129575375485726\n",
      "\n",
      "Episodio: 276\n",
      "Recompensa: 279.68644422821365\n",
      "Recompensa promedio: 0.8594300230149656\n",
      "\n",
      "Episodio: 279\n",
      "Recompensa: 219.89753349002837\n",
      "Recompensa promedio: 0.7699636090922494\n",
      "\n",
      "Episodio: 282\n",
      "Recompensa: 236.52782490254083\n",
      "Recompensa promedio: 0.9648374799611144\n",
      "\n",
      "Episodio: 287\n",
      "Recompensa: 236.47067423998\n",
      "Recompensa promedio: 0.5504842702178018\n",
      "\n",
      "Episodio: 295\n",
      "Recompensa: 210.34349971499523\n",
      "Recompensa promedio: 0.6305507922348158\n",
      "\n",
      "Episodio: 299\n",
      "Recompensa: 274.193182680282\n",
      "Recompensa promedio: 1.1093530402924754\n",
      "\n",
      "Episodio: 307\n",
      "Recompensa: 254.46336040269654\n",
      "Recompensa promedio: -0.9135744012709784\n",
      "\n",
      "Episodio: 310\n",
      "Recompensa: 219.52238623544838\n",
      "Recompensa promedio: -0.4578340097614885\n",
      "\n",
      "Episodio: 313\n",
      "Recompensa: 243.74399525452316\n",
      "Recompensa promedio: 0.15037184622028493\n",
      "\n",
      "Episodio: 316\n",
      "Recompensa: 247.68185633055768\n",
      "Recompensa promedio: 0.6613723030258499\n",
      "\n",
      "Episodio: 325\n",
      "Recompensa: 237.68292381549898\n",
      "Recompensa promedio: 0.5962353116629898\n",
      "\n",
      "Episodio: 327\n",
      "Recompensa: 201.2117465893521\n",
      "Recompensa promedio: 0.5989431469290812\n",
      "\n",
      "Episodio: 332\n",
      "Recompensa: 230.00312183300798\n",
      "Recompensa promedio: 0.529739826518148\n",
      "\n",
      "Episodio: 341\n",
      "Recompensa: 257.99069225848126\n",
      "Recompensa promedio: -0.6245387044816524\n",
      "\n",
      "Episodio: 343\n",
      "Recompensa: 233.00586153560172\n",
      "Recompensa promedio: -0.5110630859337337\n",
      "\n",
      "Episodio: 345\n",
      "Recompensa: 211.1870842045932\n",
      "Recompensa promedio: 0.1395423879470793\n",
      "\n",
      "Episodio: 346\n",
      "Recompensa: 220.18688237631\n",
      "Recompensa promedio: 0.7495063816368079\n",
      "\n",
      "Episodio: 351\n",
      "Recompensa: 265.41136913797686\n",
      "Recompensa promedio: 0.39320688498821216\n",
      "\n",
      "Episodio: 353\n",
      "Recompensa: 240.9057458358209\n",
      "Recompensa promedio: 1.1749253683364598\n",
      "\n",
      "Episodio: 373\n",
      "Recompensa: 276.8013794423476\n",
      "Recompensa promedio: -1.278120396571197\n",
      "\n",
      "Episodio: 376\n",
      "Recompensa: 258.8680754654705\n",
      "Recompensa promedio: -0.7703604811781788\n",
      "\n",
      "Episodio: 378\n",
      "Recompensa: 223.30326419905884\n",
      "Recompensa promedio: -0.726102768112678\n",
      "\n",
      "Episodio: 379\n",
      "Recompensa: 281.7439134058017\n",
      "Recompensa promedio: -0.13499625896446826\n",
      "\n",
      "Episodio: 386\n",
      "Recompensa: 203.50603163490746\n",
      "Recompensa promedio: -0.8589326724760219\n",
      "\n",
      "Episodio: 391\n",
      "Recompensa: 230.60061539076932\n",
      "Recompensa promedio: -0.2551315007219018\n",
      "\n",
      "Episodio: 394\n",
      "Recompensa: 212.01443898907496\n",
      "Recompensa promedio: -0.3440904597005462\n",
      "\n",
      "Episodio: 398\n",
      "Recompensa: 256.9371082651766\n",
      "Recompensa promedio: -0.07532402609516727\n",
      "\n",
      "Episodio: 401\n",
      "Recompensa: 228.0855484711944\n",
      "Recompensa promedio: -0.0036618316921016303\n",
      "\n",
      "Episodio: 406\n",
      "Recompensa: 208.23052816237674\n",
      "Recompensa promedio: -0.12808910562067768\n",
      "\n",
      "Episodio: 413\n",
      "Recompensa: 212.13294871096411\n",
      "Recompensa promedio: -1.6831468289681615\n",
      "\n",
      "Episodio: 415\n",
      "Recompensa: 207.06384317051376\n",
      "Recompensa promedio: -1.6839437860770907\n",
      "\n",
      "Episodio: 417\n",
      "Recompensa: 245.2220288292142\n",
      "Recompensa promedio: -1.0968177572232383\n",
      "\n",
      "Episodio: 419\n",
      "Recompensa: 272.8510045326896\n",
      "Recompensa promedio: -0.8705656396265351\n",
      "\n",
      "Episodio: 439\n",
      "Recompensa: 237.03420479030822\n",
      "Recompensa promedio: -2.271124576200035\n",
      "\n",
      "Episodio: 443\n",
      "Recompensa: 256.69227181934514\n",
      "Recompensa promedio: -2.2325335639076167\n",
      "\n",
      "Episodio: 445\n",
      "Recompensa: 241.35424704669484\n",
      "Recompensa promedio: -1.7664986771247941\n",
      "\n",
      "Episodio: 450\n",
      "Recompensa: 223.55961145084427\n",
      "Recompensa promedio: -1.7690864344999935\n",
      "\n",
      "Episodio: 451\n",
      "Recompensa: 249.360838108692\n",
      "Recompensa promedio: -1.2694662618052168\n",
      "\n",
      "Episodio: 461\n",
      "Recompensa: 239.04662009710043\n",
      "Recompensa promedio: -1.5775396363386407\n",
      "\n",
      "Episodio: 466\n",
      "Recompensa: 247.5226202888536\n",
      "Recompensa promedio: -3.0441228839390297\n",
      "\n",
      "Episodio: 468\n",
      "Recompensa: 258.08263552683\n",
      "Recompensa promedio: -2.9740601672364484\n",
      "\n",
      "Episodio: 471\n",
      "Recompensa: 231.99414445230926\n",
      "Recompensa promedio: -2.5512306550717514\n",
      "\n",
      "Episodio: 472\n",
      "Recompensa: 253.5938875266828\n",
      "Recompensa promedio: -2.054312487471368\n",
      "\n",
      "Episodio: 478\n",
      "Recompensa: 265.08963881129864\n",
      "Recompensa promedio: -1.4411878360437849\n",
      "\n",
      "Episodio: 479\n",
      "Recompensa: 242.3847411128679\n",
      "Recompensa promedio: -0.8847560476359718\n",
      "\n",
      "Episodio: 481\n",
      "Recompensa: 290.33981766732836\n",
      "Recompensa promedio: -0.3664914556991281\n",
      "\n",
      "Episodio: 483\n",
      "Recompensa: 204.17903215332788\n",
      "Recompensa promedio: -0.33737079277538246\n",
      "\n",
      "Episodio: 487\n",
      "Recompensa: 237.1699374506725\n",
      "Recompensa promedio: -0.06978182552620614\n",
      "\n",
      "Episodio: 492\n",
      "Recompensa: 236.18278316243052\n",
      "Recompensa promedio: 0.7195102020586971\n",
      "\n",
      "Episodio: 498\n",
      "Recompensa: 288.5227265227072\n",
      "Recompensa promedio: 0.4063926089154372\n",
      "\n",
      "Episodio: 500\n",
      "Recompensa: 252.52429725477762\n",
      "Recompensa promedio: 0.8261599623102538\n",
      "\n",
      "Episodio: 529\n",
      "Recompensa: 268.19305965544606\n",
      "Recompensa promedio: -1.1766977149204574\n",
      "\n",
      "Episodio: 534\n",
      "Recompensa: 256.37659968679327\n",
      "Recompensa promedio: -0.5148493486116116\n",
      "\n",
      "Episodio: 547\n",
      "Recompensa: 281.20399053292925\n",
      "Recompensa promedio: -2.3844471724246197\n",
      "\n",
      "Episodio: 554\n",
      "Recompensa: 225.59700997451512\n",
      "Recompensa promedio: -2.579117342850587\n",
      "\n",
      "Episodio: 558\n",
      "Recompensa: 246.12492897165728\n",
      "Recompensa promedio: -2.7506045542303603\n",
      "\n",
      "Episodio: 562\n",
      "Recompensa: 234.23811155605557\n",
      "Recompensa promedio: -2.462716811329505\n",
      "\n",
      "Episodio: 576\n",
      "Recompensa: 238.59710391719173\n",
      "Recompensa promedio: -3.332707123917362\n",
      "\n",
      "Episodio: 581\n",
      "Recompensa: 224.2450507588888\n",
      "Recompensa promedio: -3.0640942249558436\n",
      "\n",
      "Episodio: 584\n",
      "Recompensa: 227.82190391989738\n",
      "Recompensa promedio: -2.802670714653473\n",
      "\n",
      "Episodio: 591\n",
      "Recompensa: 235.164372337067\n",
      "Recompensa promedio: -3.414402481569639\n",
      "\n",
      "Episodio: 592\n",
      "Recompensa: 244.9196226458391\n",
      "Recompensa promedio: -3.011397794375997\n",
      "\n",
      "Episodio: 596\n",
      "Recompensa: 248.03679341415818\n",
      "Recompensa promedio: -2.85861669136854\n",
      "\n",
      "Episodio: 598\n",
      "Recompensa: 266.62614135430067\n",
      "Recompensa promedio: -2.689813991308564\n",
      "\n",
      "Episodio: 600\n",
      "Recompensa: 216.0989729803085\n",
      "Recompensa promedio: -2.1760359795186437\n",
      "\n",
      "Episodio: 602\n",
      "Recompensa: 212.2326806704334\n",
      "Recompensa promedio: -2.1725213479349303\n",
      "\n",
      "Episodio: 608\n",
      "Recompensa: 207.23201129272348\n",
      "Recompensa promedio: -2.2284981206323464\n",
      "\n",
      "Episodio: 617\n",
      "Recompensa: 292.95789119631104\n",
      "Recompensa promedio: -2.535364177416292\n",
      "\n",
      "Episodio: 625\n",
      "Recompensa: 239.99164463814375\n",
      "Recompensa promedio: -2.5535519460936555\n",
      "\n",
      "Episodio: 634\n",
      "Recompensa: 268.25209575726797\n",
      "Recompensa promedio: -2.4049097869002485\n",
      "\n",
      "Episodio: 639\n",
      "Recompensa: 240.68576941473304\n",
      "Recompensa promedio: -2.468000257774419\n",
      "\n",
      "Episodio: 641\n",
      "Recompensa: 243.1317691355454\n",
      "Recompensa promedio: -2.1703768301842947\n",
      "\n",
      "Episodio: 642\n",
      "Recompensa: 262.48088673318983\n",
      "Recompensa promedio: -1.7882862601442173\n",
      "\n",
      "Episodio: 654\n",
      "Recompensa: 228.3911398611288\n",
      "Recompensa promedio: -2.2504864571734235\n",
      "\n",
      "Episodio: 669\n",
      "Recompensa: 250.60631090853087\n",
      "Recompensa promedio: -2.973971310456048\n",
      "\n",
      "Episodio: 678\n",
      "Recompensa: 242.31874217763695\n",
      "Recompensa promedio: -3.133331563106371\n",
      "\n",
      "Episodio: 680\n",
      "Recompensa: 242.98073526631595\n",
      "Recompensa promedio: -2.5142439136250374\n",
      "\n",
      "Episodio: 689\n",
      "Recompensa: 200.9033295021839\n",
      "Recompensa promedio: -3.263810800125594\n",
      "\n",
      "Episodio: 702\n",
      "Recompensa: 222.44248638266254\n",
      "Recompensa promedio: -3.5914971946900187\n",
      "\n",
      "Episodio: 704\n",
      "Recompensa: 257.7972830665998\n",
      "Recompensa promedio: -3.5148920693084063\n",
      "\n",
      "Episodio: 706\n",
      "Recompensa: 226.89936942224332\n",
      "Recompensa promedio: -3.194755827657753\n",
      "\n",
      "Episodio: 707\n",
      "Recompensa: 234.60553105865\n",
      "Recompensa promedio: -2.8693044482378074\n",
      "\n",
      "Episodio: 708\n",
      "Recompensa: 252.06887116570218\n",
      "Recompensa promedio: -2.5338880139060453\n",
      "\n",
      "Episodio: 709\n",
      "Recompensa: 206.46467206971386\n",
      "Recompensa promedio: -2.1747868020871337\n",
      "\n",
      "Episodio: 710\n",
      "Recompensa: 265.6462668030306\n",
      "Recompensa promedio: -1.8809284093099494\n",
      "\n",
      "Episodio: 712\n",
      "Recompensa: 253.0026767974844\n",
      "Recompensa promedio: -1.5854248377124416\n",
      "\n",
      "Episodio: 717\n",
      "Recompensa: 237.22464365105682\n",
      "Recompensa promedio: -2.021786718990361\n",
      "\n",
      "Episodio: 724\n",
      "Recompensa: 204.74492870369878\n",
      "Recompensa promedio: -1.4243395871960045\n",
      "\n",
      "Episodio: 728\n",
      "Recompensa: 215.10082971534436\n",
      "Recompensa promedio: -1.403066643337267\n",
      "\n",
      "Episodio: 738\n",
      "Recompensa: 244.84756873526126\n",
      "Recompensa promedio: -1.9267048436619618\n",
      "\n",
      "Episodio: 743\n",
      "Recompensa: 239.8951480910237\n",
      "Recompensa promedio: -1.8797455347817185\n",
      "\n",
      "Episodio: 764\n",
      "Recompensa: 219.1547960873479\n",
      "Recompensa promedio: -2.527574709839817\n",
      "\n",
      "Episodio: 765\n",
      "Recompensa: 226.15274953810467\n",
      "Recompensa promedio: -2.2377938329807483\n",
      "\n",
      "Episodio: 773\n",
      "Recompensa: 228.5683697887288\n",
      "Recompensa promedio: -2.9377482396473416\n",
      "\n",
      "Episodio: 781\n",
      "Recompensa: 204.57586360992076\n",
      "Recompensa promedio: -2.897244241703808\n",
      "\n",
      "Episodio: 786\n",
      "Recompensa: 226.34568487182378\n",
      "Recompensa promedio: -2.851245358478863\n",
      "\n",
      "Episodio: 790\n",
      "Recompensa: 293.2483515144831\n",
      "Recompensa promedio: -2.667736705306566\n",
      "\n",
      "Episodio: 794\n",
      "Recompensa: 279.03615292786327\n",
      "Recompensa promedio: -2.517875340776891\n",
      "\n",
      "Episodio: 795\n",
      "Recompensa: 237.0275370706181\n",
      "Recompensa promedio: -2.163719330376086\n",
      "\n",
      "Episodio: 802\n",
      "Recompensa: 312.50191779018775\n",
      "Recompensa promedio: -2.035724202685849\n",
      "\n",
      "Episodio: 805\n",
      "Recompensa: 275.85459152842316\n",
      "Recompensa promedio: -1.9106577552159822\n",
      "\n",
      "Episodio: 816\n",
      "Recompensa: 211.92696099393115\n",
      "Recompensa promedio: -2.8338999168351906\n",
      "\n",
      "Episodio: 820\n",
      "Recompensa: 217.72817515356974\n",
      "Recompensa promedio: -2.521986903222674\n",
      "\n",
      "Episodio: 827\n",
      "Recompensa: 263.875762753839\n",
      "Recompensa promedio: -2.8562517992801184\n",
      "\n",
      "Episodio: 833\n",
      "Recompensa: 246.95116921347625\n",
      "Recompensa promedio: -2.8509829961723416\n",
      "\n",
      "Episodio: 838\n",
      "Recompensa: 214.97856524541322\n",
      "Recompensa promedio: -2.882305918160211\n",
      "\n",
      "Episodio: 840\n",
      "Recompensa: 237.9920649124332\n",
      "Recompensa promedio: -2.6866369580253595\n",
      "\n",
      "Episodio: 845\n",
      "Recompensa: 265.53833329933934\n",
      "Recompensa promedio: -2.1485630442169827\n",
      "\n",
      "Episodio: 849\n",
      "Recompensa: 234.0365965841518\n",
      "Recompensa promedio: -2.5674808540542307\n",
      "\n",
      "Episodio: 859\n",
      "Recompensa: 241.88354723665077\n",
      "Recompensa promedio: -2.937604651946973\n",
      "\n",
      "Episodio: 863\n",
      "Recompensa: 276.1252168301784\n",
      "Recompensa promedio: -2.78266997965525\n",
      "\n",
      "Episodio: 868\n",
      "Recompensa: 231.22933919703752\n",
      "Recompensa promedio: -3.1856575452275515\n",
      "\n",
      "Episodio: 869\n",
      "Recompensa: 281.1164484728365\n",
      "Recompensa promedio: -2.9159049597934144\n",
      "\n",
      "Episodio: 877\n",
      "Recompensa: 235.43344448799084\n",
      "Recompensa promedio: -2.7132928487748544\n",
      "\n",
      "Episodio: 883\n",
      "Recompensa: 264.92594379494926\n",
      "Recompensa promedio: -2.6874340745411356\n",
      "\n",
      "Episodio: 894\n",
      "Recompensa: 223.0226939774466\n",
      "Recompensa promedio: -3.3261437887027636\n",
      "\n",
      "Episodio: 895\n",
      "Recompensa: 236.84585303425752\n",
      "Recompensa promedio: -3.073240059355111\n",
      "\n",
      "Episodio: 904\n",
      "Recompensa: 212.08706369333382\n",
      "Recompensa promedio: -3.502647782450523\n",
      "\n",
      "Episodio: 911\n",
      "Recompensa: 212.38472823089418\n",
      "Recompensa promedio: -3.338097319281048\n",
      "\n",
      "Episodio: 920\n",
      "Recompensa: 210.20343715871763\n",
      "Recompensa promedio: -3.64541418799515\n",
      "\n",
      "Episodio: 931\n",
      "Recompensa: 296.24426308244824\n",
      "Recompensa promedio: -2.8224147684088496\n",
      "\n",
      "Episodio: 933\n",
      "Recompensa: 250.5941526268156\n",
      "Recompensa promedio: -2.3775666159921953\n",
      "\n",
      "Episodio: 934\n",
      "Recompensa: 272.60415601475074\n",
      "Recompensa promedio: -2.106718950850003\n",
      "\n",
      "Episodio: 935\n",
      "Recompensa: 239.94010420924553\n",
      "Recompensa promedio: -1.8129105284269005\n",
      "\n",
      "Episodio: 937\n",
      "Recompensa: 205.6052650265655\n",
      "Recompensa promedio: -1.8378191459694553\n",
      "\n",
      "Episodio: 940\n",
      "Recompensa: 248.99608639259515\n",
      "Recompensa promedio: -1.9999219324523563\n",
      "\n",
      "Episodio: 951\n",
      "Recompensa: 258.3982359074546\n",
      "Recompensa promedio: -2.1687854551897625\n",
      "\n",
      "Episodio: 958\n",
      "Recompensa: 232.2675619056392\n",
      "Recompensa promedio: -2.604062860897476\n",
      "\n",
      "Episodio: 965\n",
      "Recompensa: 210.49289042642175\n",
      "Recompensa promedio: -2.4653908992459295\n",
      "\n",
      "Episodio: 972\n",
      "Recompensa: 212.09617299720367\n",
      "Recompensa promedio: -2.5814819787623957\n",
      "\n",
      "Episodio: 975\n",
      "Recompensa: 228.80528006653256\n",
      "Recompensa promedio: -2.4077638585360517\n",
      "\n",
      "Episodio: 985\n",
      "Recompensa: 227.633276550515\n",
      "Recompensa promedio: -3.2169911269163713\n",
      "\n",
      "Episodio: 986\n",
      "Recompensa: 284.929828588451\n",
      "Recompensa promedio: -2.982863066391593\n",
      "\n",
      "Episodio: 988\n",
      "Recompensa: 270.95084944798816\n",
      "Recompensa promedio: -2.4877769914275314\n",
      "\n",
      "Episodio: 997\n",
      "Recompensa: 264.9868038163052\n",
      "Recompensa promedio: -2.8805657284404087\n",
      "\n",
      "Tasa de éxito: 0.187. Se obtuvo -2.8530657077942583 de recompensa, en promedio\n"
     ]
    }
   ],
   "source": [
    "entorno = gym.make('LunarLander-v2').env\n",
    "exitos = 0\n",
    "recompensa_episodios = []\n",
    "num_episodios = 1000\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio(agente, aprender = False, max_iteraciones=1000)\n",
    "    recompensa_episodios += [recompensa]\n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        exitos += 1\n",
    "        print(f'Episodio: {i}')\n",
    "        print(f'Recompensa: {recompensa}')\n",
    "        print(f'Recompensa promedio: {np.mean(recompensa_episodios)}')\n",
    "        print('')\n",
    "\n",
    "    \n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {np.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
